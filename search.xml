<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[neutron-- neutron 分布式虚拟路由（DVR）]]></title>
      <url>%2F2017%2F06%2F20%2Fneutron-dvr%2F</url>
      <content type="text"><![CDATA[引出公司有个测试云平台的虚拟机密码初始化出现了问题，根据之前的经验可能跟这篇问题一样,没想到花费了将近一天的时间才将问题解决，发现最终是跟neutron 分布式虚拟路由有关，这里记录一下。 问题是这样的：nova boot的时候传入 user-data（user-data中含有需要修改的密码），instance起来的时候会执行cloud-init，然后会使用EC2的datasource方式（即“169.254.169.254”）去获取user-data，然而发现密码没有更改，查看日志发现,发现获取user-data时，报错如下 Calling ‘http://169.254.169.254/2009-04-04/meta-data/instance-id‘ failed [0/120s]: bad status code [500] 初步断定就是instance metadata 的某个环节网络不通。 问题发现 第一步想到的就是重启下相关服务，将neutron-metada-agent,neutron-l3-agent,nova-api重启后发现问题依旧。 查看相关日志，没有找到有关报错信息，而且奇怪的是网络节点对应的neutron-ns-metadata-proxy-xxxxxxxxx.log都没有处理请求的日志，说明请求可能没有走到这里。 只能按照请求的处理步骤一步步排查，首先确定虚拟机所在的网络是有路由器相连的，所以metadata请求是走的路由器的namespace，相关知识参考openstack– openstack instance metadata 服务机制探索,网络节点对应路由器的namespace中一切正常，9697 端口的重定向，以及监听9697端口的neutron-metadata-ns-proxy进程也正常。没办法，在该路由器的namespace中对应端口抓包试试，虚拟机中发送169.254.169.254，却一点反应没有，断定请求压根就没有走到该路由器的namespace。只能返回去虚拟机实例中抓包试试，使用traceroute命令发现请求是发到对应网关的，而对应网关就应该在路由器的namespace中啊，到这里基本就蒙逼了。 在经过一番徒劳无功的试错后，终于找到了一点线索。我在查看neutron agent状态的时候，发现neutron-l3-agent不只是在网络节点，计算节点也存在neutron-l3-agent，立马想到neutron-l3-agent会在本地计算节点也创建虚拟路由器，所以获取metadata的请求就发送到本地计算节点的路由器 namespace，果然，本地计算节点也有一个路由器，抓包发现确实是发送到了这里，而且该节点对应网络的neutron-ns-metadata-proxy-xxxxxxxxx.log发现了 socket.error报错信息，说明neutron-ns-metadata-proxy通过 unix domian socket 转发给 neutron-metadata-agent的时候出现了问题。本能的想是不是配置错误导致的，查看l3-agent的配置文件时发现有个配置项：agent_mode = dvr ，赶紧搜了一下，终于找到了进入的正确姿势。 neutron 分布式虚拟路由（DVR）整体架构neutron 的DVR 是属于openstack HA的一部分，主要解决虚拟路由器的HA（即neutron-l3-agent的HA）。之前写过一篇记录openstack的HA,但没有仔细研究过neutron DVR。 在没有使用dvr时，我们的整体架构大致是这样的： 计算节点只需要安装L2 Agent就可以，而使用dvr模式后： 计算节点除了L2 Agent外，还要安装L3 Agent以及Metadata Agent。也就是说，通过使用dvr，计算节点也有了网络节点的三层转发和NAT功能，起到了分流的作用。 安装配置 控制节点：修改neutron.conf 如下，重启neutron-server 12[DEFAULT]router_distributed = True 网络节点：修改 openswitch_agent.ini如下，重启Open vSwitch agent 12[DEFAULT]enable_distributed_routing = True 修改l3_agent.ini如下，重启Layer-3 agent， 12[DEFAULT]agent_mode = dvr_snat 计算节点：安装l3-agent,metadata-agent,修改 openswitch_agent.ini如下，重启Open vSwitch agent 12[DEFAULT]enable_distributed_routing = True 修改l3_agent.ini 如下，重启l3-agent 1234[DEFAULT]interface_driver = openvswitchexternal_network_bridge =agent_mode = dvr 可以通过 neutron agent-list 验证agent 是否启动。 网络流通方向 照着这张图梳理一遍以下几种情况的网络走向： 没有floating-ip的instance的南北向网络流通情况即图中蓝色标注的线路，以图中左边计算节点的instance为例，首先通过一对veth设备与一个linux-bridge相连（该linux-bridge的存在主要是为了利用iptables实现Sec-group），又是一对veth设备，连接到OVS生成的br-int网桥，该instance在三层网络的第一跳便是到与该br-int相连的Disk router namespace的网关接口，因为没有floating-ip ，所以不会通过FIP namespace，而是通过patch设备到br-tun，再到网络节点的br-int,到达snat namespace，进行snat(源地址转换)，最后通过ovs-provider-bridge与外网联通。 有floating-ip的instance的南北向网络流通情况刚开始与第一种情况类似，但是跳到本地router namespace后，接下来会跳到本地的RIP namespace，做snat,然后直接通过本地的ovs-provider-bridge连接到外网。 不同子网但有虚拟路由连接的instance东西向网络刚开始类似，之后在本地router namespace进行路由选择，并通过br-int,br-tun进入对应虚拟机的计算节点（该部分工作由ovs 的openflow完成，同时还完成了snat），到了 目标计算节点上，依次被 br-tun，br-int 处理，直到通过 tap 设备进入另一instance。 关于详细描述以及实验可以参考官方文档,以及这篇。 问题解决回到刚开始碰到的问题，确定了问题的所在，就是在本地计算节点neutron-ns-metadata-proxy通过 unix domian socket 转发给 neutron-metadata-agent的时候出现了问题，neutron-ns-metadata-proxy是正常的，那么问题就出在neutron-metadata-agent上，果然，该agent在计算节点并没有启动，启动后正常。 参考文章Network Troubleshooting Neutron Networking: Neutron Routers and the L3 Agent Neutron/DVR 理解 OpenStack 高可用（HA）（3）：Neutron 分布式虚拟路由（Neutron Distributed Virtual Routing） Neutron 理解 (6): Neutron 是怎么实现虚拟三层网络的 [How Neutron implements virtual L3 network] Neutron DVR实现multi-host特性打通东西南北流量提前看 Open vSwitch: High availability using DVR END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python-- python 中的一些隐藏特性]]></title>
      <url>%2F2017%2F06%2F14%2Fpython-hidden-feature%2F</url>
      <content type="text"><![CDATA[引出偶然间翻到Stackoverflow上的这个问题，浏览了一下，觉得有点意思，摘录了一些感觉能用到的纪录在此。 使用‘ * ’ 代表函数的list/dict参数1234567def draw_point(x, y): print(x,y)point_foo = [3, 4] # or (3,4)point_bar = &#123;'y': 3, 'x': 2&#125;draw_point(*point_foo)draw_point(**point_bar) 这个特性其实使用还算常见，不过pylint不建议这样使用，因为debug的时候会不清晰。 链式比较操作1234567891011&gt;&gt;&gt; x = 5&gt;&gt;&gt; 1 &lt; x &lt; 10True&gt;&gt;&gt; 10 &lt; x &lt; 20 False&gt;&gt;&gt; x &lt; 10 &lt; x*10 &lt; 100True&gt;&gt;&gt; 10 &gt; x &lt;= 9True&gt;&gt;&gt; 5 == x &gt; 4 True 比如最后一个比较，python会将这一句转化为 (5 == x and x &gt; 4) ，这是跟C或java不一样的。 装饰器装饰器用一个函数或方法封装另一个函数以增加功能或修改参数，结果等。 12345678910111213&gt;&gt;&gt; def print_args(function):&gt;&gt;&gt; def wrapper(*args, **kwargs):&gt;&gt;&gt; print 'Arguments:', args, kwargs&gt;&gt;&gt; return function(*args, **kwargs)&gt;&gt;&gt; return wrapper&gt;&gt;&gt; @print_args&gt;&gt;&gt; def write(text):&gt;&gt;&gt; print text&gt;&gt;&gt; write('foo')Arguments: ('foo',) &#123;&#125;foo 小心使用可变的默认参数12345678910&gt;&gt;&gt; def foo(x=[]):... x.append(1)... print x... &gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1, 1]&gt;&gt;&gt; foo()[1, 1, 1] 可以加一个哨兵值来做判断： 123456789&gt;&gt;&gt; def foo(x=None):... if x is None:... x = []... x.append(1)... print x&gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1] 利用python 正则语法树来debug正则表达式12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; re.compile("^\[font(?:=(?P&lt;size&gt;[-+][0-9]&#123;1,2&#125;))?\](.*?)[/font]", re.DEBUG)at at_beginningliteral 91literal 102literal 111literal 110literal 116max_repeat 0 1 subpattern None literal 61 subpattern 1 in literal 45 literal 43 max_repeat 1 2 in range (48, 57)literal 93subpattern 2 min_repeat 0 65535 any Nonein literal 47 literal 102 literal 111 literal 110 literal 116``` 当然，最常用的还是使用注解的方式以增强正则的可读性。```bash&gt;&gt;&gt; re.compile(""" ^ # start of a line \[font # the font tag (?:=(?P&lt;size&gt; # optional [font=+size] [-+][0-9]&#123;1,2&#125; # size specification ))? \] # end of tag (.*?) # text between the tags \[/font\] # end of the tag """, re.DEBUG|re.VERBOSE|re.DOTALL) 利用enumerate 获取list的下标123456789&gt;&gt;&gt; a = ['a', 'b', 'c', 'd', 'e']&gt;&gt;&gt; for index, item in enumerate(a, start=1): print index, item...1 a2 b3 c4 d5 e&gt;&gt;&gt; 生成器12345678&gt;&gt;&gt; n = ((a,b) for a in range(0,2) for b in range(4,6))&gt;&gt;&gt; for i in n:... print i (0, 4)(0, 5)(1, 4)(1, 5) 需要注意的是，列表生成式是实实在在生成了列表存在了内存里，可以多次迭代使用，而生成器只能迭代一次。 巧用list中的slice操作slice操作如下： 123a = [1,2,3,4,5]&gt;&gt;&gt; a[::2] # iterate over the whole list in 2-increments[1,3,5] 将一个list逆序，最简洁写法： 12&gt;&gt;&gt; a[::-1][5,4,3,2,1] for…else 语法12345for i in foo: if i == 0: breakelse: print("i was never 0") 上述代码如果break，那么else代码是不会执行的。上述代码的执行顺如跟下面的代码是一样的：1234567found = Falsefor i in foo: if i == 0: found = True breakif not found: print("i was never 0") 两个变量互换12345678&gt;&gt;&gt; a = 10&gt;&gt;&gt; b = 5&gt;&gt;&gt; a, b(10, 5)&gt;&gt;&gt; a, b = b, a&gt;&gt;&gt; a, b(5, 10) with 声明语句with 在Python2.5需要从future模块导入，2.6+已经可以直接使用。示例： 1234from __future__ import with_statementwith open('foo.txt', 'w') as f: f.write('hello!') with 语句在这里的作用是在调用with时，会调用这个文件实例的 enter 方法，和 exit 方法，如果发生异常，异常的详细信息会传给 exit 方法处理并抛出，with在这里的主要用途是保证文件句柄最终关闭，其实with 可以看作是异常处理的一种抽象。除了打开文件经常使用with外，线程锁以及数据库事务也经常用到。 参考文章Hidden features of Python END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph-- ceph rbd快照的实现原理]]></title>
      <url>%2F2017%2F06%2F05%2Fceph-rbd-snapshot%2F</url>
      <content type="text"><![CDATA[引出关于openstack 中的快照，备份部分之前做过一次总结，openstack 中的快照与备份 ,因为目前基本使用ceph rbd作为共享存储后端，所以本篇文章想讨论以rbd作为存储后端时的备份与快照情况： nova image-create：具体实现参考openstack 中的快照与备份 nova backup: 同上。 cinder snapshot：直接利用 rbd snapshot 实现 cinder backup：支持增量备份，也用到了rbd snapshot，参考 cinder-backup 利用ceph实现增量备份 由以上情况可知，rbd snapshot 几乎都要用到，所以接下来探讨下rbd snapshot 的实现原理。 rbd snapshot vs rbd clone首先知道这两者的区别： snapshot：是某一镜像(image) 在特定时间点的一个只读副本,注意，是只读副本，所以该snapshot是无法作为一个image去进行写操作的（比如起一个虚拟机）。 clone: 是针对snapshot的一种操作，可以理解为对snapshot的再度拷贝，从而实现可写操作，简单讲就是将image的某一个snapshot 的状态复制变成另一个image，该image状态与snapshot完全一致，但是可读可写（可以从此image起一个虚拟机）。 弄懂了两者的区别，接下来需要认真探讨下两者实现的原理： rbd snapshot 实现原理注意：snapshot时，一定要对原rbd image 停止IO操作，否则会引起数据不一致。 跟git 实现版本控制类似，使用 COW （copy on write）方式实现。这里叙述下对某一pool中的某个image 进行snapshot 的过程： 直接引用自解析Ceph: Snapshot： 每个Pool都有一个snap_seq字段，该字段可以认为是整个Pool的Global Version。所有存储在Ceph的Object也都带有snap_seq，而每个Object会有一个Head版本的，也可能会存在一组Snapshot objects，不管是Head版本还是snapshot object都会带有snap_seq，那么接下来我们看librbd是如何利用该字段创建Snapshot的。 用户申请为”pool”中的”image-name”创建一个名为”snap-name”的Snapshot librbd向Ceph Monitor申请得到一个”pool”的snap sequence，Ceph Monitor会递增该Pool的snap_seq，然后返回该值给librbd。 librbd将新的snap_seq替换原来image的snap_seq中，并且将原来的snap_seq设置为用户创建的名为”snap-name”的Snapshot的snap_seq。 每个Snapshot都掌握者一个snap_seq，Image可以看成一个Head Version的Snapshot，每次IO操作对会带上snap_seq发送给Ceph OSD，Ceph OSD会查询该IO操作涉及的object的snap_seq情况。如”object-1″是”image-name”中的一个数据对象，那么初始的snap_seq就”image-name”的snap_seq，当创建一个Snapshot以后，再次对”object-1″进行写操作时会带上新的snap_seq，Ceph接到请求后会先检查”object-1″的Head Version，会发现该写操作所带有的snap_seq大于”object-1″的snap_seq，那么就会对原来的”object-1″克隆一个新的Object Head Version，原来的”object-1″会作为Snapshot，新的Object Head会带上新的snap_seq，也就是librbd之前申请到的。 实验过程可以参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone],实验结论摘抄如下： rbd clone 实现原理从用户角度来说，clone操作实现了对某个只读snapshot 的image化，即clone之后，可以对这个clone进行读写，snapshot等等，跟一个rbd image一样。从系统实现来说，也是利用 COW方式实现，clone会将clone与snapshot 的父子关系保存起来,以备IO操作时查找。 实验验证部分参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone]。 结论摘抄如下： COW VS ROW要想了解COW 与ROW，首先需要知道快照的两种类型，一种是全量快照，一种是增量快照。 全量快照，顾名思义，就是为源数据卷创建并维护一个完整的镜像卷。 增量快照，利用COW 或ROW的方式实现的差量快照。 COW(COPY-ON-WRITE) 写时复制在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，先将原始数据copy到快照卷中（预留空间），更新快照卷的数据指针表，再将更新数据写入源数据卷。 写时复制的优势在于创建快照速度快（仅复制数据指针表），且占用存储空间少，但因为创建快照后每次写入操作都要进行一次复制才开始写入源数据，所以降低了源数据卷的写性能。所以COW 适合读多写少的场景，除此之外，如果一个应用容易出现对存储设备的写入热点(只针对某个有限范围内的数据进行写操作),也是比较理想的选择.因为其数据更改都局限在一个范围内（局部性原理）, 对同一份数据的多次写操作只会出现一次写时复制操作。 ROW(REDIRECT-ON-WRITE)写时重定向在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，将源数据卷数据指针表中的被更新原始数据指针重定向到新的存储空间，所以由此至终, 快照卷的数据指针表和其对应的数据是没有被改变过的。恢复快照的时候,只需要按照快照卷数据指针表来进行寻址就可以完成恢复了. 除了COW的优势外，ROW 因为更新源数据卷只需要一次写操作, 解决了 COW写两次的性能问题. 所以 ROW 最明显的优势就是不会降低源数据卷的写性能。但ROW 的快照卷数据指针表保存的是源数据卷的原始副本,而源数据卷数据指针表保存的则是更新后的副本,导致在删除快照卷之前需要将快照卷数据指针表指向的数据同步至源数据卷中. 而且当创建了多个快照后, 会产生一个快照链,使原始数据的访问、快照卷和源数据卷数据的追踪以及快照的删除将变得异常复杂且消耗时间。除此之外,因为源数据卷数据指针指向的数据会很快的被重定向分散, 所以 ROW 另一个主要缺点就是降低了读性能(局部空间原理)。在传统存储设备上,ROW快照在多次读写后,源数据卷的数据被分散,对于连续读写的性能不如COW. 所以 ROW 比较适合Write-Intensive(写密集) 类型的存储系统. 但是, 在分布式存储设备上, ROW 的连续读写的性能会比 COW 更加好. 一般而言,读写性能的瓶颈都在磁盘上.而分布式存储的特性是数据越是分散到不同的存储设备中, 系统性能越高。所以ROW的源数据卷重定向分散性反而带来了好处。 因此, ROW 逐渐成为了业界分布式存储的主流。 参考文章SNAPSHOTS 解析Ceph: Snapshot 关于Ceph的snapshot和clone Ceph Snapshots: Diving into Deep Waters 理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone] ROW/COW 快照技术原理解析 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python-- python threadpool 的前世今生]]></title>
      <url>%2F2017%2F05%2F18%2Fpython-thread-pool%2F</url>
      <content type="text"><![CDATA[引出首先需要了解的是threadpool 的用途，他更适合于用到一些大量的短任务合集，而非一些时间长的任务，换句话说，适合大量的CPU密集型短任务，那些消耗时间较长的IO密集型长任务适合用协程去解决。 目前，python 标准库（特指python2.X）中的threadpool模块是在 multiprocessing.pool.threadpool，或者multiprocessing.dummy.ThreadPool（dummy模块是针对threading 多线程的进一步封装）。该模块有个缺点就是在所有线程执行完之前无法强制退出。实现原理大同小异：实例化pool的时候会创建指定数目的线程，把task 传给一个task-queue，线程会读取task-queue 的task，没有就阻塞，读取到后就执行，并将结果交给一个result-queue。 除了标准库中的threadpool，还有一些使用比较多的threadpool，以下展开。 pip 中的 ThreadPool安装简单：pip install threadpool使用如下： 1234pool = ThreadPool(poolsize) # 定义线程池，指定线程数量requests = makeRequests(some_callable, list_of_args, callback) # 调用makeRequests创建了要开启多线程的函数，以及函数相关参数和回调函数 [pool.putRequest(req) for req in requests] # 所有要运行多线程的请求扔进线程池pool.wait() # 等待所有线程完成后退出 原理类似，源码解读可以参考python——有一种线程池叫做自己写的线程池 ,该博客还给出了对其的一些优化。 自己定制 threadpool根据需要的功能定制适合自己的threadpool 也是一种常见的手段，常用的功能比如：是否需要返回线程执行后的返回值，线程执行完之后销毁还是阻塞等等。以下为自己经常用的的一个比较简洁的threadpool，感谢@kaito-kidd提供，源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# coding: utf8"""线程池，用于高效执行某些任务。"""import Queueimport threadingclass Task(threading.Thread): """ 任务 """ def __init__(self, num, input_queue, output_queue, error_queue): super(Task, self).__init__() self.thread_name = "thread-%s" % num self.input_queue = input_queue self.output_queue = output_queue self.error_queue = error_queue self.deamon = True def run(self): """run """ while 1: try: func, args = self.input_queue.get(block=False) except Queue.Empty: print "%s finished!" % self.thread_name break try: result = func(*args) except Exception as exc: self.error_queue.put((func.func_name, args, str(exc))) else: self.output_queue.put(result)class Pool(object): """ 线程池 """ def __init__(self, size): self.input_queue = Queue.Queue() self.output_queue = Queue.Queue() self.error_queue = Queue.Queue() self.tasks = [ Task(i, self.input_queue, self.output_queue, self.error_queue) for i in range(size) ] def add_task(self, func, args): """添加单个任务 """ if not isinstance(args, tuple): raise TypeError("args must be tuple type!") self.input_queue.put((func, args)) def add_tasks(self, tasks): """批量添加任务 """ if not isinstance(tasks, list): raise TypeError("tasks must be list type!") for func, args in tasks: self.add_task(func, args) def get_results(self): """获取执行结果集 """ while not self.output_queue.empty(): print "Result: ", self.output_queue.get() def get_errors(self): """获取执行失败的结果集 """ while not self.error_queue.empty(): func, args, error_info = self.error_queue.get() print "Error: func: %s, args : %s, error_info : %s" \ % (func.func_name, args, error_info) def run(self): """执行 """ for task in self.tasks: task.start() for task in self.tasks: task.join()def test(i): """test """ result = i * 10 return resultdef main(): """ main """ pool = Pool(size=5) pool.add_tasks([(test, (i,)) for i in range(100)]) pool.run()if __name__ == "__main__": main() 参考文章Python Thread Pool 理解python的multiprocessing.pool threadpool多线程 thread_pools.py Cinder磁盘备份原理与实践 python线程池（threadpool）模块使用笔记 thread_pool END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- cinder-backup 利用ceph实现增量备份]]></title>
      <url>%2F2017%2F05%2F09%2Fopenstack-cinder-incremental-backup-with-ceph%2F</url>
      <content type="text"><![CDATA[配置首先如果想要实现 ceph backend cinder的增量备份的话，要保证cinder-volume 与 cinder-backup 的backend 都是ceph，涉及到具体实践的话就是openstack 与 ceph integration 的 配置，ceph官网有详实的步骤 BLOCK DEVICES AND OPENSTACK ,当然，中文版的一搜也一大把openstack的glance、nova、cinder使用ceph做后端存储 ,这里提一点，就是 ，如果cinder-volume 是默认以ceph rbd为backend，那么glance 配置文件其实也可以直接默认用cinder 作为默认存储，但最终其实都是存在ceph 里，glance配置文件可以类似这样，这样cinder list 也可以列出镜像盘： 123456 ......[glance_store]stores=file,http,cinder,rbddefault_store=cindercinder_os_region_name=RegionOne..... 还想提一点就是，backup 尽量使用 multi-ceph 的情况，因为个人感觉备份主要是为了容灾，单 ceph的情况其实容灾能力有限。 使用注：以Liberty版本示例使用非常简单，首先对 volume 做一个全量备份： 1$ cinder backup-create --name fullbackup &lt;volume-ID&gt; 然后再做一次备份就是增量备份了， 1$ cinder backup-create --name incrementalbackup --incremental &lt;volume-ID&gt; 其实不用加incremental tag也是可以的。 可以利用删除backup 来证明一下，增量备份都会有一个 parent backup（父备份），就是基于父备份来做增量备份，通常来说，就是我们刚开始做的那个全量备份。在删除父备份之前，必须先把所有它的子备份删除，才可以成功删除父备份。示例如下： 实现原理关于备份的具体流程可以参考这张图： 熟悉ceph的话，应该知道ceph rbd的export-diff，import-diff 功能： export-diff ：将某个 rbd image 在两个不同时刻的数据状态比较不同后导出补丁文件。 import-diff :将某个补丁文件合并到某个 rbd image 中。 ceph 增量备份就是基于这两个基本功能，详细命令示例可以参考rbd的增量备份和恢复 首次备份 在用作备份的ceph集群中新建一个base image，与源 volume 大小一致，name 的形式为 “volume-VOLUMD_UUID.backup.base” 源 rbd image 新建一个快照，name形式为 backup.BACKUP_ID.snap.TIMESTRAMP 源rbd image 上使用 export-diff 命令导出从刚开始创建时到上一步快照时的差异数据，其实就是现在整个rbd的数据，然后通过管道将差量数据导入刚刚在备份集群上新创建的base image中 再次备份 在要备份的源volume 中找到满足 r”^backup.([a-z0-9-]+?).snap.(.+)$” 的最近一次快照。 源volume 创建一个新的快照，name 形式为 backup.BACKUP_ID.snap.TIMESTRAMP。 源rbd image 上使用 export-diff 命令导出与最近的一次快照比较的差量数据，然后通过管道将差量数据导入到备份集群的rbd image中 恢复时相反，只需要从备份集群找出对应的快照并导出差量数据，导入到原volume即可 参考文章BLOCK DEVICES AND OPENSTACK openstack的glance、nova、cinder使用ceph做后端存储 Ceph backup driver Cinder磁盘备份原理与实践 Openstack 中cinder backup三种backend的对比 Inside Cinder’s Incremental Backup END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- 关于openstack HA 的一些记录]]></title>
      <url>%2F2017%2F04%2F14%2Fopenstack-ha%2F</url>
      <content type="text"><![CDATA[概念所谓HA(High Availability),高可用,就是指在本地系统的某个组件出现故障的情况下，系统依然可访问,应用的能力。而为了达到这种目的，我们一般采用节点冗余的方法组成集群来提供服务。通常来说，可以分为两种冗余的方式： Active/Passive HA: 即A/P模式，中文主备模式，通常会有一个主节点来提供服务，备用节点会同步/异步与主节点进行数据同步。当主节点故障时，备用节点会启用代替主节点。通常会使用CRM软件比如Pacemaker来进行主备节点之间的切换并提供一个VIP提供服务。 Active/Active HA:也就是我们常说的A/A方式，中文也称作双活或多主模式。这种模式下，系统在集群的所有服务器上运行同样的负载，也就是说集群内服务器的功能都是一样的。这种模式下，通常会采用负载均衡软件如HAProxy提供VIP进行负载均衡。 涉及到具体的服务时，HA的部署方式也不尽相同，一般来说，一些无状态的服务比如提供http请求转发的http服务器等，可以直接部署A/A模式，无需考虑数据不一致的问题。而一些有状态的服务，比如数据库等，则需要具体问题具体对待，如果这类服务提供原生的A/A服务，那么尽量利用他们提供的原生A/A服务。如果实在不行那么只能从A/P角度去部署。 关于openstack 的HA根据openstack部署节点的功能来分，可以将各节点分为以下四种，这四种节点的HA方式也不尽相同： Cloud Controller Node （云控制节点）：安装各种 API 服务和内部工作组件（worker process）。同时，通常也会将共享的 DB 和 MQ 安装在该节点上。 Neutron Controller Node （网络控制节点）：安装 Neutron L3 Agent，L2 Agent，LBaas，VPNaas，FWaas，Metadata Agent 等 Neutron 组件。 Storage Controller Node （存储控制节点）：安装 Cinder volume 以及 Swift 组件。 Compute node （计算节点）：安装 Nova-compute 和 Neutron L2 Agent，在该节点上创建虚机。 本文接下来就主要按照这四个节点的顺序记录下对应的HA方案，通常来说，部署的原则遵循尽量A/A,尽量采用原生支持的HA方案,考虑负载均衡，以及方案尽量简单。 云控制节点 HA注：这里只考虑A/A方案，A/P方案可以通过Pacemaker+Corosync 的方式搭建集群，业界应用的比较少。 首先看下控制节点中的各种服务，对于无状态服务（API服务+内部工作组件如nova-scheduler等），我们可以直接多节点部署，如果是提供API 的利用pacemaker 提供VIP,HAproxy提供负载均衡。对于有状态的服务我们可以尽量使用原生的AA方案，罗列如下（摘自理解 OpenStack 高可用（HA）（1）：OpenStack 高可用和灾备方案 [OpenStack HA and DR]）： API 服务：包括 *-api, neutron-server，glance-registry(注：glance api v2 版本将该服务与glance-api集合到了一起), nova-novncproxy，keystone，httpd 等。由 HAProxy 提供负载均衡，将请求按照一定的算法转到某个节点上的 API 服务。由 Pacemaker 提供 VIP。 内部组件：包括 *-scheduler，nova-conductor，nova-cert 等。它们都是无状态的，因此可以在多个节点上部署，它们会使用 HA 的 MQ 和 DB。RabbitMQ：跨三个节点部署 RabbitMQ 集群和镜像消息队列。可以使用 HAProxy 提供负载均衡，或者将 RabbitMQ host list 配置给 OpenStack 组件（使用 rabbit_hosts 和 rabbit_ha_queues 配置项）。 MariaDB：至少跨三个节点部署 Gelera MariaDB 多主复制集群。由 HAProxy 提供负载均衡。 HAProxy：向 API，RabbitMQ 和 MariaDB 多活服务提供负载均衡，其自身由 Pacemaker 实现 A/P HA，提供 VIP，某一时刻只由一个HAProxy提供服务。在部署中，也可以部署单独的 HAProxy 集群。 Memcached：它原生支持 A/A，只需要在 OpenStack 中配置它所有节点的名称即可，比如，memcached_servers = controller1:11211,controller2:11211。当 controller1:11211 失效时，OpenStack 组件会自动使用controller2:11211。 从一个请求的角度大致时序如下： 这里在重点说一下rabbitmq 与mysql用的HA方案： rabbitmq 的HA首先了解下rabbitmq集群的消息传递模式，rabbitmq的集群有两种模式，如下： 普通模式：默认的集群模式，以两个节点（rabbit01、rabbit02）为例来进行说明。对于Queue来说，消息实体只存在于其中一个节点rabbit01（或者rabbit02），rabbit01和rabbit02两个节点仅有相同的元数据，即队列的结构。当消息进入rabbit01节点的Queue后，consumer从rabbit02节点消费时，RabbitMQ会临时在rabbit01、rabbit02间进行消息传输，把A中的消息实体取出并经过B发送给consumer。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。当rabbit01节点故障后，rabbit02节点无法取到rabbit01节点中还未消费的消息实体。如果做了消息持久化，那么得等rabbit01节点恢复，然后才可被消费；如果没有持久化的话，就会产生消息丢失的现象。该模式只是起到单纯的扩展作用（如增加queue的存储空间等），并没有达到HA 的效果。 镜像模式：将需要消费的队列变为镜像队列，存在于多个节点，这样就可以实现RabbitMQ的HA高可用性。作用就是消息实体会主动在镜像节点之间实现同步，而不是像普通模式那样，在consumer消费数据时临时读取。缺点就是，集群内部的同步通讯会占用大量的网络带宽。 rabbitmq的 A/P方案可以采用Pacemaker + （DRBD 或者其它可靠的共享 NAS/SNA 存储） + （CoroSync 或者 Heartbeat 或者 OpenAIS）来实现，安装配置可以参考理解 OpenStack 高可用（HA）（5）：RabbitMQ HA. openstack 官方推荐采用 rabbitmq 镜像模式实现A/A, 虽然是A/A，但是rabbitmq 镜像模式集群中还是有master/slave的概念,这个概念是针对queue的，通常创建queue的节点为该queue 的master节点，其他节点为slave节点。通常是一个master,多个slave ，master节点失效后，会自动选出另一个master。对于消息的发布，producer可以连接任意节点，如果该节点不是master，则会转发给master，master会向其他slave节点发送该消息，后进行消息本地化处理，并组播复制消息到其他节点存储；对于consumer，可以选择任意一个节点进行连接，消费的请求会转发给master,为保证消息的可靠性，consumer需要进行ack确认，master收到ack后，才会删除消息，ack消息会同步(默认异步)到其他各个节点，进行slave节点删除消息。可以看到虽然起到了H/A的效果，但是并没有达到减轻负载的作用，所以rabbitmq mirror 不支持负载均衡。关于这部分详细讲解可以参考Highly Available (Mirrored) Queues openstack rabbitmq H/A 的具体安装配置参考Messaging service for high availability mysql 的HAmysql 的HA 方案有很多，这里只讨论openstack 官方推荐的mariadb galara 集群。Galera Cluster 是一套在innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到各个节点上去。在数据方面完全兼容 MariaDB 和 MySQL。 特点如下： 1）同步复制，（&gt;=3）奇数个节点 2）Active-active的多主拓扑结构 3）集群任意节点可以读和写 4）自动身份控制,失败节点自动脱离集群 5）自动节点接入 6）真正的基于”行”级别和ID检查的并行复制 7）无单点故障,易扩展 openstack doc 中提到在用HAproxy做galara cluster 的负载均衡时，因为该集群不支持跨节点对表加锁，也就是说如果OpenStack 某组件有两个会话分布在两个节点上同时写入某一条数据，会出现其中一个会话遇到死锁的情况。可以参考Understanding reservations, concurrency, and locking in Nova,讲解的比较清晰。 解决方案上文作者就提出了一种方式，楼主看了下最新版代码已经没有with_lockmode(‘update’) 语句。 安装配置参考Database (Galera Cluster) for high availability 网络控制节点 HA这里说的网络控制节点是为了方便描述，并不是neutron的所有服务都在该节点，看下图， 可以认为neutron-service在控制节点，L3 agent,DHCP agent, Metadata agent,L2 agent等所在节点为网络控制节点，接下来就是针对具体的服务讨论HA 的方案，通常来说，我们只需考虑L3 agent,DHCP agent 这两个服务的HA即可，L2 agent只在所在的网络或者计算节点上提供服务，不需要HA。neutron-metadata-agent需要和 neutron-ns-metadata-proxy 通过soket 通信，可以在所有 neutron network 节点上都运行该 agent，只有 virtual router 所在的L3 Agent 上的 neutron-metadata-agent 才起作用，别的都standby。 L3 agent HAL3 agent 的HA ,官方给出的方案有两种，一种是利用VRRP协议（虚拟路由冗余协议）实现，另一种是DVR(分布式虚拟路由)实现。关于VRRP协议，可以参考Neutron L3 Agent HA 之 虚拟路由冗余协议（VRRP）, 具体配置以及网络的连接情况直接参考官方给的guide 同样，关于DVR的资料参考Neutron 分布式虚拟路由（Neutron Distributed Virtual Routing）,以及官方guide DHCP agent HADHCP协议本身支持多个DHCP服务器，只需修改配置，为每个租户网络创建多个DHCP agent,即可实现HA。 存储控制节点 HAcinder-volume的HA A/A方案目前还未实现，只能采取pacemaker 实现H/A，不过，随着openstack Tooz 项目的开发完善，cinder-volume的A/A方案也渐渐明朗，就是采用openstack Tooz 项目实现的分布式锁来实现。详细信息参考cinder-volume如何实现AA高可用 计算节点 HA包括计算节点和虚拟机的HA,社区从2016年9月开始一直致力于一个虚拟机HA的统一方案，详细参考igh Availability for Virtual Machines.目前还是处于开发阶段。 业界目前使用的方案大致有以下几种： controller节点与compute节点通信（ping等），检查nova 服务运行状态，对于有问题的节点进行简单粗暴的evacuate. Pacemaker-remote： 突破Corosync的集群规模限制，参考RDO的方案 集中式检查 分布式健康检查，参考分布式健康检查：实现OpenStack计算节点高可用 参考文章openstack doc 世民谈云计算 Understanding reservations, concurrency, and locking in Nova Fuel openstack HA guide RabbitMQ分布式集群架构和高可用性（HA） Highly Available (Mirrored) Queues END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux-- 常见的linux 服务器性能监控工具]]></title>
      <url>%2F2017%2F04%2F06%2Flinux-performance-monitoring-tool%2F</url>
      <content type="text"><![CDATA[引出性能监控是服务器运维非常重要的一部分，本文记录非常好用的几个性能监控工具，算是一个小的总结。本文组织方式是按cpu,mem(内存)，网络，磁盘io等具体性能监控的方向分类，末尾介绍一些相对大而全，涵盖各方面的企业级监控工具。。 cpu/mem 监控工具top这应该是第一个想到的性能监控命令，示例截图如下： 我们关注得点应该包括：cpu,mem负载，load-average，以及占用内存或cpu最多的进程。该命令还有一些交互的子命令，比如 “c” ,是按照CPU 占用排序，”m”是按照内存占用排序等。 sar该命令号称系统监控的瑞士军刀，目前Linux上最为全面的系统性能分析工具之一，可以从14个大方面对系统的活动进行报告，包括文件的读写情况、系统调用的使用情况、串口、CPU效率、内存使用状况、进程活动及IPC有关的活动等，使用也是较为复杂。sar 默认显示的是从零点开始每隔十分钟到现在的CPU情况，如果是查看之前的报告，需要指定日志报告，sar -f /var/log/sysstat/sa25 。 解释下各列的指标： %user 用户模式下消耗的CPU时间的比例； %nice 通过nice改变了进程调度优先级的进程，在用户模式下消耗的CPU时间的比例 %system 系统模式下消耗的CPU时间的比例； %iowait CPU等待磁盘I/O导致空闲状态消耗的时间比例； %steal 利用Xen等操作系统虚拟化技术，等待其它虚拟CPU计算占用的时间比例； %idle CPU空闲时间比例； 查看 内存使用情况：sar -r 查看内存页面交换发生状况：sar -W 查看带宽信息：sar -n DEV 要判断系统瓶颈问题，有时需几个 sar 命令选项结合起来； 怀疑CPU存在瓶颈，可用 sar -u 和 sar -q 等来查看 怀疑内存存在瓶颈，可用sar -B、sar -r 和 sar -W 等来查看 怀疑I/O存在瓶颈，可用 sar -b、sar -u 和 sar -d 等来查看 更详细的命令参数可以参考10 Useful Sar (Sysstat) Examples for UNIX / Linux Performance Monitoring 网络监控工具netstat该命令会显示各种与网络相关的信息，比如网络连接，路由表，接口状态等。 解释下主要有两个部分： Active Internet connections，称为源TCP连接，其中”Recv-Q”和”Send-Q”指接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积 Active UNIX domain sockets，称为有源Unix域套接口(跟网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议,RefCnt表示连接到本套接口上的进程号,Types显示套接口的类型,State显示套接口当前的状态,Path表示连接到套接口的其它进程使用的路径名。常用参数如下： a (all)显示所有选项，默认不显示LISTEN相关 t (tcp)仅显示tcp相关选项 u (udp)仅显示udp相关选项 n 拒绝显示别名，能显示数字的全部转化成数字。 l 仅列出有在 Listen (监听) 的服務状态 iptraf非常实用的tcp/udp网络监控工具，有一个非常简洁的界面，常用的功能包括： IP流量监控器，用来显示网络中的IP流量变化信息。包括TCP标识信息、包以及字节计数，ICMP细节，OSPF包类型。 简单的和详细的接口统计数据，包括IP、TCP、UDP、ICMP、非IP以及其他的IP包计数、IP校验和错误，接口活动、包大小计数。 TCP和UDP服务监控器，能够显示常见的TCP和UDP应用端口上发送的和接收的包的数量。局域网数据统计模块，能够发现在线的主机，并显示其上的数据活动统计信息。 TCP、UDP、及其他协议的显示过滤器，允许你只查看感兴趣的流量。 以下为一个查看网卡统计信息的界面： 大而全的企业级性能监控工具目前开源的企业级性能监控工具应用比较广泛的应该属Nagios 与Zabbix了，关于两者对比，可以参考： Zabbix vs Nagios vs PandoraFMS: an in depth comparison 开源监控系统中 Zabbix 和 Nagios 哪个更好？ 参考文章10 Useful Sar (Sysstat) Examples for UNIX / Linux Performance Monitoring 你需要知道的16个Linux服务器监控命令 linuxtools-rst iptraf：一个实用的TCP/UDP网络监控工具 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- Open vSwitch 安装以及常用操作]]></title>
      <url>%2F2017%2F03%2F16%2Fopenvswitch-quick-start%2F</url>
      <content type="text"><![CDATA[引出openvswitch,简称ovs, 是一个开源的分布式虚拟多层交换机，随着云计算的崛起，它逐渐成为网络虚拟化的基石。在openstack 中，默认也是采用ovs作为二层网络实现的组件。本篇博客即是对ovs安装，以及框架，常用命令的一个记录。 在CentOS 7 上安裝 Open vSwitch确定linux核心版本,以安装对应版本OVS1$ uname -r 根据下图查看需要安装哪一个版本，一般来说，如果kernel支持的话，最好下载2.5.1版，也就是目前的LTS(Long Term Support)版。 安装 toolchain 以及一些依赖包目前ovs安装还是需要自己编译，所以需安装toolchain。 123$ yum groupinstall "Development Tools"$ yum install openssl-devel wget kernel-devel 下载源文件并打包为rpm包123456789$ wget http://openvswitch.org/releases/openvswitch-2.5.1.tar.gz #下载$ tar xzvf openvswitch-2.5.1.tar.gz #解压缩$ mkdir -p ~/rpmbuild/SOURCES &amp; cp openvswitch-2.5.1.tar.gz ~/rpmbuild/SOURCES/ #创建一个打包目录并复制源文件$ sed 's/openvswitch-kmod, //g' openvswitch-2.5.1/rhel/openvswitch.spec &gt; openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec # 修改spec文件$ rpmbuild -bb --nocheck ~/openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec # 打包 完成后我们在~/mbuild/RPMS/x86_64/目录下有两个rpm包，其中，openvswitch-2.5.1-1.x86_64.rpm就是我们需要的rpm包。 安装rpm包1234567$ yum localinstall /root/rpmbuild/RPMS/x86_64/openvswitch-2.5.1-1.x86_64.rpm$ ovs-vsctl -V #查安装是否成功$ systemctl start openvswitch.service #启动ovs service（即守护进程）$ chkconfig openvswitch on #设置开机自启动 OpenvSwitch的架构以及基本概念ovs架构 主要由三个组件组成： ovs-vswitchd ：ovs的守护进程。 ovsdb-server ：ovs的轻量数据库服务。 openvswitch_mod.ko ：ovs的内核模块，主要是利用datapath将flow的match结果缓存起来。 ovs 基本概念因为ovs最常用到的场景是neutron，所以这里以neutron vxlan为例子讲下，下图就是一个neutron vxlan的示意图： bridge: ovs中的网桥可以理解为现实世界中的物理交换机，作用就是根据一定流规则（如openflow），把从某个端口(port)收到的数据包转发到另一个或多个端口。图中br-int与br-tun就是两个bridge。 port: ovs 中的port是收发数据包的单元,每个端口都附属于某一个bridge。图中qvoxxx，以及patch-tun,patch-int等都是port。 interface: 接口是ovs与外部交换数据包的组件。一个接口就是操作系统的一块网卡，这块网卡可能是ovs生成的虚拟网卡，也可能是物理网卡挂载在ovs上，也可能是操作系统的虚拟网卡（TUN/TAP）挂载在ovs上。 flowtable: 上文讲过，ovs会根据流表进行包的转发，最常用的就是openflow协议，下图为openflow的流表匹配顺序：首先按照从小到大的顺序匹配流表，表内按照优先级匹配表项，示例参见openstack– neutron 二/三层网络实现探究。 OpenvSwitch的常用命令ovs的操作命令大致有如下四种： ovs-vsctl用于控制ovs db ovs-ofctl用于管理OpenFlow switch 的 flow ovs-dpctl用于管理ovs的datapath ovs-appctl用于查询和管理ovs daemon ovs的常用子命令如下,直接引用自OVS常用操作总结： ovs-dpctl show -s ovs-ofctl show, dump-ports, dump-flows, add-flow, mod-flows, del-flows ovsdb-tools show-log -m ovs-vsctl show 显示数据库内容 关于桥的操作 add-br, list-br, del-br, br-exists. 关于port的操作 list-ports, add-port, del-port, add-bond, port-to-br. 关于interface的操作 list-ifaces, iface-to-br ovs-vsctl list/set/get/add/remove/clear/destroy table record column [value], 常见的表有bridge, controller,interface,mirror,netflow,open_vswitch,port,qos,queue,ssl,sflow. ovs-appctl list-commands, fdb/show, qos/show OpenvSwitch实践参考基于 Open vSwitch 的 OpenFlow 实践 参考文章CentOS 7 安裝 Open vSwitch Open vSwitch and OpenStack Neutron troubleshooting ovs wiki Open vSwitch的ovs-vsctl命令详解 OVS常用操作总结 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[neutron-- 对neutron一些常见网络问题的troubleshooting]]></title>
      <url>%2F2017%2F03%2F13%2Fneutron-troubleshooting-common-problems%2F</url>
      <content type="text"><![CDATA[引出一直对neutron网络的troubleshooting 感到头疼，今天偶然翻到一篇博客，感觉写的非常棒，Openstack Neutron: troubleshooting and solving common problems,这篇博客也是两个教学视频的总结，顺便看了一下视频，感觉内容差不多就没看完，链接在后面。 本篇博客算是对上述博客以及视频的一个翻译（没有严格翻译）与整理，以及自己补充的一些东西。主要解决四类常见的网络问题： 无法用private ip ping/ssh 通虚拟机 虚拟机无法连通外部网络 虚拟机无法连通 metadata server VIF plugging timeout (vif 插件超时) 常见问题分类问题的原因通常来说分为以下两种： 配置错误这是最常见的错误原因，因为配置文件的错误配置导致失败。这里的配置文件不止是neutron的配置文件，还有相关的nova,cinder,各种agent甚至物理机网络的配置。需要注意的是，宿主机网络的配置错误会严重影响neutron 的使用，因为所有的网络包最终都是通过物理网络来传输。所以，对neutron 进行trouble shooting之前，首先确定宿主机之间是可以联通的。 代码出现bug出现这种情况的话，一般在neutron的launchpad 会有对应的bug report。如果没有的话，那就自己提一个，会有社区的developer进行 bug fixed。 第一类问题：无法用private ip ping/ssh 通虚拟机在解决这个问题之前，我们首先要了解一个虚拟机是如何获取IP的。 在neutron 中，有一个agent叫做 DHCP agent，它通过 RPC 跟 neutron-server 通信，它会为每一个 network 创建一个dhcp namaspace，以实现网络的隔离。在这个namespace里，有一个dnsmasq的进程，它是真正实现分配IP 的服务。 关于这部分的具体代码分析，可以参考Neutron 理解（5）：Neutron 是如何向 Nova 虚机分配固定IP地址的 （How Neutron Allocates Fixed IPs to Nova Instance）接下来我们看下packets 的flow流向，以便更精准的定位问题,因为二层网络的实现方式不一样——————openvswitch方式和Linux bridge方式，所以流向也不尽相同，先看下ovs的流向。 流程图一目了然，原文其实做了很多解释，这里就不多说了，如果不清楚的可以参看原文，或者看我之前的一篇openstack– neutron 二/三层网络实现探究 同样，linux bridge实现的二层网络，流向如下： ok,接下来开始debug: 首先利用nova list 确定虚拟机是 active状态,如果虚拟机是failure，那么可以用 nova show 查看failure原因，以及查看 nova日志。 确定虚拟机是running 的，接下来就要进行网络的分析，首先需要确定两件事：一是物理网络是没问题的，二是虚拟机的security group是允许ping/ssh的。 接下来查看port绑定是否成功，分为在虚拟机上的port绑定和在DHCP/router上的绑定。一般来说虚拟机上的port绑定是比较容易被发现的，因为在boot虚拟机的过程中一旦绑定不成功就会报错。而在DHCP/router上的绑定，因为是异步过程所以不容易被发现（当创建一个port的时候即使最终port binding失败，但是也会返回创建成功）。这就需要我们使用 neutron port-show PORT_ID 查看对应port是否绑定成功，不成功的话会如下图所示： 一般来说，port绑定不成功的话有两个原因：第一个原因是OVS agent 挂掉了，可以使用 neutron agent-list查看，OVS agent挂掉的另一个显著特征是 ovs-vsctl show | grep tap -A 3 发现br-int网桥的tap设备没有vlan tag. 第二个原因就是neutron agent 或者neutron server配置错误。 接下来看一下虚拟机是否获取到IP。通过 vnc-console 进入虚拟机，执行ip a 命令，如果没有ip的话，首先查看neutron dhcp agent是否挂掉，还是用neutron agent-list命令查看。如果agent没有挂掉的话，再去查看下对应network的dnsmasq服务是否挂掉，使用该命令： ps -ef | grep dnsmasq | grep Network_Id 。再去查看一下对应network的dhcp host文件，是否有对应虚拟机的mac与ip 记录，该文件的位置为：/var/lib/neutron/dhcp/Network_ID/host。如果还是一切正常，那么去查看dhcp-agent 的日志。同时，我们要保证宿主机与虚拟机之间网络是可以联通的，可以通过手动设置虚拟机ip的方法进行验证下。 如果还是没有解决问题，那就只有一个方法了，使用tcpdump 抓包吧。 第二类问题：虚拟机无法连通外部网络解决这类问题，首先要弄懂L3 agent的工作原理。说白了，就是利用namespace+iptables实现的，参考openstack– neutron 二/三层网络实现探究。 还是看下包流向，因为2层网络的实现方式不同，流向也不尽相同。首先看下ovs方式的流向： 再看下linux bridge的方式，图示为从外部网络ping 虚拟机floating ip过程。 debug步骤如下： 查看security group 是否允许ping。可以的话，首先要保证宿主机与private ip之间是可以ping通的，如果两者ping不通，那么使用floating ip也是绝壁不可能ping通的。 接下来从两个角度入手：从虚拟机到网络节点的对应router是否联通，从router到external net是否联通。 从虚拟机到router:通过 vnc-console进入虚拟机，查看ip a 是否获取到ip,查看路由信息：route -n，查看能否ping 通默认网关（即路由器IP） 从router到external net：查看在router namespace内能否ping通外部网络：ip netns exec qrouter-078766fe-c4b6-4a14-82fa-e7f85e26c248 ping www.baidu.com 。查看router namespace能否ping通 floating ip: ip netns exec qrouter-078766fe-c4b6-4a14-82fa-e7f85e26c248 ping Floating-IP .这看起来可能有点傻，因为floating ip其实就在这个namespace中，不过这会让我们对问题的严重性有一个认识。 如果还是没有找到问题，就去查看l3-agent 日志。 第三类问题：虚拟机无法连通 metadata server还是先要搞明白metadata server的原理。metadata 获取有两种方式，一种是config drive，一种是metadata server。这里就是讨论metadata server的方式，metadata server 的路由方式有两种：如果虚拟机与router联通的话，就通过router路由，如果虚拟机没有router直连的话，就通过对应dhcp namespace路由。参考openstack– openstack instance metadata 服务机制探索。 首先看下通过路由器路由的work-flow: 注意：metdata proxy 是由l3-agent实现，它会在指定端口监听（默认 9697），接收到request后，会增加虚拟机ip,router id到request header并转发到metadata agent。 再看下没有路由器直连（isolated-network）的情况: 注意：要想开启这一功能，需要在dhcp-agent配置文件中设置：enable_isolated_metadata = True. 接下来开始debug: 首先查看metadata-agent是否挂掉：neutron agent-list 还是两个角度：从虚拟机到router/dhcp namespace，从router/dhcp namespace 到nova-api-metadata。 从虚拟机到router/dhcp namespace：ping一下查看是否连通。 查看router/dhcp namespace 是否有metadata-proxy 在监听：ip netns exec qrouter-f6396cfe-2ac9-4d6a-9437-eb8e7d26c776 ps -ef | grep metadata-proxy 。如果这里出现错误的话，可以去查看下metadata-agent 的日志，以及对应 namespace的neutron-ns-metadata-proxy的日志。 从router/dhcp namespace 到nova-api-metadata：ip netns exec qrouter-f6396cfe-2ac9-4d6a-9437-eb8e7d26c776 ping Metadata-Server IP 终极大招：tcpdump 第四类问题： VIF plugging timeout这类问题通常出现在boot虚拟机的时候，跟L2 agent(ovs/bridge agent)有关。 workflow如下： 当nova 发出allocate_network 请求时，会设置一个默认5分钟的等待时间，如果5分钟过后还没有收到neutron 回复，就会报 VIF plugging timeout 这个错误。 debug步骤： 查看对应host的nova-computer日志，以及openvswitch-agent日志。 查看网络节点neutron-server日志。 如果是做压测，在Nova的配置文件中，可以适当调大这个vif_plugging_timeout 时间，或增加rpc_thread_pool_size &amp; rpc_conn_pool_size 。 原文还列举了一些常用的网络调试命令，因为之前总结过，openstack–openstack 常用网络调试命令,不再赘述。 参考文章Openstack Neutron: troubleshooting and solving common problems I Can’t Ping My VM! Learn How to Debug Neutron and Solve Common Problems OpenStack Neutron Troubleshooting Neutron 理解（5）：Neutron 是如何向 Nova 虚机分配固定IP地址的 （How Neutron Allocates Fixed IPs to Nova Instance） END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux-- 利用LVM 对虚拟机进行手动扩容]]></title>
      <url>%2F2017%2F03%2F10%2FLVM-command%2F</url>
      <content type="text"><![CDATA[引出一般来说，如果openstck镜像中安装了growpart(之前的版本叫做growroot),在创建虚拟机的时候就会实现自动扩容。但是今天碰到几个虚拟机没有实现自动扩容的情况，需要手动进行扩容，用到lvm的一些知识，这里记录一下。 关于LVM的一些知识LVM利用Linux内核的device-mapper来实现存储系统的虚拟化（系统分区独立于底层硬件）。通过LVM，你可以实现存储空间的抽象化并在上面建立虚拟分区，可以更简便地扩大和缩小分区，可以增删分区时无需担心某个硬盘上没有足够的连续空间。 简单来说：LVM是Linux环境中对磁盘分区进行管理的一种机制，是建立在硬盘和分区之上、文件系统之下的一个逻辑层，可提高磁盘分区管理的灵活性。 LVM的基本组成块如下： 物理卷Physical volume (PV)：可以在上面建立卷组的媒介，可以是硬盘分区，也可以是硬盘本身或者回环文件（loopback file）。物理卷包括一个特殊的header，其余部分被切割为一块块物理区域（physical extents）。 卷组Volume group (VG)：将一组物理卷收集为一个管理单元。 逻辑卷Logical volume (LV)：虚拟分区，由物理区域（physical extents）组成。 对虚拟机进行手动扩容注：本文测试为实验环境，直接利用workstation 开启虚拟机，扩展磁盘，最后实现手动扩展。 首先，确认磁盘使用lvm以及确定磁盘大小： 可以看到磁盘/dev/sda大小总共约为53G，/dev/sda2是已经加入卷组的一个物理卷。 /dev/mapper/cl-root 与/dev/mapper/cl-swap 设备其实是逻辑卷。 接下来在workstation面板进行磁盘扩展： 查看磁盘情况： 可以看到磁盘已经扩展为60G+了，但是磁盘分区还是两个且大小没变，而且挂载root的/dev/mapper/cl-root依然是50G,所以接下来我们需要做的就是将多出来的10G空间进行磁盘分区并最终扩展到逻辑卷dev/mapper/cl-root。 首先磁盘分区并将该分区设置为LVM格式。 重启机器（或者不重启，执行 partprobe 命令让内核更新分区表），将/dev/sda3创建物理卷，并将该物理卷加入卷组。 扩展逻辑卷并将对应的文件系统扩展，如果是ext3/4文件系统用resize2fs 命令。 扩展成功。 参考文章LVM WIKI LVM逻辑卷管理配置小结 Linux LVM逻辑卷配置过程详解 How to Increase the size of a Linux LVM by expanding the virtual machine disk END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- 如何升级现有镜像]]></title>
      <url>%2F2017%2F03%2F07%2Fupdate-your-openstack-image%2F</url>
      <content type="text"><![CDATA[引出公司云平台有些镜像的密码注入出了点问题，问题的原因初步诊断为镜像问题，需要对这些镜像进行cloudinit/cloudbase的升级操作。对于已知密码的镜像，可以直接开启一个虚拟机，完成升级后，对该虚拟机做snapshot 操作（nova image-create）即可。对于不知道密码的镜像，经过一番搜索后，可以通过将镜像挂载到本地宿主机，然后chroot的方法进行升级,后来又从官网了解到利用libguestfish 的方法。以下为一些简单记录。 利用 nova image-create 升级镜像 利用镜像开启一个虚拟机 进入虚拟机做升级操作，并删除/var/lib/cloud 下的文件，windows需删除LocalScripts/下的文件。 完成操作后，退出虚拟机并对该虚拟机做snapshot操作。 镜像完成。 利用 本地挂载的方法升级镜像如果忘记密码的话我们可以使用该方法进行镜像的升级，不过因为glance后端存储方式的不同，挂载到本地HOST的方法也不同，下文分别以本地存储，ceph存储讲解。 glance 存储为本地存储镜像格式为raw格式且镜像文件系统中使没有用LVM1234567891011$ fdisk -lu cirros-0.3.0-x86_64-disk.raw # 查看该镜像的分区情况以及是否使用LVM$ kpartx -av cirros-0.3.0-x86_64-disk.raw #读取镜像的分区表，然后生成代表相应分区的设备/dev/loop0 ，/dev/loop1等等$ mount /dev/mapper/loop0p1 /mnt/image # 将需要挂载的分区设备进行挂载，此时进入/mnt/image 应该就可以看到熟悉的系统文件目录了 $ cd /mnt/image &amp;&amp; cp /etc/resolv.conf etc/resolv.conf # 覆盖dns$ chroot /mnt/image bash #chroot到镜像文件中$ source /etc/profile &amp;&amp; source ~/.bashrc #初始化环境变量 然后就可以进行升级操作了，操作完成后，需要将临时文件清除，yum clean all 等。最后退出chroot 并进行unmount 操作： 12345$ exit # 退出chroot$ umount /mnt/image $ kpartx -d cirros-0.3.0-x86_64-disk.raw # 删除设备映射关系 镜像格式为raw格式且镜像文件系统中使用LVM1234567891011121314151617$ fdisk -lu centos7-x86_64.img # 查看该镜像的分区情况以及是否使用LVM$ kpartx -av centos7-x86_64.img # 读取镜像的分区表，然后生成代表相应分区的设备/dev/loop0 ，/dev/loop1等等$ pvscan # 查找lvm设备并记下vg的名字示例为centos$ vgchange -ay centos # 激活vg中的lvm$ lvs # 查看lvm设备列表$ mount /dev/centos/root /media/ #挂载对应的lvm$ cd /media/ &amp;&amp; cp /etc/resolv.conf etc/resolv.conf$ chroot /mnt/image bash #chroot到镜像文件中$ source /etc/profile &amp;&amp; source ~/.bashrc #初始化环境变量 退出chroot 并进行unmount 操作如下： 1234567$ exit # 退出chroot$ umount /media/ $ vgchange -an centos # deactive vg 中的lvm$ kpartx -d centos7-x86_64.img # 删除设备映射关系 如下为一个完整实例（省略chroot操作）： 镜像格式为qcow2格式qcow2 格式的镜像需要使用qemu自带的一个工具qemu-nbd，该工具会生成一个nbd(network block device)，然后将镜像映射到该nbd,便可以像普通block设备一样挂载了。 1234567891011$ modinfo nbd # 查看系统kernel是否支持nbd模块$ modprobe nbd max_part=16 # 加载 nbd模块$ lsmod | grep nbd # 查看nbd模块是否加载$ qemu-nbd -c /dev/nbd0 centos7.qcow2 # 将qcow2镜像映射为网络块设备(nbd)$ ll /dev/nbd0* # 查看映射情况$ mount /dev/nbd0p1 /media/ # 挂在对应的nbd设备 卸载的步骤如下： 123$ umount /media$ qemu-nbd -d /dev/nbd0 如果该qcow2镜像使用LVM的话，前半段挂载nbd设备，后半段参照raw格式。 glance 存储为ceph存储首先需要知道ceph作为glance存储后端的时候，openstack会充分利用ceph 的分层clone的特性来支持快速创建虚拟机。镜像会首先生成一个以snap结尾命名的快照，以后每次开启一个虚拟机都会clone该snapshot。该snapshot是protected，所以无法更改，而如果直接修改image的话，无法同步对应的snapshot。所以最稳妥的方法是先复制一个rbd image，然后修改这个复制的image，修改成功后覆盖或直接删除之前的image。 1234567$ rbd cp $POOL/$&#123;IMAGE_ID&#125; $POOL/$&#123;IMAGE_ID&#125;_copy # 复制image （允许跨pool）$ rbd map $POOL/$&#123;IMAGE_ID&#125;_copy # 挂载rbd镜像到本地（注意kernel的支持）$ mount /dev/rbd0p1 /mnt # 挂载对映的rbd到挂载点(rbd0p1 只是示例，依据真实情况填写)$ ll /mnt 之后就可以执行chroot 以及镜像的更新操作了。之后的umount操作如下：123$ umount -r -l /mnt/ $ rbd unmap /dev/rbd0 #从rbd中卸载 再然后就是更新glance了，大致步骤如下： 123456789101112131415$ glance image-create --name NewImage # 创建image，占个坑，不上传，得到new imageID $ rbd mv $POOL/$&#123;IMAGE_ID&#125;_copy $POOL/$&#123;NEW_IMAGE_ID&#125; # rbd改名$ rbd --pool=$POOL --image=$&#123;NEW_IMAGE_ID&#125; --snap=snap snap create # 创建snapshot$ rbd --pool=$POOL --image=$&#123;NEW_IMAGE_ID&#125; --snap=snap snap protect #对snapshot做保护$ glance image-update --name="$DISPLAY_NAME" --disk-format=raw --container-format=bare --is-public=True $&#123;NEW_IMAGE_ID&#125; # 更新glance 元数据$ glance image-update --property image_meta="$image_meta" $&#123;NEW_IMAGE_ID&#125;$ glance image-update --property hw_qemu_guest_agent=yes $&#123;NEW_IMAGE_ID&#125;$ glance image-update --location rbd://$FSID/$POOL/$NEW_IMAGE_ID/snap $&#123;NEW_IMAGE_ID&#125; #更新镜像的地址 使用 libguestfs 修改虚拟机镜像libguestfs项目介绍libguestfs 其实是一系列工具组成的，目的就是为了连接并修改本地虚拟机镜像。可以实现的功能有很多，包括：修改镜像内文件，脚本，查看镜像文件系统容量使用情况，物理机与虚拟镜像之间的文件传递，备份，克隆，甚至新建虚拟机实例，格式化磁盘,修改磁盘大小等等。参考libguestfs 部分示例安装比较简单，如果是rh/centos系列，直接yum安装： 1$ yum install -y libguestfs-tools 下面主要分三部分guestfish，guestmount 以及virt-* tools 讲解 guestfish 讲解与示例通俗讲，guestfish 的作用就是修改镜像内的文件。它并不会将镜像文件系统直接挂载到本地，而是提供了一个类似shell的交互接口允许你查看，编辑，删除文件。 示例： 1$ guestfish --rw -a centos63.raw # 进入guestfish shell ,Mount the image in read-write mode as root 进入之后 ，先执行run ，会创建一个虚拟机实例。 如果出现错误的话，可以 export LIBGUESTFS_DEBUG=1 打开debug模式查找错误。或者利用 libguestfs-test-tool 命令测试一下。接下来示例编辑网卡配置： 1234567&gt;&lt;fs&gt; list-filesystems #列出可挂载的文件系统&gt;&lt;fs&gt; mount /dev/vg_centosbase/lv_root / #挂载&gt;&lt;fs&gt; edit /etc/sysconfig/network-scripts/ifcfg-eth0 #编辑文件&gt;&lt;fs&gt; exit #退出 guestmount 讲解与示例guestmount 可以实现直接将镜像的文件系统挂载到本地。示例如下： 12345$ guestmount -a centos63_desktop.qcow2 -i --rw /mnt # i 参数表示自动查找root分区并挂载$ rpm -qa --dbpath /mnt/var/lib/rpm #示例查看rpm包$ umount /mnt 若umount失败（通常报错 device is busy），可以使用lazy umount，加 l 参数，即： 1$ umount /mnt -l virt-* tools 讲解与示例大概以下几种工具： virt-edit : 修改镜像内的文件。 virt-df : 查看镜像磁盘占用情况。 virt-resize : resize 镜像。 virt-sysprep : 准备发布镜像前的一系列操作（比如删除SSH HOST，删除mac地址，删除user 信息） virt-sparsify : 镜像稀疏（消除镜像空洞） virt-p2v : 物理机转换为虚拟机（kvm）. virt-v2v : xen或vmware镜像转换为kvm镜像。 几个示例（来自openstack doc）: 第一个示例是修改镜像文件： 12345$ virsh shutdown instance-000000e1$ virt-edit -d instance-000000e1 /etc/shadow # d means domain$ virsh start instance-000000e1 第二个示例是 resize image： 12345$ virt-filesystems --long --parts --blkdevs -h -a /data/images/win2012.qcow2 #查看分区$ qemu-img create -f qcow2 /data/images/win2012-50gb.qcow2 50G #新建一块qcow2 image 空间$ virt-resize --expand /dev/sda2 /data/images/win2012.qcow2 /data/images/win2012-50gb.qcow2 # 扩展空间 更新 2017-03-23 增加 libguestfs 修改虚拟机镜像的方法。 2017-03-34 增加 libguestfs 部分示例 参考文章Modify images 在线升级glance镜像技巧 挂载raw和qcow2格式的KVM硬盘镜像 使用Yum快速更新升级CentOS内核 如何挂载一个镜像文件(HOW TO MOUNT AN IMAGE FILE) 挂载虚拟机镜像文件里的 LVM 逻辑分区 libguestfs kvm虚拟化小结（六）libguestfs-tools libguestfs详解 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph-- 关于ceph rbd 的一些记录]]></title>
      <url>%2F2017%2F03%2F03%2Fsomething-about-ceph-rbd%2F</url>
      <content type="text"><![CDATA[引出ceph 的中文文档做的真是非常棒，比起openstack的中文文档来说，不仅全，而且更新也比较及时，openstack的文档还是直接查阅英文版的比较好。 本文是在查阅ceph官网文档过程中，联系实际操作做出的一些思考与记录。 ceph rbd 的简单说明官网对rbd的定义是这样的：Ceph 块设备是精简配置的、大小可调且将数据条带化存储到集群内的多个 OSD。 ceph rbd是基于 rados来做的，也就是说，一个 rbd image 首先会被分成多个等大的 object，这些object再 根据 crush map 存储到对应的3个（或n个，n为奇数）OSD中，理解这一点对于理解 rbd 中的 stripe-unit和 stripe-count 有帮助。所以如果考虑深层的话需要结合RADOS 的多种能力，如快照、复制和一致性。 对于openstack来说，通常是通过libvirt 来控制磁盘的虚拟化(比如qemu),再通过qemu来与rbd打交道，如下图： qemu会调用librbd 实现rbd的增删改查等一系列操作，当然也有配套的 qemu命令，详见QEMU 与块设备 libvirt 是一套虚拟机抽象层，它隐藏了底层多种虚拟化的具体实现，提供给client一套统一的通用API和shell接口。关于如何用 virsh 新建一个挂载rbd 的vm，可以参考通过 LIBVIRT 使用 CEPH RBD 关于openstack 与ceph 的整合与配置，参考块设备与 OPENSTACK rbd 常用操作以及命令rbd 的常用操作大致包括三个部分，一个是 常见的增删改查，一个是 针对 snapshot的操作，再加一个map/unmap操作。 rbd的增删改查12345678910$ rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125; # 创建 rbd$ rbd ls &#123;poolname&#125; # 罗列 rbd$ rbd info &#123;pool-name&#125;/&#123;image-name&#125; # 查看某一rbd具体信息$ rbd resize --size 2048 foo (to increase) # 增加尺寸$ rbd resize --size 2048 foo --allow-shrink (to decrease) # 减小尺寸$ rbd rm &#123;pool-name&#125;/&#123;image-name&#125; # 删除rbd镜像 rbd snapshot 相关操作快照是映像在某个特定时间点的一份只读副本,ceph 的快照有一个比较高级的特性就是分层clone，在openstack 中会实现非常快的创建VM，下文会详细讲解。 rbd snapshot 的基本操作123456789$ rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 创建快照$ rbd snap ls &#123;pool-name&#125;/&#123;image-name&#125; #罗列 某个镜像的快照$ rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 快照回滚$ rbd snap rm &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 删除某一快照$ rbd snap purge &#123;pool-name&#125;/&#123;image-name&#125; # 清除某一镜像的所有快照 ceph 快照的分层 cloneCeph 支持为某一rbd snapshot创建很多个写时复制（ COW ）克隆。所谓写时复制，就是执行 clone命令后，并没有立即执行clone操作，而是引用原文件，当发生写入操作时才真正执行clone操作。 注意： clone只能用于 protected 的snapshot,且仅支持克隆 format 2 的映像（即用 rbd create –image-format 2 创建的）。 分层快照使得 Ceph 块设备客户端可以很快地创建映像，这样便能实现openstack 中非常快的创建VM,而且，随着 openstack 的Nova image-create与ceph的snapshot的无缝连接，镜像的制作更快捷方便了，关于这部分，可以参考基于Ceph RBD的OpenStack Nova快照 关于 rbd snapshot 的内在原理 ，参考ceph rbd快照原理解析 clone 的 步骤如下： 相关命令如下： 123456789$ rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 保护快照$ rbd clone &#123;pool-name&#125;/&#123;parent-image&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;child-image-name&#125; #克隆快照$ rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 删除快照前，必须先取消保护。此外，不可以删除被克隆映像引用的快照，所以在删除快照前，必须先拍平（ flatten ）此快照的各个克隆。$ rbd children &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 罗列快照的子孙$ rbd flatten &#123;pool-name&#125;/&#123;image-name&#125; # 拍平克隆映像 rbd 与内核相关的map/unmap注意：由于Linux kernel支持rbd的版本较高，一般需要升级到较高版本（建议升级到3.11+） 升级内核参考使用Yum快速更新升级CentOS内核 12345$ rbd map &#123;pool-name&#125;/&#123;image-name&#125; --id &#123;user-name&#125; # 映射块设备$ rbd showmapped # 查看已映射块设备$ rbd unmap /dev/rbd/&#123;poolname&#125;/&#123;imagename&#125; # 取消块设备映射 参考文章CEPH 块设备 基于Ceph RBD的OpenStack Nova快照 ceph rbd快照原理解析 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph-- 使用国内yum源搭建ceph集群]]></title>
      <url>%2F2017%2F02%2F28%2Finstall-ceph-with-domestic-yum-source%2F</url>
      <content type="text"><![CDATA[引出初衷是自己搭建一套ceph的实验集群，按照官方的quick start,按理说应该是分分钟的事，因为GFW的原因，搞得忙活了一上午，再加上心情烦躁,中间出现了各种差错，这里记录一下。 系统预检并安装ceph-deploy准备集群系统，网络设置，SSH无密码登录等设置略过，详细参考PREFLIGHT CHECKLIST 这里有一个地方需要注意的是：centos7 在修改HOSTNAME 的时候命令如下，无需重启： 1$ sudo hostnamectl --static set-hostname &lt;host-name&gt; 预检完成后，ceph-deploy 的安装以及 ceph在各节点的安装如果出现错误的话很大可能上是yum 源的问题，所以重点说下如何配置yum国内源。 更换国内 YUM 源 更换base源为国内163源: 1$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 更换epel源为上海交大的源(注：随着版本更迭，以下链接可能会失效，如果出现404错误，那就需要更改链接)： 123$ rpm -Uvh http://ftp.sjtu.edu.cn/fedora/epel/7/x86_64/e/epel-release-7-9.noarch.rpm$ rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 更换ceph源为国内163源 建议不用改变ceph.repo,直接设置环境变量即可，如果更改ceph.repo，那么执行ceph-deploy的时候还是需要在命令行中指定国内源url，否则又会使用官方源。 123456789$ export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-jewel/el7$ export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc$ yum clean all$ yum makecache$ yum update -y 安装并配置ceph cluster接下来的事情就跟官网的步骤差不多了，大致如下： 123456789101112131415161718# Create monitor nodeceph-deploy new node1 node2 node3# Software Installationceph-deploy install deploy node1 node2 node3# Gather keysceph-deploy mon create-initial# Ceph deploy parepare and activateceph-deploy osd prepare node1:/dev/sdb node2:/dev/sdb node3:/dev/sdbceph-deploy osd activate node1:/var/lib/ceph/osd/ceph-0 node2:/var/lib/ceph/osd/ceph-1 node3:/var/lib/ceph/osd/ceph-2# Make 3 copies by defaultecho "osd pool default size = 3" | tee -a $HOME/ceph.conf# Copy admin keys and configuration filesceph-deploy --overwrite-conf admin deploy node1 node2 node3 一个尚未解决的顽疾在公司使用国内源安装ceph的时候，出现了一个这样的错误： 1http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/ceph-mds-10.2.5-0.el7.x86_64.rpm: [Errno -1] Package does not match intended download. Suggestion: run yum --enablerepo=ceph clean metadata Package does not match intended 这个错误在我安装其他软件的时候也出现过，用尽了各种办法也没有解决该问题，但我在家里的时候却没有出现过该问题。所以推测是公司网络用了类似缓存的机制。 更新-2017-3-18找到了解决上述问题的方法，就是先直接wget下来rpm包，然后用yum localinstall 或者rpm -ivh 安装,如果出现两个包互相依赖的情况，就两个包同时安装。 123wget http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/ceph-mds-10.2.5-0.el7.x86_64.rpmyum localinstall ceph-mds-10.2.5-0.el7.x86_64.rpm 参考文章PREFLIGHT CHECKLIST STORAGE CLUSTER QUICK START 如何使用国内源部署Ceph？ CentOS7修改主机名 CentOS 7 x86_64适用的EPEL安装源 国内镜像列表 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- 如何调试openstack rest API]]></title>
      <url>%2F2017%2F02%2F22%2Fhow-to-debug-openstack-rest-api%2F</url>
      <content type="text"><![CDATA[引出在学习openstack 的过程中，基本都是这样一个过程，先是通过 dashboard 对openstack 有一个感性的认识，之后通过命令行了解openstack 更多的功能，但openstack 的内部实现还是一个黑盒。这时就需要了解 openstack 提供的最原始的 rest API, rest API 不仅是提供给终端 user使用，它内部组件的通信机制也用的rest API ，当然还有AMQP,关于 rest与AMQP 的比较见这篇文章 。本篇文章主要讲如何使用 postman 调试openstack 的rest api，之前会花一些篇幅讲一点openstack api version 的知识。 openstack api 版本演变openstack 自开源以来发展还是挺迅猛的，对应各组件的api version演变也很快，特别是 nova, cinder, keystone等组件的api。在最新的官方文档中，也就是 N 版本的openstack,目前使用的API(只列出一些常用的) 为 ： Bare Metal API v1 (microversions) Block Storage API v3 (microversions) Clustering API v1 Compute API (microversions) Container Infrastructure Management API (microversions) Identity API v3 Identity API v3 extensions Image service API v2 Messaging API v2 Networking API v2.0 Object Storage API v1 还支持或者兼容的older api为： Block Storage API v2 废弃的api 为： Block Storage API v1 Identity API v2.0 Identity admin API v2.0 Identity API v2.0 extensions Image service API v1 microversion 解释这里简单解释下 microversion,microversion的使用是为了向后兼容性，因为openstack的版本迭代比较快，当我们对openstack进行版本升级后，对应的client 或其他调用openstack api的系统因为某些原因没法升级，仍然依赖之前的API，这个时候就可以使用microversion来解决这个问题。举个栗子，比如Compute API，我们先通过一个 get请求( youropenstackIP:8774/ )获取支持的version 版本，返回如下： 123456789101112131415161718192021222324252627282930&#123; "versions": [ &#123; "id": "v2.0", "links": [ &#123; "href": "http://openstack.example.com/v2/", "rel": "self" &#125; ], "status": "SUPPORTED", "version": "", "min_version": "", "updated": "2011-01-21T11:33:21Z" &#125;, &#123; "id": "v2.1", "links": [ &#123; "href": "http://openstack.example.com/v2.1/", "rel": "self" &#125; ], "status": "CURRENT", "version": "2.42", "min_version": "2.1", "updated": "2013-07-23T11:33:21Z" &#125; ]&#125; 可以看到支持 V2 版本以及V2.1 的microversion,该 microversion，支持从 V2.1 到 V2.42的所有版本的API，但如果我们使用的话还是需要指定一个版本，在 header中加一个参数：1X-OpenStack-Nova-API-Version: 2.4 keystone v3 简介这里简单说下keystone v3，因为 keystone v3相对于 v2版本来说增加了很多新的特性，这里简单记录一下。 v2版本对用户的权限管理以每一个用户为单位，需要对每一个用户进行角色分配，并不存在一种对一组用户进行统一管理的方案，v3版本中引入了 Domain 和 Group的概念，将 tenant改为project,从而实现对一组用户进行统一管理。 直接引用OpenStack Keystone V3 简介 V3 利用 Domain 实现真正的多租户（multi-tenancy）架构，Domain 担任 Project 的高层容器。云服务的客户是 Domain 的所有者，他们可以在自己的 Domain 中创建多个 Projects、Users、Groups 和 Roles。通过引入 Domain，云服务客户可以对其拥有的多个 Project 进行统一管理，而不必再向过去那样对每一个 Project 进行单独管理。 Group 是一组 Users 的容器，可以向 Group 中添加用户，并直接给 Group 分配角色，那么在这个 Group 中的所有用户就都拥有了 Group 所拥有的角色权限。通过引入 Group 的概念，Keystone V3 实现了对用户组的管理，达到了同时管理一组用户权限的目的。这与 V2 中直接向 User/Project 指定 Role 不同，使得对云服务进行管理更加便捷。 使用 postman 调试openstack 的rest apiopenstack 常用组件默认监听端口在使用postman调试之前，先列出openstack 中常用组件默认监听的端口： 其他服务组件的默认监听端口： 使用 postman 调试 openstack rest api调试 rest api 的客户端工具有很多，官网用的是curl，但不是很直观，postman 算是颜值比较高的一款 http api调试工具。我比较习惯于使用 chrome插件版本的postman，安装完毕后，开始进行调试： 向 keystone 发送post请求获取 token 以及 service endpoint: request headers 内容如下： request body 内容如下： 返回结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; "access": &#123; "token": &#123; "issued_at": "2017-02-22T08:17:13.025922", "expires": "2017-02-23T08:17:12Z", "id": "1213f82cd41f472ba4291e75a2de8288", "tenant": &#123; "description": "Admin Tenant", "enabled": true, "id": "585324ae7e934e2eb50d84ad7aee3179", "name": "admin" &#125;, "audit_ids": [ "T7g-8ZGYTZOUN5oEI-2q5w" ] &#125;, "serviceCatalog": [ &#123; "endpoints": [ &#123; "adminURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "region": "RegionOne", "internalURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "id": "899cc922536f4038922bac198616d086", "publicURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179" &#125;, &#123; "adminURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "region": "testVlan", "internalURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "id": "2e4cf2fac2be47a4936ba19e15894030", "publicURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179" &#125; ], "endpoints_links": [], "type": "compute", "name": "nova" &#125;, ........................................ ], ] &#125; &#125;&#125; 可以看到 token id 即我们要的token标示，而且下一步需要的url即endpoint也已经出现在该json串中，以 compute service 的endpoint 为例，获取 虚拟机列表。 向 nova 发送 获取虚拟机列表的rest 请求，将上一步获取到的 token id填入 X-Auth-Token中。 发送该请求即可得到我们的结果： 1234567891011121314151617181920212223242526272829303132&#123; "servers": [ &#123; "id": "712ea870-9ac7-467d-a2d7-5e3caa8499a6", "links": [ &#123; "href": "http://192.168.24.161:8774/v2/ca36ea9826b04643897f19b7c03be011/servers/712ea870-9ac7-467d-a2d7-5e3caa8499a6", "rel": "self" &#125;, &#123; "href": "http://192.168.24.161:8774/ca36ea9826b04643897f19b7c03be011/servers/712ea870-9ac7-467d-a2d7-5e3caa8499a6", "rel": "bookmark" &#125; ], "name": "test" &#125;, &#123; "id": "257a2852-940e-4271-b188-fcd4d0a8c10c", "links": [ &#123; "href": "http://192.168.24.161:8774/v2/ca36ea9826b04643897f19b7c03be011/servers/257a2852-940e-4271-b188-fcd4d0a8c10c", "rel": "self" &#125;, &#123; "href": "http://192.168.24.161:8774/ca36ea9826b04643897f19b7c03be011/servers/257a2852-940e-4271-b188-fcd4d0a8c10c", "rel": "bookmark" &#125; ], "name": "testmime9" &#125; ]&#125; 其他请求类似。 参考文章OpenStack API versions penStack API Documentation OpenStack DocumentationAppendix Firewalls and default ports END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- openstack instance metadata 服务机制探索]]></title>
      <url>%2F2017%2F02%2F17%2Fopenstack-instance-metadata-discovery%2F</url>
      <content type="text"><![CDATA[引出在调试 cloud-init的时候发现在虚拟机内执行“ curl 169.254.169.254” 的时候出现“host not reachable”错误。在对 openstack instance（确切的说应该是server,因为还没有正式生成云主机） metadata 服务机制探索了一番之后，发现了错误所在，在分析该问题之前先记录下metadata 服务的原理，最后再记录下问题的解决过程。 openstack instance metadata 服务机制广义上的metadata包含很多，不光 instance 有metadata属性，image,host,aggregate等都有。在openstack中，因为instance利用metadata 属性可以做很多事情，研究的价值更高一些，所以这里只讨论 instance 的metadata。(注：这里的metadata 服务 泛指 提供userdata &amp;&amp; metadata 的服务，因为userdata与metadata只是 内容以及 虚拟机获取之后处理方式不同，虚拟机获取方式是一样的)。关于 userdata 与 metadata，这里直接引用OpenStack 的 metadata 服务机制中的一段： 在创建虚拟机的时候，用户往往需要对虚拟机进行一些配置，比如：开启一些服务、安装某些包、添加 SSH 秘钥、配置 hostname 等等。在 OpenStack 中，这些配置信息被分成两类：metadata 和 user data。Metadata 主要包括虚拟机自身的一些常用属性，如 hostname、网络配置信息、SSH 登陆秘钥等，主要的形式为键值对。而 user data 主要包括一些命令、脚本等。User data 通过文件传递，并支持多种文件格式，包括 gzip 压缩文件、shell 脚本、cloud-init 配置文件等。这些信息被获取到后被cloudinit 利用并实现各自目的。虽然 metadata 和 user data 并不相同，但是 OpenStack 向虚拟机提供这两种信息的机制是一致的，只是虚拟机在获取到信息后，对两者的处理方式不同罢了 openstack 中，虚拟机获取metadata 的信息有两种，即config drive 和 metadata restful 服务（即EC2方式）。下面分别记录下两种方式的原理： config drive 的方式所谓 config drive的方式就是将metadata信息写入虚拟机的一个特殊配置设备中，虚拟机启动的时候会挂载并读取metadata信息。要想实现这个功能，还需要宿主机和镜像满足一些条件： 宿主机满足条件： 虚拟化方式为以下几种：libvirt,xen,hyper-v,vmware. 对于 libvirt, xen, vmware的虚拟化方式，需要安装genisoimage.并且需要设置mkisofs为 程序的安装目录，如果是跟nova-compute安装在一个目录内就不用了。 如果是 hyper-v的虚拟化方式，需要用mkisofs的命令行指定mkisofs.exe 程序的绝对安装路径，且在hyperv的配置文件中设置qemu_img_cmd 为qemu-img 的命令行安装路径。 镜像满足条件： 安装了cloud-init，且最好是0.7.1+ 的版本。如果是没有安装的话，就需要自己去定制脚本了。 如果使用xen 虚拟化，需要配置 xenapi_disable_agent 为true。 在openstack中，在使用命令行创建虚拟机的时候指定 –config-drive true 即可使用config drive。如下实例（也可以用 nova command）： 12345openstack server create --config-drive true --image my-image-name \ --flavor 1 --key-name mykey --user-data ./my-user-data.txt \ --file /etc/network/interfaces=/home/myuser/instance-interfaces \ --file known_hosts=/home/myuser/.ssh/known_hosts \ --property role=webservers --property essential=false MYINSTANCE 也可以在/etc/nova/nova.conf配置文件中指定 force_config_drive = true ，那么默认就使用config drive。 如果镜像操作系统支持通过标签访问磁盘的话，可以使用如下命令查看config drive中的内容：123mkdir -p /mnt/configmount /dev/disk/by-label/config-2 /mnt/configcd /mnt/config &amp; ls -l 进入该目录后，看到的内容大致如下：1234567891011ec2/2009-04-04/meta-data.jsonec2/2009-04-04/user-dataec2/latest/meta-data.jsonec2/latest/user-dataopenstack/2012-08-10/meta_data.jsonopenstack/2012-08-10/user_dataopenstack/contentopenstack/content/0000openstack/content/0001openstack/latest/meta_data.jsonopenstack/latest/user_data 可以看出，config drive支持 openstack以及EC2的方式获取数据，官方手册建议使用opensatck的方式（即读取opensatck目录里的内容），因为EC2的目录以后可能会弃用。而且最好使用最近的版本目录读取。 metadata restful 服务的方式就是我们在虚拟机通过169.254.169.254的方式来获取metadata的方式。由以下三个组件来完成这项工作： Nova-api-metadata：运行在计算节点，启动restful服务，真正负责处理虚拟机发送来的rest 请求。从请求头中获取租户，虚拟机ID，再去数据库中查询相应的metadata信息并返回结果。 Neutron-metadata-agent：运行在网络节点，负责将接收到的获取 metadata 的请求转发给 nova-api-metadata。 Neutron-ns-metadata-proxy：由于虚拟机获取 metadata 的请求都是以路由和 DHCP 服务器作为网络出口，所以需要通过 neutron-ns-metadata-proxy 联通不同的网络命名空间，将请求在网络命名空间之间转发。 流程大概如下： 具体看下路由的过程，如果虚拟机所在的subnet连接在router上，该请求会先通过router来发送请求，如果没有router相连的话会通过对应dns 的网络空间进行传递，示例可以查看OpenStack 的 metadata 服务机制 文章末。 问题解决了解了metadata rest的原理后，解决上述问题的思路就比较明确了，首先看下虚拟机确是使用restful 服务的方式而不是config drive的方式，从网络拓扑看，虚拟机所在的子网是连接着router的，但是查看虚拟机的路由表，却发现169.152.169.254发送到了dns的网络空间，而不是router的网络空间，然后再去查看对应dns的网络空间，发现并没有监听 80 端口的 neutron-ns-metadata-proxy 服务，而在router的网络空间却有对应的路由规则。大致猜测虚拟机boot的时候没有发现对应子网有router相连，所以该虚拟机boot后会将169.152.169.254路由到对应dns，但其实该子网是有router相连的，所以neutron还是将监听的路由规则放到了router上。与router相关的服务就是neytron-l3-agent,重启该服务后，一切正常。 参考文章OpenStack 的 metadata 服务机制 Store metadata on a configuration drive metadata-service END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack-- neutron 二/三层网络实现探究]]></title>
      <url>%2F2017%2F02%2F12%2Fneutron-layer2-3-realization-discovry%2F</url>
      <content type="text"><![CDATA[引出 Neutron 是openstack 中提供网络虚拟化的组件，根据二层网络的实现方式不同(即agent的不同)，可以分为Linux bridge的方式，Openvswitch的方式。而且，lay2 network分为local,flat,vlan,vxlan 等类型(gre与vxlan类似，不再考虑)，本文就分析两种实现方式在这四种网络中的具体实现异同。因为vxlan会依赖lay3层网络，所以还会分析下lay3网络的实现。 本文内容主要来自每天5分钟玩转 OpenStack,图片也是来自对应博客，本文仅是对其总结对比并作记录。 lay2 local 型网络生产环境中，local与flat网络是不会被使用的,vlan与vxlan是使用比较多的layer2网络。因为local网络只支持在同一宿主机的虚拟机互联，而flat网络会每个网络独占宿主机的一个物理接口，这在现实世界中是不允许的。但是vlan与vxlan的实现都是在local,flat网络的基础上实现的，所以还是有必要看一下这两种类型的网络的实现的。 linux bridge 实现local 型网络对于每个local network，ML2 linux-bridge agent 会创建一个bridge,instance的tap设备连接到该bridge。位于同一local network的instance会连接同一个bridge，这样instance之间就可以通信了。但因为bridge没有与宿主机物理网卡相连，所以跟宿主机无法通信，也没法与宿主机之外的其他机器通信。下图为示例： 图中创建了两个local network,对应两个网桥。 VM0 与 VM1 在同一个 local network中，它们之间可以通信. VM2 位于另一个 local network，无法与 VM0 和 VM1 通信。 两个local network都有自己的DHCP server,由dnsmasq在各自独立的net namespace实现，通过tap设备挂接在各自network的bridge上。 openvswitch 实现local 型网络ovs的实现要相对复杂些，neutron完成相应配置并启动后，ovs-agent会调用ovs自动创建三个ovs bridge，br-ex,br-int和br-tun，看名字大概也可以猜出，br-ex是用于外部网络，br-int是内部虚拟机的网络，br-tun是用于overlay network,也就是vxlan类型的网络会用到该bridge。local network 只需考虑br-int bridge。示例图如下： 图中每个instance并不是跟之前一样通过tap设备直接挂载在ovs的 br-int上，而是通过一个新建的linux bridge以及一对veth pair跟br-int相连，为什么这样做呢，因为Open vSwitch 目前还不支持将 iptables 规则放在与它直接相连的 tap 设备上，这样就实现不了Security group，所以就加了一个linux bridge以支持iptables。 两个local network都有自己的DHCP server,由dnsmasq在各自独立的net namespace实现，通过tap设备挂接在各自network的bridge上。 都挂载在同一个br-int上，如何区分不同的local network呢，其实，br-int上挂载的虚拟网卡或DHCP对应的port都有一个特殊的tag属性。同一网络的tag相同，不同网络的tag不同，这里跟vlan的vlan ID类似，同一tag属性的port是二层连通的。 lay2 flat 型网络flat型网络是在local 型网络的基础上实现不同宿主机的instance之间二层互联。但是每个flat network都会占用一个宿主机的物理接口，所以生产环境中也不会使用。 linux bridge 实现flat 型网络每一个flat network对应一个物理网卡，该对应关系需要在ml2_conf.ini配置文件中指明。下图为flat network示例： 该 flat network 在计算和控制节点都使用eth1作为接口。如果是新建另一个flat network,只能使用除eth1之外的其他物理接口，且在配置文件中添加相应的mapping配置。 同一网络，不同宿主机上的Linux bridge 名称是一样的。 只在控制节点有DHCP的设备，因为同一网络只使用一个DHCP server。 其他跟local network类似。 openvswitch 实现flat 型网络同linux bridge实现的flat network一样，每一个flat network会占用一个物理接口，需要在配置文件ml2_conf.ini中指定对应关系，这里，需要利用ovs新建一个ovs bridge，将该bridge挂载对应物理接口。下图为flat network 示例： br-eth1就是我们新建的ovs bridge,为什么要新建一个ovs bridge与物理网卡连接，而不是直接用br-int与物理网卡连接呢，因为我们在配置文件中需要配置的是某一flat network 的label(flat network type,就是一个标识)与ovs-bridge的mapping,如果是直接用br-int与物理网卡连接的话，那么当建立多个flat-network的时候，br-int与多个物理网卡相连，br-int是无法辨识的。而linux bridge实现flat network的时候是不同的linux bridge与不同的物理网卡连接。 当再新建一个flat network 的时候需要再新建一个ovs bridge，连接另一网卡， 并于配置文件指定mapping。 这里出现了一种新型的网络设备叫做patch port,就是br-eth1与br-int相连的设备，与veth pair的功能类似，只不过这种设备只适用于两个ovs bridge之间互联。 只在控制节点有DHCP的设备，因为同一网络只使用一个DHCP server。 br-int内不同网络的区分跟local network一样，也是通过tag实现。 lay2 vlan 型网络linux bridge 实现vlan 型网络vlan network是在flat network的基础上实现多个不同的vlan network 共用同一个物理接口。需要在配置文件中指定vlan的范围（租户的网络id 范围），以及 vlan network 与物理网卡的对应关系。vlan network如下示例： 可以看出，bridge上除了挂载instance的tap,dhcp的tap设备外，还挂载了一个vlan interface eth1.10x ,该vlan interface 便是区分不同vlan的关键。每一个vlan network有一个对应的vlan id，该vlan id 对应相应数字的vlan interface。 上图所示，有两个vlan,vlan 100 和vlan 101,vm1 与 vm2 在同一网络vlan100，是可以联通的，vm3 与vm1,vm2不连通。 openvswitch 实现vlan 型网络 配置文件指定tenant网络类型,以及vlan id范围，对应的ovs bridge mapping。 与 Linux Bridge driver 不同，Open vSwitch driver 并不通过 eth1.100, eth1.101 等 VLAN interface 来隔离不同的 VLAN。所有的 instance 都连接到同一个网桥 br-int。Open vSwitch 通过 flow rule（流规则）来指定如何对进出 br-int的数据进行转发，进而实现 vlan 之间的隔离。vlan network如下示例： 乍一看，跟ovs 实现的flat network差不多，不过该图示有两个vlan network，都通过br-eth1与外界宿主机的instance联通。 下面看下Open vSwitch 如何通过 flow rule（流规则）进行vlan 的隔离。通过 ovs-ofctl dump-flows $BRIDGE 命令查看指定bridge 的flow rule。简单解释下：每一行代表一条rule,priority代表该rule的优先级，值越大优先级越高，in_port代表传入数据的port编号，每个port在ovs bridge中 都会有一个编号，可以通过 ovs-ofctl show $BRIDGE 命令查看 port 编号。如上图：eth1 编号为 1；phy-br-eth1 编号为 2。dl_vlan是数据包原始的 VLAN ID，actions表示对数据包进行的操作。那么第一条rule表示的意思就是：从 br-eth1 的端口 phy-br-eth1（in_port=2）接收进来的包，如果 VLAN ID 是 1（dl_vlan=1），那么需要将 VLAN ID 改为 100（actions=mod_vlan_vid:100。为什么要转换呢，之前讲local/flat network时讲过， br-int为了区分不同网络会给挂载的设备打上tag,这里的tag就类似vlan id，不过是仅限于br-int可以识别，宿主机之外是无法识别的，所以需要将该tag转换为外界可以识别的vlan id。再看下br-int 的flow rule: 图中圈出来的第一条rule的意思就是：从int-br-eth1接收进来的数据包，如果 VLAN 为 100，则改为内部 VLAN 1。其他rule分析类似。 lay3 网络lay3 网络主要是一个路由的功能，即实现两个不同subnet之间instance的互联。除此之外，与外网的互联，firewall以及instance attach floating-ip 等功能也都是由L3 agent实现的虚拟路由器（net namespace + iptables）完成的。 linux bridge 实现lay3 网络首先需要修改配置文件，启用 l3 agent,配置文件位于控制节点/网络节点的/etc/neutron/l3_agent.ini，mechanism driver 是linux bridge(或openvswitch)。创建一个router后，如下示例： 看一下router是如何连接两个vlan 的subnet: l3 agent 会为每个 router 创建了一个 namespace，通过 veth pair 与 TAP 相连，然后将 Gateway IP 配置在位于 namespace 里面的 veth interface 上，这样就能提供路由了。这样便实现 vlan100 和 vlan 101 的连通了。 再简单说下flaoting-ip attach的实现原理，floating-ip 的出现是为了让外网能够直接访问到被attached的instance。如图，是有两个内部vlan network和一个external network（网络类型可能是flat或vlan）,一个虚拟路由器将这三个网络连在一起，vm1通过虚拟路由器可以访问外网，但是外网没法直接主动连接vm1.我们将一个flaoting-ip 如10.10.10.3，attach到vm1之后，发现外网可以通过该floating-ip 直接连接vm1.进入vm1，看下它的网卡配置，发现并没有floating-ip对应的网卡存在。怎么回事呢，其实真正实现floating-ip attach操作的是发生在虚拟路由器上，查看下对应的虚拟路由器的网卡， 再看下虚拟路由器iptables的nat配置， 这样，便明白了，floating IP 是配置在 router 的外网 interface 上的，而非 instance，floating IP 能够让外网直接访问租户网络中的 instance，这是通过在 router 上应用 iptalbes 的 NAT 规则实现的。 openvswitch 实现lay3 网络与linux bridge 的实现类似，只不过这里不是挂载在不同bridge上，而是直接挂载在br-int上。 两个 Gateway IP 分别配置在 qr-2ffdb861-73 和 qr-d295b258-45 上,从而实现两个subnet的连接。floating-ip attach 与Linux bridge类似实现。 lay2 vxlan 型网络除了local, flat, vlan 这几类网络，OpenStack 还支持 vxlan 和 gre 这两种 overlay network。所谓overlay，是指指建立在其他网络上的网络，比如vxlan就是在udp的基础上实现。 vxlan 和 gre 都是基于隧道技术实现的。目前 linux bridge 只支持 vxlan，不支持 gre；open vswitch 两者都支持。因为vxlan与gre类似，这里只讨论vxlan。VXLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。与 VLAN 相比，VXLAN 有下面几个优势： 支持更多的二层网段。 VLAN 使用 12-bit 标记 VLAN ID，最多支持 4094 个 VLAN，这对于大型云部署会成为瓶颈。VXLAN 的 ID （VNI 或者 VNID）则用 24-bit 标记，支持 16777216 个二层网段。 能更好地利用已有的网络路径。 VLAN 使用 Spanning Tree Protocol 避免环路，这会导致有一半的网络路径被 block 掉。VXLAN 的数据包是封装到 UDP 通过三层传输和转发的，可以使用所有的路径。 避免物理交换机 MAC 表耗尽。 由于采用隧道机制，TOR (Top on Rack) 交换机无需在 MAC 表中记录虚拟机的信息。 VXLAN 是将二层建立在三层上的网络。 通过将二层数据封装到 UDP 的方式来扩展数据中心的二层网段数量。 VXLAN 是一种在现有物理网络设施中支持大规模多租户网络环境的解决方案。 VXLAN 的传输协议是 IP + UDP。 VXLAN 定义了一个 MAC-in-UDP 的封装格式。 在原始的 Layer 2 网络包前加上 VXLAN header，然后放到 UDP 和 IP 包中。 通过 MAC-in-UDP 封装，VXLAN 能够在 Layer 3 网络上建立起了一条 Layer 2 的隧道。 关于 vxlan 的包格式以及相关的设备的，参考VXLAN 概念（Part I） - 每天5分钟玩转 OpenStack（108） linux bridge 实现vxlan 网络VXLAN 概念（Part II）- 每天5分钟玩转 OpenStack（109） ovs 实现vxlan 网络OVS vxlan 底层结构分析 - 每天5分钟玩转 OpenStack（148） END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack--openstack 常用网络调试命令]]></title>
      <url>%2F2017%2F02%2F10%2Fopenstack-common-net-command%2F</url>
      <content type="text"><![CDATA[引出openstack网络部分应该算是最为复杂的，这里简单罗列下常用的命令作为备注，主要包括以下三个部分： neutron 命令 ip netns 命令（即网络命名空间） ovs/brctl 命令（即ovs和网桥命令） iptables 命令 neutron 命令 对net, subnet, port 这些core service的操作，略 对ext service 的操作(略)，常见ext service 包括：1. router 2. firewall 3. loadbalance 4. security-group 5. vpn 6. floatingip 查看agent列表：neutron agent-list 查看service provider: neutron service-provider-list ip netns 命令 列出宿主机所有网络命名空间：ip netns 在某一网络命名空间执行命令：ip netns exec $NAMESPACE COMMAND ,示例：1. ip netns exec qrouter-a123f550-747b-4b67-8006-2c29a5dd4c6d ip a 2. ip netns exec qrouter-a123f550-747b-4b67-8006-2c29a5dd4c6d ping www.baidu.com ovs/brctl 命令主要是针对 openvswitch 创造的网桥信息 和 linux bridge 创造的网桥信息 显示ovs创建的网桥信息：ovs-vsctl show 查看网桥br-tun上的流表信息：ovs-ofctl dump-flows br-tun ovs 创建网桥br-eth0：ovs-vsctl add-br br-eth0 将网卡eth0 桥接在br-eth0上：ovs-vsctl add-port br-eth0 eth0 查看datapath统计信息：ovs-dpctl show -s 显示linux bridge 创建的网桥信息：brctl show linux bridge 创建网桥br-eth1: brctl addbr br-eth1 将网卡eth1 桥接在br-eth1上: brctl addif br-eth1 eth1 iptables 命令neutron 中的L3-agent 默认是用iptables来实现3层网络的实现，而且，security-group, Firewall的实现也都与iptables息息相关。 新增规则到某个规则链的最后一个：iptables -A INPUT … 删除某个规则：iptables -D INPUT –dport 80 -j DROP 取代现行规则，顺序不变：iptables -R INPUT 1 -s 192.168.0.1 -j DROP 插入一条规则：iptables -I INPUT 1 –dport 80 -j ACCEPT 列出某规则链中的所有规则：iptables -L INPUT 列出nat表所有链中的所有规则：iptables -t nat -L 参考文章Linux 防火墙和 iptables Linux iptables 命令行操作常用指令 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[other--博客从jekyll搬到hexo记录]]></title>
      <url>%2F2017%2F02%2F03%2Fform-jekyll-to-hexo-blog%2F</url>
      <content type="text"><![CDATA[目录结构：引出 blog 搬家的步骤 引出 之前写blog的初衷是作为自己以后翻阅的记录，所以就用了一个非常简单的jekyll theme，随着写的越来越多，因为这个theme没有归档功能，发现越来越不好打理，所以决定换个主题。 网上搜了一下后，发现hexo可以实现本地预览的功能，就决定用hexo来搭，next这个主题是个比较经典的主题，作者的维护也一直很棒，甚至出了一个用户手册,基本能实现我的所有需求。 因为原blog就是搭在github上，所以现在所要做的工作就是利用hexo做的blog替换原有的在github page上的blog。 blog 搬家的步骤 删除原github page 项目。注意保留本地之前写的blog。 安装hexo，并按照用户手册做一些初始化的工作。 做blog的迁移工作，将blog的md文件复制到 source/_post文件夹，并在 _config.yml 中修改 new_post_name 参数。 1new_post_name: :year-:month-:day-:title.md 修改blog中的部分md格式，比如代码高亮格式等。 下载 next theme 到themes文件夹，并配置该theme，参考next theme。注意如果是使用git clone下来的，那么会存在两个远程分支（github page 一个，next theme一个），容易出错，所以最好还是直接下载下来放到themes文件夹（当然，也可以使用git submodules来管理）。 git bash 中运行依次 hexo g （生成静态文件），hexo s（启动本地服务器） ，然后本地查看。 接下来需要将blog部署到github pages。因为我需要在家里和公司的电脑进行博客的更新，所以涉及到一个协作的问题。github page 上面挂的都是一些静态的文件，而如果修改主题或者配置的话，就没法做到同步了。搜了一下，知乎上找到一个比较完美的解决方案，通过创建两个分支来解决，参考使用hexo，如果换了电脑怎么更新博客？。 参考文章Next theme hexo doc 使用hexo，如果换了电脑怎么更新博客？ END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--openstack 中的快照与备份]]></title>
      <url>%2F2017%2F01%2F23%2Fopenstack-incremental-backup%2F</url>
      <content type="text"><![CDATA[目录结构：引出 nova 相关的快照/备份机制 cinder相关的快照/备份机制 引出 备份作为一种常见的容灾机制，是系统设计中不可或缺的一环，openstack自然也不例外，本文列举了一些opensatck中常见的snapshot,backup等操作。 重点讨论nova,cinder。其他备份比如trove不再考虑 简单谈一下快照与备份的区别，快照保存的是瞬时数据，依赖于源数据，主要用于做重要升级之前先做个snapshot，以备出现差错回滚。备份主要是为了容灾，不依赖于源数据，且一般备份数据在不同数据中心。 nova 相关的快照/备份机制Nova 快照 命令为nova image-create 严格来说，该快照并不是真正意义的快照，更像是创建一个镜像，所以命令为image-create.它并没有使用virt提供的virsh snapshot-create来创建snapshot，仅仅将虚拟机的系统磁盘文件完整的复制一份，然后把复制的文件上传至 glance 中作为镜像，之后二者毫无关联，类似于全量快照(没有考虑ceph作为统一存储)check this。 nova 中的快照支持冷快照和live snapshot,两者的命令都是 nova image-create。当满足QEMU 1.3+ and libvirt 1.0+版本要求时会使用live snapshot,否则使用冷快照。关于两者区别以及流程，check this 如果是使用ceph作为统一存储(即nova,glance,cinder都将ceph rbd作为默认存储)，那么snapshot可以利用ceph的cow等特性实现更快速的快照，L版本已实现，流程大概如下： 这样做的坏处是会产生新建镜像对原虚拟机的依赖性，需要定期flatten。还不知道L版是怎么处理的，挖个坑，回头看下具体实现。详细参考基于rbd提升虚机快照创建速度 Nova 备份 命令为 nova backup 其实该命令跟image-create的作用差不多，也是创建一个image,最大的区别就是它有一个rotation，会保证备份的个数为rotation个。详细的用法参考这个实例Openstack - Nova Backup/Restore nova backup 不会自动每天/月周期执行，只能是我们自己命令，所以可以利用nova backup 命令写cron脚本或其他脚本实现自动化备份。这里有一个实例OpenStack: Quick and automatic instance snapshot backup and restore (and before an apt upgrade) with nova backup cinder相关的快照/备份机制cinder 快照 命令为 cinder snapshot-create 同nova volume-snapshot-create一样效果。除了create,还有其他list,delete,metadata等命令。 可以用snapshot 对volume进行恢复，见示例Openstack - Cinder Snapshot/Restore 如果cinder使用rbd作为后端存储，那么这时候的snapshot 便是利用rbd snapshot来实现的，关于ceph rbd snapshot的实现原理，参考ceph– ceph rbd快照的实现原理 cinder 备份 命令 cinder backup-create cinder支持多种backend,对全量备份，增量备份的支持也不尽相同，这里只讨论以ceph rbd 作为backend的情况，其他的backend可以参考这里，Openstack 中cinder backup三种backend的对比- 使用 rbd作为后端，支持全量备份和增量备份。backup-create时先尝试创建增量备份，如果不成功，会创建全量备份，不需要指明专门的参数。 参考这篇, cinder-backup 利用ceph实现增量备份。 参考文章Nova 快照分析 OpenStack: Quick and automatic instance snapshot backup and restore (and before an apt upgrade) with nova backup 基于rbd提升虚机快照创建速度 OpenStack Nova snapshots on Ceph RBD Openstack - Nova Backup/Restore openstack liberty版本使用 ceph 作为存储后端 Openstack 中cinder backup三种backend的对比 Inside Cinder’s Incremental Backup END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--重设虚拟机实例密码几种方法]]></title>
      <url>%2F2017%2F01%2F19%2Fopenstack-reset-instance-password%2F</url>
      <content type="text"><![CDATA[目录结构：引出 采用 nova get-password 方式 采用 libvirt-set-admin-password 采用 nova rebuild instance 的方式 采用 cloud-init 的方式 引出 要解决的问题很明确：就是如果虚拟机的连接采用用户名密码登录的方式，而密码忘记的话，需要采取什么手段解决。 其实解决方案是要取决于真实的生产环境，虚拟化方式的不同，初始化虚拟机密码方式的不同，openstack版本的不同，都会造成某个方案的可行不可行。以下几种方案可能或多或少会出现无法实现的情况，楼主尽量把条件讲清楚。 采用 nova get-password 方式 利用nova 提供的这个接口可以获取instance的password,就不用密码reset了。 适用条件：虚拟化方式为XEN，不支持libvirt. 采用 libvirt-set-admin-password Openstack L 版本新加入的功能，直接使用 “nova set-password “(或早期版本client的”nova root-password”) 就可以，之前的版本该命令不支持Libvirt,仅支持XEN。 适用条件：Openstack Libvirt+ 版本，宿主机libvirt版本1.2.16+，虚拟机镜像安装2.3+ 版本的qemu-guest-agent，详见虚拟机系统密码的修改方案¶ 博主试验了linux 几个主要版本（debian,ubuntu，centos）,只要满足以上的限制条件，都能修改成功，不过官网提供的cloud 版本镜像大都没有安装 qemu-guest-agent，或者版本太低，需要自己安装并制作成镜像。windows 的镜像相对比较麻烦点，宿主机装的virtio-win，通过文件挂载的方式给guest安装对应驱动和QGA,其实不用安装也可以，实现最终都是guest 安装virtio-serial驱动,然后安装quemu-guest-agent就可以了，还有一点就是要设置镜像的property ,例如：hw_qemu_guest_agent=yes，os_type=windows。参考Running the QEMU Guest Agent on a Windows Guest，Can I have virtio-win package on CentOS CentOS 7.1 QEMU guest agent 安装 与 使用 (注：这篇文章有点过时了，因为QGA以及驱动的版本已经升级了好几版，大部分功能都已经实现，比如密码修改等，但是整个流程是一致的) 关于qemu-guest-agent ，可以参考nova 通过 qemu-guest-agent 修改用户密码 采用 nova image-create / nova rebuild的方式 如果虚拟机是根据user-data来设定初始密码的，那么cloud-init只在第一次创建虚拟机执行一次，以后不会执行（reboot也不会执行）。那么我们也只能再次launch一下，方法如下。 首先对当前虚拟机做一次snapshot. 利用该snapshot ，设定好user-data重新boot 一个新的虚拟机 注意：此处只是保证系统盘数据是不变的，如果是数据盘的话还要将对应的数据盘detach再attch到新建的虚拟机中。当然，如果虚拟机是直接用的adminPass的话（即injectPassword的方式）也可以直接利用rebuild命令（rebuild只能用于image启动的instance,而不能用于volume 启动的instance）。 这种方法其实比较笨拙，不到万不得已一般不会这么做。 采用 cloud-init 的方式 这种方法算是所有方法里面最轻便的，但坏处是需要自己定制脚本。对于cloud-init，不熟悉的话可以先翻一下官方手册 原理很简单：借助cloud-init,在虚拟机启动的时候开启一个服务，用来监测metadata中设定的某个值，如果该值发生改变（或者满足其他条件）即做出密码更改的动作并reboot。 可喜的是，我在github找到了类似的代码openstack-password-reset ,不过这个代码只是考虑了RH7系列，而且密码是随机生成的，如果再推给openstack，可能更复杂了。我又更改一下脚本，支持更多Linux版本，且把重设后的密码定死了。年后会把改过的代码挂到github上。 这里面的reset Python程序是通过外链获取的，于是干脆在nova里加了一个API，用来获取该程序。 如果是传递多个文件给cloud-init的话，需要使用MIME的格式，tips:一般是把多个脚本/cloud-config文件 打包成MIME格式文件，然后压缩成gzip格式，传给cloud-init。 参考文章Password Reset 虚拟机系统密码的修改方案 CloudInit &amp; User-Data END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 系列--利用Vmware workstation手动制作 openstack centos镜像简记]]></title>
      <url>%2F2017%2F01%2F17%2Fmake-openstack-image%2F</url>
      <content type="text"><![CDATA[目录结构：准备工作 创建虚拟机 安装Centos系统 配置系统 上传镜像 自动创建镜像工具 参考文章 tips:在制作镜像的时候最好先知晓镜像所要满足的要求，比如：是否支持磁盘分区，重设根分区大小，密码初始化等等。最常见的需求有如下几种： Disk partitions and resize root partition on boot (cloud-init) No hard-coded MAC address information SSH server running Disable firewall Access instance using ssh public key (cloud-init) Process user data and other metadata (cloud-init) Paravirtualized Xen support in Linux kernel (Xen hypervisor only with Linux kernel version &lt; 3.0) 这部分内容可参考官方文档，Image requirements 。 准备工作 下载Vmware workstation,并安装Centos7,配置好网络。注：刚开始是用的virtualbox,发现不支持nested KVM,Vmware workstation 10+版本以上应该都可以,注意开启workstation的虚拟化设置。 宿主机（刚装好的Centos）安装libvirt系列工具: 1234yum groupinstall Virtualization "Virtualization Client" yum install libvirtyum install libguestfs-toolsservice libvirtd restart 从Centos镜像源下载一个最小的Centos7镜像。本文是以centos为例,其他操作系统类似，详见Create images manually 1wget - O http://mirrors.163.com/centos/7.2.1511/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso 创建虚拟机 创建一个容量为5G的磁盘,并更改所属者： 123cd /imageqemu-img create -f qcow2 centos.qcow2 5Gchown qemu:qemu /image/centos.qcow2 -R 创建并启动虚拟机 123456sudo virt-install --virt-type kvm --name centos7 --ram 1024 \ # name 是自己取得 --disk centos.qcow2,format=qcow2 \ #disk参数为上面创建的磁盘 --network network=default \ # 采用KVM虚拟机网络为nat，其实更建议用bridge模式，删除本行，默认即为bridge模式，不过bridge模式需要自己手动配置宿主机网桥 --graphics vnc,listen=0.0.0.0 --noautoconsole \ --os-type=linux --os-variant=rhel7 \ --cdrom=/image/CentOS-7-x86_64-Minimal-1511.iso #下载的镜像 用“virsh list” 命令查看虚拟机是否启动，如果没有启动的话用“virsh start $NAME” 启动。 执行 “virsh vncdisplay $NAME” ,之后就可以利用vnc-client 访问该虚拟机了。 安装Centos系统 利用vnc-viewer 连接 虚拟机，默认是宿主机IP+5900端口。并进行虚拟机的安装。安装时注意大多数配置默认即可，SOFTWARE SELECTION选择Minimal Install，INSTALLATION DESTINATION需要选择手动配置分区，我们只需要一个根分区即可，不需要swap分区，文件系统选择ext4，存储驱动选择Virtio Block Device。 安装完成后，为了能够联外网，以及用宿主机直接SSH 虚拟机，还要进行网络的配置。KVM虚拟机的网络配置方式主要有两种，Bridge模式和NAT 模式，关于两种模式的介绍可以参考这篇文章，KVM虚拟机网络配置 Bridge方式，NAT方式。我们这里使用默认的NAT模式，但是NAT模式有个弊端就是无法从宿主机SSH虚拟机，这样我们还要做些额外的工作实现该功能。 通过端口转发来实现宿主机直连虚拟机，具体参考这里。 1234567echo 1 &gt;/proc/sys/net/ipv4/ip_forward # 打开ip转发功能，这种方法是暂时的，可以直接修改/etc/sysctl.conf 文件，增加net.ipv4.ip_forward = 1 达到永久效果，文件修该完毕后，要使用sysctl –p使其生效iptables -t nat -A POSTROUTING -o br0 -j MASQUERADE # 开启KVM服务器的IPtables的转发功能iptables -t nat -A PREROUTING -d 192.168.1.102 -p tcp -m tcp --dport 8022 -j DNAT --to-destination 192.168.122.173:22iptables -t nat -A POSTROUTING -s 192.168.122.0/255.255.255.0 -d 192.168.122.173 -p tcp -m tcp --dport 22 -j SNAT --to-source 192.168.122.1 配置系统 SSH到虚拟机后就可以开始一系列的配置工作，首先修改yum源为国内源。 安装acpid服务，并设置开机自启动（acpid服务是用于hypervisor与虚拟机的交互）。 12yum install -y acpidsystemctl enable acpid 虚拟机需要打开boot日志功能，并指定console，这样nova console-log才能获取虚拟机启动时的日志。修改配置文件/etc/default/grub，设置GRUB_CMDLINE_LINUX为： 1GRUB_CMDLINE_LINUX=" vconsole.keymap=us console=tty0 vconsole.font=latarcyrheb-sun16 console=ttyS0,115200" 执行以下语句： 1grub2-mkconfig -o /boot/grub2/grub.cfg ubuntu 的话执行 update-grub。 手动安装qemu-guest-agent(版本必须是2.3+，L 版的动态修改密码需要用到)： 1yum install -y qemu-guest-agent 配置qemu-ga，修改/etc/sysconfig/qemu-ga，配置内容为: 123456TRANSPORT_METHOD="virtio-serial"DEVPATH="/dev/virtio-ports/org.qemu.guest_agent.0"LOGFILE="/var/log/qemu-ga/qemu-ga.log"PIDFILE="/var/run/qemu-ga.pid"BLACKLIST_RPC=""FSFREEZE_HOOK_ENABLE=0 为了使虚拟机能够和外部的metadata service通信，需要禁用默认的zeroconf route： 1echo "NOZEROCONF=yes" &gt;&gt; /etc/sysconfig/network 最后安装cloud-init： 1yum install -y cloud-init 为了实现自动扩容, 需要安装growpart： 1234yum update -yyum install -y epel-releaseyum install -y cloud-utils-growpartrpm -qa kernel | sed 's/^kernel-//' | xargs -I &#123;&#125; dracut -f /boot/initramfs-&#123;&#125;.img &#123;&#125; 基本完成，虚拟机关机。 宿主机上进行清理工作： 12virt-sysprep -d centos7 # 移除mac地址信息virsh undefine centos7 # 删除虚拟机 上传镜像 镜像格式转换（ceph只支持raw格式） 1qemu-img convert -f qcow2 -O raw centos.qcow2 centos.raw 直接用glance上传镜像就不赘述了，这里有个窍门就是如果后端是ceph的话可以利用rbd import 的方式来加快速度，如下。 首先glance create 一条记录，但没有执行镜像上传操作，只是新建一条数据库记录。并记录下image-id。 1glance image-create 利用ceph import 镜像并执行快照 123rbd --pool=volumes import centos.raw --image=$IMAGE_ID --new-format --order 24rbd --pool=volumes --image=$IMAGE_ID --snap=snap snap createrbd --pool=volumes --image=$IMAGE_ID --snap=snap snap protect 完善镜像的必要属性 1234glance image-update --name="centos-7.2-64bit" --disk-format=raw --container-format=bare $IMAGE_ID# 配置qemu-ga，该步骤是必须的，否则libvert启动虚拟机时不会生成qemu-ga配置项，导致虚拟机内部的qemu-ga由于找不到对应的虚拟串行字符设备而启动失败，提示找不到channelglance image-update --property hw_qemu_guest_agent=yes $IMAGE_ID 设置镜像的location url1glance location-add --url rbd://$FS_ROOT/glance_images/$IMAGE_ID/snap $IMAGE_ID#这里的$FS_ROOT 可以通过查看ceph -s 中的cluster. 具体参考手动制作Openstack镜像 的上传镜像部分。 自动创建镜像工具openstack 文档提供了几个类似的工具，没有尝试，Tool support for image creation. 参考文章openstack doc:Example: CentOS image 手动制作Openstack镜像 制作openstack用的centos6.5镜像 KVM虚拟机网络配置 Bridge方式，NAT方式 烂泥：KVM使用NAT联网并为VM配置iptables端口转发，kvmiptables 谈谈Openstack的CentOS镜像 Image requirements END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--初始化虚拟机实例密码几种方法]]></title>
      <url>%2F2017%2F01%2F13%2Fopenstack-init-instance-password%2F</url>
      <content type="text"><![CDATA[目录结构：引出 关于cloud init openstack 初始化虚拟机密码几种方式 引出 最初的问题是这样的：公司的云平台没有提供虚拟机密码重置的功能，需要加上，但我看到创建虚拟机的时候是可以设置密码的，于是想将重置密码的功能重用这部分，结果发现问题不是这么简单。 关于虚拟机重置密码放在下一篇，今天主要写写初始化密码的几种方式。 其实虚拟机初始化设置密码，跟虚拟机初始化设置网络，主机名是一样的道理，都是通过cloud-init来实现。 先看下cloud-init。 关于cloud init Cloud-Init 是一个用来自动配置虚拟机的初始设置（如主机名，网卡和密钥）的工具（支持Linux,对于Windows系列有一个类似的工具，cloudbase-init）。它可以在使用模板部署虚拟机时使用，从而达到避免网络冲突的目的。在使用这个工具前，cloud-init 软件包必须在虚拟机上被安装。安装后，Cloud-Init 服务会在系统启动时搜索如何配置系统的信息。您可以使用只运行一次窗口来提供只需要配置一次的设置信息；或在新建虚拟机、编辑虚拟机和编辑模板窗口中输入虚拟机每次启动都需要的配置信息。（以上内容直接引用自red hat 官网） cloud-init 的配置数据有两种： userdata:文件的形式，常见的如yaml文件，shell scripts，cloud config file。（cloud-init会自动识别以 “#!” 或 “#cloud-config” 开头的文件。） metedata:键值对的形式，常见的如：server name, instance id, display name and other cloud specific details. cloud-init 的数据源：即cloud-init会从以下几种方式获取userdata或metedata： EC2方式：顾名思义，就是AWS的那一套，其实本质是创建一个http server,虚拟机通过这个http server获取到instance的userdata和metadata。这个http server的IP 通常为169.254.169.254。 Config Drive方式：主要是openstack提供的一种方式，本质是把data写入一个特殊的配置设备中，然后在虚拟机启动时，自动挂载并读取 metadata 信息。 Alt cloud：通常用于 RHEVm 和 vSphere中获取userdata。 vSphere 用于为了向VMWare的vSphere虚拟机中注入userdata。 No Cloud 允许用户在没有网络的情况下提供user-data和medatata给instances openstack 初始化虚拟机密码几种方式因为目前大部分私有云平台基本是KVM虚拟化，后台存储为ceph，所以只选适合这种架构的方案，早期的inject-password,貌似只支持QCOW2，不支持RAW镜像(没有验证，但在ceph的文档里确实把这个功能的相关配置默认为false,而且推荐使用cloud-init)以及只支持XEN的clear-password ， root-password等API不再考虑。 利用user-data注入：我司的云平台就是使用这种方式，用法很简单 cloud-init数据源是采用最广泛的EC2模式，EC2的restAPI相关知识check this。关于metadata server的配置不在描述，主要是Nova，和Neutron 的相关配置（默认配置即可）。 登录到对应的虚拟机验证下： 关于metadata和userdata的服务机制,参考OpenStack 的 metadata 服务机制 修改cloud-init的方式：注意在执行命令nova boot的时候会生成一个adminPass的字段。当然，如果你想用自己的密码，也可以在meta中指定，然后修改下代码即可，参考这里 。 我们可以获取该字段，然后交给cloud-init进行密码的设置。前文所说，虚机在通过cloud-init获取元数据时可以使用api-metadata、ConfigDrive等方式，但是因为只有ConfigDrive方式才会把adminPass字段传递给虚机，所以这里我们只能用config drive的方式。关于config drive的使用方式，check this 。而且，为了能够使cloud-init获取到该字段，需要加个patch: 12345678910111213141516171819202122232425diff --git a/cloudinit/config/cc_set_passwords.py b/cloudinit/config/cc_set_passwords.pyindex 4ca85e2..5b5cae4 100644--- a/cloudinit/config/cc_set_passwords.py+++ b/cloudinit/config/cc_set_passwords.py@@ -44,6 +44,12 @@ def handle(_name, cfg, cloud, log, args): else: password = util.get_cfg_option_str(cfg, "password", None)- # use the admin_pass available in the ConfigDrive- if not password:- metadata = cloud.datasource.metadata- if metadata and 'admin_pass' in metadata:- password = metadata['admin_pass']+ expire = True pw_auth = "no" change_pwauth = False@@ -59,6 +65,8 @@ def handle(_name, cfg, cloud, log, args): (user, _user_config) = ds.extract_default(users) if user: plist = "%s:%s" % (user, password)- #add change root password- plist = plist + "\nroot:%s" % password else: log.warn("No default or defined user to change password for.") 同时cloud.cfg中也要添加: chpasswd: { expire: False } 这样保证修改的密码不是过期的，否则vm启动后输入密码，系统让你重新修改才能进入。 3, 利用L版的新特性set-admin-password：这种方式本身要求的条件有点高，没有去尝试，详见虚拟机系统密码的修改方案 参考文章How to reset password of openstack instance using KVM and libvirt? Cloud Init and Config Drive Cloud-init初探 和 OpenStack应用 openstack 中 metadata 和 userdata 的配置和使用 虚拟机系统密码的修改方案 设置虚拟机密码 Cloud config examples END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 系列--nova-api 的paste&route的简单抽象]]></title>
      <url>%2F2017%2F01%2F12%2Fopenstack-pasteDeploy%26route%26application%2F</url>
      <content type="text"><![CDATA[目录结构：由Python的调用实例想到的 nova-api 的paster&amp;route 模型 由Python的调用实例想到的 公司的云平台有两个自己写的服务，highland和rainflow。也没有开发文档，只能自己去看代码琢磨一下，从paste配置文件看下最终调用的route app，是一个factory方法，我记得factory方法应该返回一个WSGI APP,但是看下源码是这样的： 1234@classmethoddef factory(cls, global_config, **local_config): """Simple paste factory, :class:`rainflow.wsgi.Router` doesn't have""" return cls() 是一个类方法返回这个类，我就有点懵了，这样是执行什么操作呢，难道是再创建一个实例。而且nova-api也是这么干的，证明不是错误。后来搜了一下资料，再试验了一下大致知道怎么回事了。 看下下面这个示例： 123456789101112131415#!/usr/bin/env pythonclass demo: @classmethod def iam_classmethod(cls): print("classmethod") return cls() def __init__(self): print("i am init") def __call__(self): print"i am calling"test=demo.iam_classmethod()test() 运行结果为： 123classmethodi am initi am calling iam_classmethod方法会返回一个类实例，执行这个类实例就是调用call() 方法。同理，上面那个代码其实就是调用call方法，我们可以在对应的父类找到。 nova-api 的paster&amp;route 模型 nova-api的路由模型其实不难，只不过层层封装，比较复杂，在网上找到一个比较好理解的示例。 首先要明白路由的生成过程跟路由的寻址过程是两条线，当我们启动nova-api的时候，路由的map就已经生成了，当我们发送rest请求时，就会根据路由的map去找对应的application以及controller。所以，重点是看下路由的生成过程。 看下一个简单的代码实现（代码来自网上，没有经过验证，只是为了说明）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293from __future__ import print_functionfrom routes import Mapperimport webob.decimport webob.excimport routes.middlewareimport testtoolsclass MyController(object): def getlist(self, mykey): print("step 4: MyController's getlist(self, mykey) is invoked") return "getlist(), mykey=" + mykeyclass MyApplication(object): """Test application to call from router.""" def __init__(self, controller): self._controller = controller def __call__(self, environ, start_response): print("step 3: MyApplication is invoked") action_args = environ['wsgiorg.routing_args'][1].copy() try: del action_args['controller'] except KeyError: pass try: del action_args['format'] except KeyError: pass action = action_args.pop('action', None) controller_method = getattr(self._controller, action) result = controller_method(**action_args) start_response('200 OK', [('Content-Type', 'text/plain')]) return [result]class MyRouter(object): """Test router.""" def __init__(self): route_name = "dummy_route" route_path = "/dummies" my_application = MyApplication(MyController()) self.mapper = Mapper() self.mapper.connect(route_name, route_path, controller=my_application, action="getlist", mykey="myvalue", conditions=&#123;"method": ['GET']&#125;) self._router = routes.middleware.RoutesMiddleware(self._dispatch, self.mapper) @webob.dec.wsgify(RequestClass=webob.Request) def __call__(self, req): """Route the incoming request to a controller based on self.map. If no match, return a 404. """ print("step 1: MyRouter is invoked") return self._router @staticmethod @webob.dec.wsgify(RequestClass=webob.Request) def _dispatch(req): """Dispatch the request to the appropriate controller. Called by self._router after matching the incoming request to a route and putting the information into req.environ. Either returns 404 or the routed WSGI app's response. """ print("step 2: RoutesMiddleware is invoked, calling our _dispatch back") match_dict = req.environ['wsgiorg.routing_args'][1] if not match_dict: return webob.exc.HTTPNotFound() app = match_dict['controller'] return app class RoutingTestCase(testtools.TestCase): def test_router(self): router = MyRouter() result = webob.Request.blank('/dummies').get_response(router) self.assertEqual(result.body, "getlist(), mykey=myvalue") 关于添加一个路由请求的操作参考这里openstack-wsgi的route中增加api流程详解（os-networks）增加 参考文章使用Python Routes + webob 的 REST API的测试程序. openstack-wsgi的route中增加api流程详解（os-networks）增加 浅析openstack中WSGI+Restful router“框架？” END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[web系列--关于http，tcp,amqp的一些思考]]></title>
      <url>%2F2017%2F01%2F10%2Fsomething-about-http%26tcp%26amqp%2F</url>
      <content type="text"><![CDATA[目录结构：为什么TCP是可以保持长连接的，而基于TCP的HTTP却不能 AMQP VS HTTP 为什么TCP是可以保持长连接的，而基于TCP的HTTP却不能 可能这个问题本身表述就不是很清楚，但我也想不出其他合适的词汇了。直接上例子吧：我们在做web应用的时候有时会碰到需要实时推送的情况，但是HTTP协议本身是不支持长时间的连接的，因为HTTP本身是request/response型的协议，可以理解为半双工通讯。而TCP 是支持长连接的，是通过heartbeat来进行连接的保持。这样，就引出我上面提到的问题了。 在解答这个问题之前，还有几点是要说明的：http1.1中其实也有长连接的概念，具体到header参数就是keep-alive，但是这个keep-alive是一个数值，比如20s,就是这个http连接对应的tcp连接会保持20S,这个时间段过来的http连接会复用上次的tcp连接。如果没有的话，那么会开启一个新的tcp连接。而tcp协议里面的keep-alive就是heartbeat发送的包内容，表示“我还活着”。 接下来回答一下上面那个问题：其实很简单，我们混淆了“基于TCP协议”这个概念，TCP本身是一个传输层的协议，就是用来作数据传输的，也决定了它必然是全双工，可以保持连接的特性。再来看下http协议，一种request/response型的协议，为什么要做成request/response这种类型呢，因为server端要服务的client太多了，如果每一个连接都保持而不断开，那么服务器的IO很快就会被堵死了。所以http基于tcp,并非就是全部利用的tcp的特性，在http连接建立之前，的确是需要tcp三次握手建立tcp连接，然后利用这个tcp连接传输http包数据，服务端收到请求的数据后，发送对应的response,之后便关闭了这个tcp连接（如果没有keep-alive）。 再回到刚才的例子：其实现在解决这种实时推送的方案还是很丰富的，比较主流的应该是利用websocket协议来实现。当然，其他的比如：ajax实现的轮询等。就不细讲了，感兴趣就自行Google吧。 AMQP VS HTTP其实这个问题应该这样描述：在做一个SOA服务时，用HTTP好还是AMQP好？ 该问题的提出主要是因为openstack的内部通信中主要包含rest和Rabbimq两种通信方式，所以不禁对于两种方式的优劣提出了疑问，经过一番思考以及搜索资料后，找到了相对满意的答案： 主要参考When designing Service Oriented Architectures, which one do you use - HTTP or AMQP? AMQP is damn awesome! 确实，amqp更快，可定制性更强，可以实现异步传输，相对于http来说，作为SOA的信息传输协议真是再好不过了。简单列举下AMQP碾压HTTP 的优点，此处以rabbitmq为例： rabbitmq可以提供面向连接，可靠的信息传递。 rabbitmq提供过载保护：比如设置queue的timeout,限制 message rate,甚至转移message到别的queue. http需要有一个service discovery 机制，rabbitmq 不用。 rabbitmq 的loadbalance 使用非常简单 rabbitmq 的后端如果做了热备，如果一个后端节点挂掉，会自动切换到另一节点。 再说下http的优点，毕竟年岁要长一些，SOA的框架非常成熟，学习的曲线也不高，lz之前用过阿里出品的dubbo框架，简单易用。更重要的是，http作为一种成熟的协议标准，想要满足任何需求基本都会有对应的中间件，而且scale up 的成本低。 综上所述，不难看出，在真正用到SOA时，还是需要取决于实际情况来取舍。如果只是一个内部的系统，交互的模块相对紧密的话推荐用AMQP。如果是一个大型的web系统，与外部交互比较多，而且是多部门协作完成多个模块的开发，那么模块件的交互还是使用标准的http吧。 对应到openstack也可以看出一二：模块之间交互都是使用http/rest,比如nova与keystone的认证交互，因为每个独立的模块都是一个项目，都有独立的开发团队。而在模块内部，比如，Nova内部，nova-api与nova-scheduler,nova-scheduler与nova-compute 都是使用amqp来实现。 参考文章The Problem We Solve 用两条 HTTP 请求就可以实现双向实时通讯吧？为什么还需要 WebSocket 微服务与SOA之间差了一个ESB When designing Service Oriented Architectures, which one do you use - HTTP or AMQP? 关于 RabbitMQ 的实时消息推送 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python系列--python decorator 简 记]]></title>
      <url>%2F2016%2F12%2F30%2Fpython-decorator%2F</url>
      <content type="text"><![CDATA[目录结构：装饰器的本质 装饰器的几种形式 注：以下内容皆出自 酷壳 PYTHON修饰器的函数式编程 ,本文只是便于自己记忆，做的一篇简单记录。 装饰器的本质看下一个最简单的装饰器的例子： 123456789101112def hello(fn): def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapper @hellodef foo(): print "i am foo" foo() @注解 是一个Python 中的语法糖，比如一个如下语句123@decoratordef func(): pass 它会被解释为 ：1func = decorator(func) 其实就是把一个函数当作参数传到另一个函数，然后再回调，最后再把decorator这个函数的返回值赋值回了原来的func。 落实到刚开始的例子就是：12345678910111213def hello(fn): def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapperdef foo(): print "i am foo"foo = hello(foo)foo() OK,这样就理清了！ 装饰器的几种形式 带参数的形式： 1234567891011121314151617def makeHtmlTag(tag, *args, **kwds): def real_decorator(fn): css_class = " class='&#123;0&#125;'".format(kwds["css_class"]) \ if "css_class" in kwds else "" def wrapped(*args, **kwds): return "&lt;"+tag+css_class+"&gt;" + fn(*args, **kwds) + "&lt;/"+tag+"&gt;" return wrapped return real_decorator @makeHtmlTag(tag="b", css_class="bold_css")@makeHtmlTag(tag="i", css_class="italic_css")def hello(): return "hello world" print hello()# 输出：# &lt;b class='bold_css'&gt;&lt;i class='italic_css'&gt;hello world&lt;/i&gt;&lt;/b&gt; 为了让 hello = makeHtmlTag(arg1, arg2)(hello) 成功，makeHtmlTag 必需返回一个decorator（这就是为什么我们在makeHtmlTag中加入了real_decorator()的原因），这样一来，我们就可以进入到 decorator 的逻辑中去了—— decorator得返回一个wrapper，wrapper里回调hello。 类的形式 1234567891011121314151617181920212223class myDecorator(object): def __init__(self, fn): print "inside myDecorator.__init__()" self.fn = fn def __call__(self): self.fn() print "inside myDecorator.__call__()" @myDecoratordef aFunction(): print "inside aFunction()" print "Finished decorating aFunction()" aFunction() # 输出：# inside myDecorator.__init__()# Finished decorating aFunction()# inside aFunction()# inside myDecorator.__call__() 可以看出在构造装饰器装饰的函数时，执行init函数，在真正调用aFunction()函数时会调用call。 看下重写上面的html.py的代码： 1234567891011121314151617class makeHtmlTagClass(object): def __init__(self, tag, css_class=""): self._tag = tag self._css_class = " class='&#123;0&#125;'".format(css_class) \ if css_class !="" else "" def __call__(self, fn): def wrapped(*args, **kwargs): return "&lt;" + self._tag + self._css_class+"&gt;" \ + fn(*args, **kwargs) + "&lt;/" + self._tag + "&gt;" return wrapped @makeHtmlTagClass(tag="b", css_class="bold_css")@makeHtmlTagClass(tag="i", css_class="italic_css")def hello(name): return "Hello, &#123;&#125;".format(name) 注意参数的调用，如果decorator有参数的话，init()成员就不能传入fn了，而fn是在call的时候传入的。 被decorator的函数其实已经是另外一个函数了，对于最前面那个hello.py的例子来说，如果你查询一下foo.name的话，你会发现其输出的是“wrapper”，而不是我们期望的“foo”，这会给我们的程序埋一些坑。所以，Python的functool包中提供了一个叫wrap的decorator来消除这样的副作用。所以就有了下面这种形式： 123456789101112131415161718from functools import wrapsdef hello(fn): @wraps(fn) def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapper @hellodef foo(): '''foo help doc''' print "i am foo" pass foo()print foo.__name__ #输出 fooprint foo.__doc__ #输出 foo help doc 参考文章PYTHON修饰器的函数式编程 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--AllInOne openstack 实验环境搭建]]></title>
      <url>%2F2016%2F12%2F26%2FAllInOne-openstack-environ%2F</url>
      <content type="text"><![CDATA[目录结构：虚拟机搭建 利用RDO 搭建allinone openstack 环境 利用Fuel 搭建openstack 环境 虚拟机搭建 virtualbox 下载安装，centos 7下载 利用virtualbox启动centos 7.遇到问题如下： 网络一直连不通，后来参考这篇文章VirtualBox下虚拟机和主机内网互通+虚拟机静态IP的网络配置 不过我只用了一个虚拟网卡就够用了，因为只建一个虚拟机，不用虚拟机之间联通。可以使用nat，或者桥接模式。最好使用桥接模式，因为可以使用客户端SSH连接。如果网络没通的话可能还需要配置一下网络参数，比如IP地址，dns地址等。 利用RDO 搭建allinone openstack 环境 搭建openstack实验环境大致有三种：RDO的packstack,Mirantis 的Fuel安装，Devstack。这三种方式所有的安装过程都不难，但是因为网络的问题，总会出现很多意料之外的事情。总体来说，RDO这种allinone的方式是最简单的，就算出了问题，80%的可能出在网络上。 按照quick start进行安装。 出现的问题： 安装 yum install -y openstack-packstack 时出现“Package does not match intended download” ，这个问题google看了好几页一直没解决，后来我在家里的时候用家里的网络就可以了，估测是公司的网络用了代理，存在缓存的原因。 最后执行 packstack –allinone 时经常卡住，一般卡在 testing if puppet apply is finished 的居多，其实也是网络问题，我在深夜网速较好的时候成功率比较高。 搭建完成后就可以在虚拟机上进行调试了，推荐一个vim 的设置A set of vim, zsh, git, and tmux configuration files.(*nix开发环境一键配置） 利用Fuel 搭建openstack 环境 目前最新版本还是openstack M版。主要是参考这篇文章部署安装Mirantis OpenStack Fuel 9.0。 没有部署成功，fuel server 部署成功了，但是最终部署openstack的时候死活过不去，上篇文章的作者也说可重复性不高，看报错日志是download error，也是网络问题。以后有机会大半夜再搞一发。 上篇文章没有提到的一件事是 在fuel server部署完成后，ubuntu镜像最好是自己下载然后上传到 Fuel server,设置resposity 的时候设置为本地源，这样可以通过网络验证。 参考文章部署安装Mirantis OpenStack Fuel 9.0 quick start A set of vim, zsh, git, and tmux configuration files.(*nix开发环境一键配置） END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--cinder 源码解读(二)---常见cinder操作工作流程]]></title>
      <url>%2F2016%2F12%2F26%2Fcinder-source-code%20explain-2%2F</url>
      <content type="text"><![CDATA[目录结构：cinder 源码大致解析 以新建 cinder volume 为例 深入 cinder 代码 关于cinder不再赘述，之前文章由介绍。 cinder 的 API 目前有三个版本，本次讨论以v2为基准。 因为版本不同，代码也可能不同，该版本为Openstack Liberty cinder 源码大致解析看下cinder项目的入口文件setup.cfg： 123456789console_scripts = cinder-api = cinder.cmd.api:main cinder-backup = cinder.cmd.backup:main cinder-manage = cinder.cmd.manage:main cinder-rootwrap = oslo_rootwrap.cmd:main cinder-rtstool = cinder.cmd.rtstool:main cinder-scheduler = cinder.cmd.scheduler:main cinder-volume = cinder.cmd.volume:main cinder-volume-usage-audit = cinder.cmd.volume_usage_audit:main cinder-api: 启动cinder 的wsgi server,每一个API对应一种资源，在setup.cfg 的entry point 中进行了配置，使用stevedore进行加载。 cinder-backup:将volume 备份到其他存储系统 cinder-manage：提供对cinder其他服务的管理操作。 cinder-rootwrap : 某些cinder操作需要root权限 cinder-rtstool :如果iSCSI target管理工具不是使用的默认的tgtadm，而是使用LIO iSCSI的话，就可以使用该命令，能够让你创建，删除，和验证卷，以及决定target，且可添加iSCSI initiator到系统。 cinder-scheduler :用于根据提供的策略调度cinder-volune节点 cinder-volume：管理 volume的生命周期，与底层存储交互。 cinder-volume-usage-audit：用于卷使用情况统计。 以新建 cinder volume 为例 深入 cinder 代码首先看下cinder –debug create volume 都发送了那些请求： 可以看到先去keystone验证，然后向cinder-api发送一个v2/{tenant-id}/volumes 的post请求，volume创建，然后发送一个v2/{tenant-id}/volumes/{volume-id} 的get请求获取创建的volume数据。 接下来看下cinder-api中接受create volume 的post请求代码，路由过程略过： /cinder/api/v2/volumes.py 主要代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@wsgi.response(202) def create(self, req, body): """Creates a new volume.""" .....对传入参数的检测....... kwargs['metadata'] = volume.get('metadata', None) snapshot_id = volume.get('snapshot_id') if snapshot_id is not None: if not uuidutils.is_uuid_like(snapshot_id): msg = _("Snapshot ID must be in UUID form.") raise exc.HTTPBadRequest(explanation=msg) # Not found exception will be handled at the wsgi level kwargs['snapshot'] = self.volume_api.get_snapshot(context, snapshot_id) else: kwargs['snapshot'] = None source_volid = volume.get('source_volid') if source_volid is not None: # Not found exception will be handled at the wsgi level kwargs['source_volume'] = \ self.volume_api.get_volume(context, source_volid) else: kwargs['source_volume'] = None source_replica = volume.get('source_replica') if source_replica is not None: # Not found exception will be handled at the wsgi level src_vol = self.volume_api.get_volume(context, source_replica) if src_vol['replication_status'] == 'disabled': explanation = _('source volume id:%s is not' ' replicated') % source_replica raise exc.HTTPBadRequest(explanation=explanation) kwargs['source_replica'] = src_vol else: kwargs['source_replica'] = None consistencygroup_id = volume.get('consistencygroup_id') if consistencygroup_id is not None: # Not found exception will be handled at the wsgi level kwargs['consistencygroup'] = \ self.consistencygroup_api.get(context, consistencygroup_id) else: kwargs['consistencygroup'] = None size = volume.get('size', None) if size is None and kwargs['snapshot'] is not None: size = kwargs['snapshot']['volume_size'] elif size is None and kwargs['source_volume'] is not None: size = kwargs['source_volume']['size'] elif size is None and kwargs['source_replica'] is not None: size = kwargs['source_replica']['size'] LOG.info(_LI("Create volume of %s GB"), size) if self.ext_mgr.is_loaded('os-image-create'): image_ref = volume.get('imageRef') if image_ref is not None: image_uuid = self._image_uuid_from_ref(image_ref, context) kwargs['image_id'] = image_uuid kwargs['availability_zone'] = volume.get('availability_zone', None) kwargs['scheduler_hints'] = volume.get('scheduler_hints', None) kwargs['multiattach'] = utils.get_bool_param('multiattach', volume) new_volume = self.volume_api.create(context, size, volume.get('display_name'), volume.get('display_description'), **kwargs) retval = self._view_builder.detail(req, new_volume) return retval 参考文章深入理解Openstack QoS控制实现与实践 Volume Transfer(volume过户) openstack使用Ceph存储后端创建虚拟机快照原理剖析 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--cinder 源码解读(一)---常见命令行操作]]></title>
      <url>%2F2016%2F12%2F23%2Fcinder-sourcecode-explain-1%2F</url>
      <content type="text"><![CDATA[目录结构：cinder 的命令行操作 关于cinder不再赘述，之前文章由介绍。 cinder 的 API 目前有三个版本，本次讨论以v2为基准。 因为版本不同，代码也可能不同，该版本为Openstack Liberty cinder 的命令行是由python-cinderclient 项目提供（已逐渐整合进python-openstackclient项目） 除此之外，还有一个cinder-manage 的命令行，主要是对cinder 各服务的管理，是由cinder项目本身cinder-manage模块提供。 cinder 的命令行操作直接命令行：cinder help ，查看一下subcommand 都包含哪些操作。（本想用图片展示，画了半天，太麻烦了。） 关于 volume 的操作： create/delete/list/rename/show force-delete :不管state如何，强制删除 migrate:迁移至另一主机 set-bootable:设置为可启动盘 reset-state:设置volume 的state upload-to-image : bootable 必须为True readonly-mode-update :设置为只读模式 关于 type 的操作： 操作如图： volume type 还是比较重要的，后面qos的设置的时候就是与volume-type 绑定的。 还有一个encryption-type 也是 type相关，绑定instance之后，该磁盘上的内容会加密。 关于 snapshot 的操作： 关于 backup 的操作： backup 与 snapshot 的区别：1，snapshot 对于原volume是强依赖的，要想删除volume，必须先删除依赖他的snapshot，snapshot主要用于恢复数据到某个时间点，且原volume破损，那么对应的snapshot也不可用。而backup则对volume就不具有依赖性。所以即使备份相关的卷出现故障，还是可以恢复备份中数据。操作方法是创建一个新的空白卷，将备份restore到这个空白卷即可。2，snapshot 只能在同一存储区域（比如 ceph 里就是同一volume,backup 可以转存到别的地方） 补：发现如果是从snapshot 创建的volume，删除snapshot 之前要先删除依赖的volume，否则，删除不了。而且没有提示，不知道是否有bug，或是版本太低，下次用最新版试下。 关于 qos的操作： qos 要与volume type 一块使用。 深入理解Openstack QoS控制实现与实践 关于 quota 的操作： 关于 transfer 的操作： 所谓 transfer 就是admin把 volume 从一个project 转到另一个project。 关于 service 的操作： 看下service-list: enable/disable 即是对status的操作 参考文章深入理解Openstack QoS控制实现与实践 Volume Transfer(volume过户) openstack使用Ceph存储后端创建虚拟机快照原理剖析 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph系列-- ceph 运维(二)-----常见TroubleShooting]]></title>
      <url>%2F2016%2F12%2F21%2Fceph-command-2%2F</url>
      <content type="text"><![CDATA[目录结构：监视器MON故障排除 OSD 故障排除 归置组排障 监视器MON故障排除 简单记下修复 monmap的过程： 1. 如果该mon节点是quorum 节点，直接获取monmap: ceph mon getmap -o /tmp/monmap 2. 如果该mon节点不是quorum 节点，从别的quorum节点(假设有ID，且已stop)获取:ceph-mon -i ID --extract-monmap /tmp/monmap 3. stop 你要修复的mon节点的mon进程 4. 注入monmap:ceph-mon -i ID --inject-monmap /tmp/monmap 5. start monitor OSD 故障排除 收集 OSD 数据 查看ceph日志：/var/log/ceph 利用管理套接字：ceph daemon {socket-file} help （可以获取在运行时配置，历史操作，操作的优先队列状态，在进行的操作，列出性能计数器） 查看空间占用：df -h I/O 统计信息: iostat -x 诊断消息: dmesg | grep scsi 停止自动重均衡 如果不想在停机维护 OSD 时让 CRUSH 自动重均衡，提前设置 noout ：ceph osd set noout 停机维护失败域内的 OSD 维护结束后，重启OSD。 解除 noout 标志：ceph osd unset noout OSD 未成功运行通常情况下，简单地重启 ceph-osd 进程就可以重回集群并恢复。 OSD 运行不起来：检查配置文件（语法错误），或配置路径错误。查看是否超过系统默认最大线程数，以及与内核版本是否冲突。 OSD down: 查看日志是否因为硬盘错误 OSD 龟速或无响应 有很多原因会导致此问题： 网络问题：netstat -s 驱动器配置:一个存储驱动器应该只用于一个 OSD 。 坏扇区和碎片化硬盘. 监视器和 OSD 在同一主机：监视器是普通的轻量级进程，但它们会频繁调用 fsync() ，这会妨碍其它工作量，特别是监视器和 OSD 共享驱动器时。 日志记录级别：如果你为追踪某问题提高过日志级别、但结束后忘了调回去，这个 OSD 将向硬盘写入大量日志。如果你想始终保持高日志级别，可以考虑给默认日志路径挂载个硬盘。 内核与 SYNCFS 问题：试试在一主机上只运行一个 OSD ，看看能否提升性能。老内核未必支持有 syncfs(2) 系统调用的 glibc 。 文件系统问题：推荐基于 xfs 或 ext4 部署集群。 内存不足：建议为每 OSD 进程规划 1GB 内存。 OLD REQUESTS 或 SLOW REQUESTS：如果某 ceph-osd 守护进程对一请求响应很慢，它会生成日志消息来抱怨请求耗费的时间过长。 一会up一会down 的OSD 即俗称的打摆子。网络部署时建议同时部署公网（前端）和集群网（后端），这样能更好地满足对象复制的容量需求。另一个优点是你可以运营一个不连接互联网的集群，以此避免拒绝攻击。 OSD 们互联和检查心跳时会优选集群网（后端），如果集群网（后端）失败、或出现了明显的延时，同时公网（前端）却运行良好， 这时 OSD 们会向监视器报告邻居 down 了、同时报告自己是 up 的，我们把这种情形称为打摆子（ flapping ）。可以强制OSD停止标记： 12ceph osd set noup # prevent OSDs from getting marked upceph osd set nodown # prevent OSDs from getting marked down 下列命令可清除标记： 12ceph osd unset noupceph osd unset nodown 归置组排障在我们查看ceph 状态的时候，通常会看到pgmap的状态：active+clean。这是正常状态。不过也有不正常的时候。 pg 状态为 inactive—–归置组长时间无活跃（即它不能提供读写服务了）：OSD 的互联心跳检测失败，标记为down,重启对应osd进程。 pg 状态为unclean——归置组长时间不干净（例如它未能从前面的失败完全恢复）：找不到对象。 pg 状态为stale—–归置组状态没有被 ceph-osd 更新，表明存储这个归置组的所有节点可能都挂了。 详参归置组排障 参考文章监视器故障排除 OSD 故障排除 归置组排障 日志记录和调试 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph系列-- ceph 运维(一)-----常用命令]]></title>
      <url>%2F2016%2F12%2F20%2Fceph-command-1%2F</url>
      <content type="text"><![CDATA[目录结构：ceph 数据组织方式 ceph 增加/删除OSD ceph 增加/删除MON 操作 ceph集群中的服务 监控 ceph 集群的一些命令 ceph 用户管理 ceph 数据管理命令 关于ceph 之前只是简单介绍过，并没有系统的去学习，总结过。之后几篇文章会详细介绍下ceph的常用运维命令。 ceph 数据组织方式首先看下ceph的数据组织方式，因为ceph与openstack结合时用到最多的便是块存储服务rbd，所以这里主要以rbd为例。 ceph 数据的组织方式是在物理资源的基础上进行了多层映射。 客户想要创建一个rbd设备前，必须创建 一个pool，需要为这个pool指定pg的数量，在一个pool中的pg数量是不一定的，同时这个pool中要指明保存数据的副本数量3个副本。再在这个pool中创建一个rbd设备rbd0，那么这个rbd0都会保存三份，在创建rbd0时必须指定rbd的size，对于这个rbd0的任何操作不能超过这个size。之后会将这个块设备进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。将每个object通过pg进行副本位置的分配，pg会寻找3个osd，把这个object分别保存在这三个osd上。osd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。最后对于object的存储就变成了存储一个文件rbd0.object1.file。 客户端使用rbd设备时，一般有两种方法。 第一种 是kernel rbd。就是创建了rbd设备后，把rbd设备map到内核中，形成一个虚拟的块设备，这时这个块设备同其他通用块设备一样，一般的设备文件为/dev/rbd0，后续直接使用这个块设备文件就可以了，可以把/dev/rbd0格式化后mount到某个目录，也可以直接作为裸设备使用。这时对rbd设备的操作都通过kernel rbd操作方法进行的。 第二种是librbd方式。就是创建了rbd设备后，这时可以使用librbd、librados库进行访问管理块设备。这种方式不会map到内核，直接调用librbd提供的接口，可以实现对rbd设备的访问和管理，但是不会在客户端产生块设备文件。 简单看下客户端写数据到osd的过程。 假设这次采用的是librbd的形式，使用librbd创建一个块设备，这时向这个块设备中写入数据，在客户端本地同过调用librados接口，然后经过pool，rbd，object、pg进行层层映射,在PG这一层中，可以知道数据保存在哪3个OSD上，这3个OSD分为主从的关系，也就是一个primary OSD，两个replica OSD。客户端与primay OSD建立SOCKET 通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点。 ceph 增加/删除OSD在官方文档里已经非常非常详细，增加/删除 OSD 简单总结一下： 增加一个osd的流程大致： 预备工作：部署硬件，网络，安装ceph软件包 增加OSD: 创建osd 创建默认目录并挂载硬盘 初始化 OSD 数据目录 注册 OSD 认证密钥 把 OSD 加入 CRUSH 图（crushmap其实就是部署物理环境的一个描述，比如哪几台机器在同一机架上，哪几个机架在同一机房里等等） 启动 OSD 观察数据迁移 删除osd的流程： 踢出集群，置为out 观察数据迁移: ceph -w 停止 OSD 进程 删除 OSD：移出集群 CRUSH 图、删除认证密钥、删除 OSD 图条目、删除 ceph.conf 条目。如果主机有多个硬盘，每个硬盘对应的 OSD 都得重复此步骤。 ceph 增加/删除MON详见官方文档，增加/删除监视器 增加一个mon流程： 预备工作：部署硬件，网络，安装ceph软件包 增加MON: 在新监视器主机上创建mon默认目录 获取监视器密钥环,保存到临时目录 获取监视器运行图，保存到临时目录 准备监视器的数据文件（包括密钥环，运行图文件） 启动监视器，指定绑定端口 删除mon的流程： 停止监视器。 从集群删除监视器。 删除 ceph.conf 对应条目。 操作 ceph集群中的服务ceph 中的服务包含 OSD ,MON两种，对这些服务的操作也无非是stop，start,restart等操作。有很多种方式，详见操纵集群 常用的是以service 的形式： 启动所有守护进程： sudo service ceph -a start (加 -a 即在所有节点上执行) 启动一类守护进程： sudo service ceph start osd (只在本节点启动OSD进程) 启动单个守护进程： sudo service ceph start osd.0 监控 ceph 集群的一些命令 检查鸡群状态：ceph status 或 ceph -s 实时检查集群：ceph -w 检查集群的容量使用情况：ceph df 检查OSD状态：ceph osd stat 或 ceph osd tree 检查MON状态： ceph mon stat 查看MON 选举状态：ceph quorum_status 检查归置组（PG）状态 ：ceph pg stat 归置组的状态有多个，详见监控 OSD 和归置组 管理套接字命令允许你在运行时查看和修改配置 ceph 用户管理 罗列用户：ceph auth list 用户的增删修改参见 用户管理 ceph 数据管理命令只说下最常见的三种数据：pool,pg,和crushmap 的管理。 pool 存储池管理：Ceph 在存储池内存储数据，它是对象存储的逻辑组；存储池管理着归置组数量、副本数量、和存储池规则集。要往存储池里存数据，用户必须通过认证、且权限合适，存储池可做快照。主要操作如下： 列出存储池： ceph osd lspools 创建存储池： ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-ruleset-name] [expected-num-objects] 设置存储池配额： ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}] 删除存储池： ceph osd pool delete {pool-name} [{pool-name} –yes-i-really-really-mean-it] 重命名存储池： ceph osd pool rename {current-pool-name} {new-pool-name} 查看存储池统计信息： rados df 拍下存储池快照： ceph osd pool mksnap {pool-name} {snap-name} 删除存储池快照：ceph osd pool rmsnap {pool-name} {snap-name} 设置对象副本数：ceph osd pool set {poolname} size {num-replicas} 获取对象副本数：ceph osd dump | grep ‘replicated size’ 归置组(Placement Group)管理：Ceph 把对象映射到归置组（ PG ），归置组是一逻辑对象池的片段，这些对象组团后再存入到 OSD 。归置组减少了各对象存入对应 OSD 时的元数据数量，根据N副本策略，每个归置组对应N个OSD。主要的操作如下： 设置归置组数量：ceph osd pool set {pool-name} pg_num {pg_num}虽然 pg_num 的增加引起了归置组的分割，但是只有当用于归置的归置组（即 pgp_num ）增加以后，数据才会被迁移到新归置组里，所以还要增加一步：ceph osd pool set {pool-name} pgp_num {pgp_num} 获取归置组数量：ceph osd pool get {pool-name} pg_num 获取归置组统计信息：ceph pg dump [–format {format}] 获取一归置组运行图：ceph pg map {pg-id} 获取一 PG 的统计信息：ceph pg {pg-id} query CRUSH 图(CRUSH Map): CRUSH 是重要组件，它使 Ceph 能伸缩自如而没有性能瓶颈、没有扩展限制、没有单点故障，它为 CRUSH 算法提供集群的物理拓扑，以此确定一个对象的数据及它的副本应该在哪里、怎样跨故障域存储，以提升数据安全。详见CRUSH 图。编辑crushmap的步骤如下： 获取 CRUSH 图：ceph osd getcrushmap -o {compiled-crushmap-filename} 反编译 CRUSH 图：crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename} 至少编辑一个设备、桶、规则 编译 CRUSH 图：crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename} 注入 CRUSH 图：ceph osd setcrushmap -i {compiled-crushmap-filename} 参考文章ceph的数据存储之路(2) —– rbd到osd的数据映射 集群运维–ceph doc END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--glanceApi 源代码解读(三)]]></title>
      <url>%2F2016%2F12%2F16%2Fglance-api-sourcecode-explain-3%2F</url>
      <content type="text"><![CDATA[目录结构：glanceApi V2 代码解读 glanceApi V2 代码解读 还是以image create为例，首先debug一下，看下都是发出了哪些restful请求： 本来是要截图的，网络出了点问题，云环境暂时用不了，直接写吧，发出了三个restful请求： 1. GET /v2/schemas/image 2. POST /v2/images 3. PUT /v2/images/xxxxxxxxxxxxxxxx/file 大致猜出第二，三个请求分别是image metedata的数据库操作请求，上传镜像的操作请求。接下来逐一分析： 先看下GET /v2/schemas/image 请求，根据路由的规则router.py找到对应的执行函数： 123456789101112131415161718192021222324252627class API(wsgi.Router): """WSGI router for Glance v2 API requests.""" def __init__(self, mapper): #默认scheme的property就几个， #如果自己增加的话需要写在一个schema-image.json文件中，通过此函数load custom_image_properties = images.load_custom_properties() reject_method_resource = wsgi.Resource(wsgi.RejectMethodController()) #根据配置文件create 一个controller，看下文分析 schemas_resource = schemas.create_resource(custom_image_properties) mapper.connect('/schemas/image', controller=schemas_resource, action='image', conditions=&#123;'method': ['GET']&#125;, body_reject=True) mapper.connect('/schemas/image', controller=reject_method_resource, action='reject', allowed_methods='GET') mapper.connect('/schemas/images', controller=schemas_resource, action='images', conditions=&#123;'method': ['GET']&#125;, body_reject=True) ............................................ schemas.create_resource，看下该方法的实现： 123def create_resource(custom_image_properties=None): controller = Controller(custom_image_properties) return wsgi.Resource(controller) 可以看到该方法初始化了一个Controller,然后封装成一个WSGI APP，看下初始化： 12345678910111213141516171819202122232425262728293031323334class Controller(object): def __init__(self, custom_image_properties=None): #创建一个image_schema self.image_schema = images.get_schema(custom_image_properties) self.image_collection_schema = images.get_collection_schema( custom_image_properties) self.member_schema = image_members.get_schema() self.member_collection_schema = image_members.get_collection_schema() self.task_schema = tasks.get_task_schema() self.task_collection_schema = tasks.get_collection_schema() # Metadef schemas self.metadef_namespace_schema = metadef_namespaces.get_schema() self.metadef_namespace_collection_schema = ( metadef_namespaces.get_collection_schema()) self.metadef_resource_type_schema = metadef_resource_types.get_schema() self.metadef_resource_type_collection_schema = ( metadef_resource_types.get_collection_schema()) self.metadef_property_schema = metadef_properties.get_schema() self.metadef_property_collection_schema = ( metadef_properties.get_collection_schema()) self.metadef_object_schema = metadef_objects.get_schema() self.metadef_object_collection_schema = ( metadef_objects.get_collection_schema()) self.metadef_tag_schema = metadef_tags.get_schema() self.metadef_tag_collection_schema = ( metadef_tags.get_collection_schema()) #调用此image方法，并最终返回image_schema.raw()的对象 def image(self, req): return self.image_schema.raw() 看下创建image_schema的方法images.get_schema： 12345678910111213141516def get_schema(custom_properties=None): #获取描述一个image的基本property，id,name ,status,owner等 properties = get_base_properties() links = _get_base_links() #根据配置文件创建Schema,allow_additional_image_properties默认True, #区别是PermissiveSchema多设置了links参数 if CONF.allow_additional_image_properties: schema = glance.schema.PermissiveSchema('image', properties, links) else: schema = glance.schema.Schema('image', properties) #如果是自己定制的property,需要做合并操作 if custom_properties: for property_value in custom_properties.values(): property_value['is_base'] = False schema.merge_properties(custom_properties) return schema 最后看下Schema对象包含什么内容 12345678910111213141516171819202122232425262728293031323334353637383940class Schema(object): def __init__(self, name, properties=None, links=None, required=None, definitions=None): self.name = name if properties is None: properties = &#123;&#125; self.properties = properties self.links = links self.required = required self.definitions = definitions ......................... #该方法返回一个dick def raw(self): raw = &#123; 'name': self.name, 'properties': self.properties, 'additionalProperties': False, &#125; if self.definitions: raw['definitions'] = self.definitions if self.required: raw['required'] = self.required if self.links: raw['links'] = self.links return raw #Schema的子类 ，分析同上class PermissiveSchema(Schema): @staticmethod def _filter_func(properties, key): return True def raw(self): raw = super(PermissiveSchema, self).raw() raw['additionalProperties'] = &#123;'type': 'string'&#125; return raw def minimal(self): minimal = super(PermissiveSchema, self).raw() return minimal 第一个请求的分析结束，所以该请求的目的就是获取镜像所支持的属性字典定义，接下来就该根据这些信息来验证用户输入参数。 看下第二条请求，路由到glance/api/v2/images.py.ImagesController.create方法： 1234567891011121314151617181920212223242526272829303132@utils.mutating def create(self, req, image, extra_properties, tags): #责任链的模式， #分别创建image_factory,image_repo对象 image_factory = self.gateway.get_image_factory(req.context) image_repo = self.gateway.get_repo(req.context) try: image = image_factory.new_image(extra_properties=extra_properties, tags=tags, **image) image_repo.add(image) except (exception.DuplicateLocation, exception.Invalid) as e: raise webob.exc.HTTPBadRequest(explanation=e.msg) except (exception.ReservedProperty, exception.ReadonlyProperty) as e: raise webob.exc.HTTPForbidden(explanation=e.msg) except exception.Forbidden as e: LOG.debug("User not permitted to create image") raise webob.exc.HTTPForbidden(explanation=e.msg) except exception.LimitExceeded as e: LOG.warn(encodeutils.exception_to_unicode(e)) raise webob.exc.HTTPRequestEntityTooLarge( explanation=e.msg, request=req, content_type='text/plain') except exception.Duplicate as e: raise webob.exc.HTTPConflict(explanation=e.msg) except exception.NotAuthenticated as e: raise webob.exc.HTTPUnauthorized(explanation=e.msg) except TypeError as e: LOG.debug(encodeutils.exception_to_unicode(e)) raise webob.exc.HTTPBadRequest(explanation=e) #返回一个 image对象 return image 关于责任链模式，check this 先看下gateway.get_image_factory方法： 1234567891011121314151617181920212223242526272829303132class Gateway(object): def __init__(self, db_api=None, store_api=None, notifier=None, policy_enforcer=None): self.db_api = db_api or glance.db.get_api() self.store_api = store_api or glance_store self.store_utils = store_utils self.notifier = notifier or glance.notifier.Notifier() self.policy = policy_enforcer or policy.Enforcer() #这里可以清晰地看出是责任链模式， def get_image_factory(self, context): image_factory = glance.domain.ImageFactory() #对location的检验 store_image_factory = glance.location.ImageFactoryProxy( image_factory, context, self.store_api, self.store_utils) #对quota的检验 quota_image_factory = glance.quota.ImageFactoryProxy( store_image_factory, context, self.db_api, self.store_utils) #policy检验 policy_image_factory = policy.ImageFactoryProxy( quota_image_factory, context, self.policy) notifier_image_factory = glance.notifier.ImageFactoryProxy( policy_image_factory, context, self.notifier) if property_utils.is_property_protection_enabled(): property_rules = property_utils.PropertyRules(self.policy) pif = property_protections.ProtectedImageFactoryProxy( notifier_image_factory, context, property_rules) authorized_image_factory = authorization.ImageFactoryProxy( pif, context) else: authorized_image_factory = authorization.ImageFactoryProxy( notifier_image_factory, context) return authorized_image_factory 可以看出ImageFactoryProxy类都继承自glance/domain/proxy.py.ImageFactory，通过类名可以大致猜出其功能：镜像工厂，那就是用来创建封装镜像对象的；各个子类也分别实现:权限检查、消息通知、策略检查、配额检查等。另外各个ImageFactoryProxy类都依赖于ImageProxy类。而各ImageProxy类都继承自glance/domain/proxy.py.Image，该类描述的是镜像的属性信息，包括：name，image_id, status等。各*ImageProxy类是对Image的扩展。 关于这个责任链的代码一直没搞清楚，感兴趣的话，看这篇文章吧，Openstack liberty Glance上传镜像源码分析,以后有机会再去解读。 按照上述给出的那篇参考文章的解释，new_image()方法和add()方法，类似一个封包，一个解包的过程，主要完成权限检查，配额检查，策略检查，发布通知以及记录数据库等操作。下面来看看镜像文件的上传过程。 第三条请求，路由到glance/api/v2/image_data.py/ImageDataController.upload方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@utils.mutating def upload(self, req, image_id, data, size): image_repo = self.gateway.get_repo(req.context) image = None refresher = None cxt = req.context try: #get方法与前述的add方法类似，首先从数据库取出image_id指向的条目， #封装成`domain/__init__.py/Image`对象， #然后经过层层封装返回 `authorization/ImageProxy`对象 image = image_repo.get(image_id) #更新镜像状态为saving - ‘保存中’ image.status = 'saving' try: if CONF.data_api == 'glance.db.registry.api': # create a trust if backend is registry try: # request user plugin for current token user_plugin = req.environ.get('keystone.token_auth') roles = [] # use roles from request environment because they # are not transformed to lower-case unlike cxt.roles for role_info in req.environ.get( 'keystone.token_info')['token']['roles']: roles.append(role_info['name']) refresher = trust_auth.TokenRefresher(user_plugin, cxt.tenant, roles) except Exception as e: LOG.info(_LI("Unable to create trust: %s " "Use the existing user token."), encodeutils.exception_to_unicode(e)) #和上面的save方法相似的处理方式，逐层调用`ImageProxy`的set_data， #在该过程中会检查用户配额，发送通知， #最后根据glance-api.conf文件中配置存储后端上传镜像文件（通过add方法）到指定地方存储 image_repo.save(image, from_state='queued') #镜像上传成功后（在`location.py/set_data方法中上传文件成功后， #修改状态为active），更新数据库状态为active（这个时候可以在 #Dashboard上看到状态为'运行中'） image.set_data(data, size) try: image_repo.save(image, from_state='saving') except exception.NotAuthenticated: if refresher is not None: # request a new token to update an image in database cxt.auth_token = refresher.refresh_token() image_repo = self.gateway.get_repo(req.context) image_repo.save(image, from_state='saving') else: raise ......................................... 主体代码也是责任链的模式，太蛋疼了，看不明白。 参考文章openstack 设计与实现 Openstack liberty Glance上传镜像源码分析 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--glanceApi 源代码解读(二)]]></title>
      <url>%2F2016%2F12%2F15%2Fglance-api-sourcecode-explain-2%2F</url>
      <content type="text"><![CDATA[目录结构：glanceApi V1 代码解读 glanceApi V1 代码解读 首先简单看下V1 代码与V2 代码的目录结构： 也可以明显看出V2相对V1加了许多API。 下面以glance image-create 为例解读下glanceAPI V1 的代码，glanceClient发出rest请求后，glance-api Server监听到对应端口的请求，根据api-paste文件经过middleware的过滤后，根据配置文件中选定的版本，最终路由到v1/images.py文件中的对应函数，先看下WSGI 路由文件router.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class API(wsgi.Router): """WSGI router for Glance v1 API requests.""" def __init__(self, mapper): reject_method_resource = wsgi.Resource(wsgi.RejectMethodController()) images_resource = images.create_resource() mapper.connect("/", controller=images_resource, action="index") mapper.connect("/images", controller=images_resource, action='index', conditions=&#123;'method': ['GET']&#125;) mapper.connect("/images", controller=images_resource, action='create', conditions=&#123;'method': ['POST']&#125;) mapper.connect("/images", controller=reject_method_resource, action='reject', allowed_methods='GET, POST') mapper.connect("/images/detail", controller=images_resource, action='detail', conditions=&#123;'method': ['GET', 'HEAD']&#125;) mapper.connect("/images/detail", controller=reject_method_resource, action='reject', allowed_methods='GET, HEAD') mapper.connect("/images/&#123;id&#125;", controller=images_resource, action="meta", conditions=dict(method=["HEAD"])) mapper.connect("/images/&#123;id&#125;", ............................... members_resource = members.create_resource() mapper.connect("/images/&#123;image_id&#125;/members", controller=members_resource, action="index", conditions=&#123;'method': ['GET']&#125;) mapper.connect("/images/&#123;image_id&#125;/members", controller=members_resource, action="update_all", conditions=dict(method=["PUT"])) mapper.connect("/images/&#123;image_id&#125;/members", controller=reject_method_resource, action='reject', allowed_methods='GET, PUT') mapper.connect("/images/&#123;image_id&#125;/members/&#123;id&#125;", controller=members_resource, action="show", conditions=&#123;'method': ['GET']&#125;) .................................. 主要有两部分，一部分是镜像image的操作，一部分是members的操作。POST /images 请求最终路由到create函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@utils.mutating def create(self, req, image_meta, image_data): """ Adds a new image to Glance. Four scenarios exist when creating an image: 1. If the image data is available directly for upload, create can be passed the image data as the request body and the metadata as the request headers. The image will initially be 'queued', during upload it will be in the 'saving' status, and then 'killed' or 'active' depending on whether the upload completed successfully. 2. If the image data exists somewhere else, you can upload indirectly from the external source using the x-glance-api-copy-from header. Once the image is uploaded, the external store is not subsequently consulted, i.e. the image content is served out from the configured glance image store. State transitions are as for option #1. 3. If the image data exists somewhere else, you can reference the source using the x-image-meta-location header. The image content will be served out from the external store, i.e. is never uploaded to the configured glance image store. 4. If the image data is not available yet, but you'd like reserve a spot for it, you can omit the data and a record will be created in the 'queued' state. This exists primarily to maintain backwards compatibility with OpenStack/Rackspace API semantics. The request body *must* be encoded as application/octet-stream, otherwise an HTTPBadRequest is returned. Upon a successful save of the image data and metadata, a response containing metadata about the image is returned, including its opaque identifier. :param req: The WSGI/Webob Request object :param image_meta: Mapping of metadata about image :param image_data: Actual image data that is to be stored :raises: HTTPBadRequest if x-image-meta-location is missing and the request body is not application/octet-stream image data. """ #上文注释里面提到create image时出现的四种情况.不赘述 #查看policy是否允许‘add_image’操作,下同 self._enforce(req, 'add_image') is_public = image_meta.get('is_public') if is_public: self._enforce(req, 'publicize_image') if Controller._copy_from(req): self._enforce(req, 'copy_from') if image_data or Controller._copy_from(req): self._enforce(req, 'upload_image') #查看property是否合法 self._enforce_create_protected_props(image_meta['properties'].keys(), req) #验证property的个数是否超过配置文件规定项目的配额quota self._enforce_image_property_quota(image_meta, req=req) #将image_meta传给glance-registry,调用glance-registrycli image_meta = self._reserve(req, image_meta) id = image_meta['id'] #上传镜像数据操作。 image_meta = self._handle_source(req, id, image_meta, image_data) location_uri = image_meta.get('location') if location_uri: self.update_store_acls(req, id, location_uri, public=is_public) # Prevent client from learning the location, as it # could contain security credentials image_meta = redact_loc(image_meta) return &#123;'image_meta': image_meta&#125; 先看下metadata 的处理过程_reserve方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def _reserve(self, req, image_meta): """ Adds the image metadata to the registry and assigns an image identifier if one is not supplied in the request headers. Sets the image's status to `queued`. :param req: The WSGI/Webob Request object :param id: The opaque image identifier :param image_meta: The image metadata :raises: HTTPConflict if image already exists :raises: HTTPBadRequest if image metadata is not valid """ #注释里说得很清楚了，给rigistry增加一个image metedata,如果没有id的话 #给他绑定一个id,并将该image status 设置为‘queued’ location = self._external_source(image_meta, req) scheme = image_meta.get('store') if scheme and scheme not in store.get_known_schemes(): msg = _("Required store %s is invalid") % scheme LOG.warn(msg) raise HTTPBadRequest(explanation=msg, content_type='text/plain') image_meta['status'] = ('active' if image_meta.get('size') == 0 else 'queued') #如果不是直接上床，路径为外部路径，那么先对外部路径validate以及获取image size if location: try: backend = store.get_store_from_location(location) except (store.UnknownScheme, store.BadStoreUri): LOG.debug("Invalid location %s", location) msg = _("Invalid location %s") % location raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") # check the store exists before we hit the registry, but we # don't actually care what it is at this point self.get_store_or_400(req, backend) # retrieve the image size from remote store (if not provided) image_meta['size'] = self._get_size(req.context, image_meta, location) #如果是本地直接上传，那么将size设置为0，上传的时候会再设置 else: # Ensure that the size attribute is set to zero for directly # uploadable images (if not provided). The size will be set # to a non-zero value during upload image_meta['size'] = image_meta.get('size', 0) try: #调用registry.client，向glance-registry发送rest请求 image_meta = registry.add_image_metadata(req.context, image_meta) self.notifier.info("image.create", redact_loc(image_meta)) return image_meta except exception.Duplicate: msg = (_("An image with identifier %s already exists") % image_meta['id']) LOG.warn(msg) raise HTTPConflict(explanation=msg, request=req, content_type="text/plain") except exception.Invalid as e: msg = (_("Failed to reserve image. Got error: %s") % encodeutils.exception_to_unicode(e)) LOG.exception(msg) raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") except exception.Forbidden: msg = _("Forbidden to reserve image.") LOG.warn(msg) raise HTTPForbidden(explanation=msg, request=req, content_type="text/plain") 进入registry.client，看下相关代码： 12345def add_image_metadata(context, image_meta): LOG.debug("Adding image metadata...") #获取registry_client ,code explain veerything c = get_registry_client(context) return c.add_image(image_meta) 看下最终的add_image()方法： 1234567891011121314151617181920def add_image(self, image_metadata): """ Tells registry about an image's metadata """ headers = &#123; 'Content-Type': 'application/json', &#125; if 'image' not in image_metadata: image_metadata = dict(image=image_metadata) encrypted_metadata = self.encrypt_metadata(image_metadata['image']) image_metadata['image'] = encrypted_metadata body = jsonutils.dump_as_bytes(image_metadata) #向registry发送POST请求。返回一个类似JSON形式 的dict res = self.do_request("POST", "/images", body=body, headers=headers) # Registry returns a JSONified dict(image=image_info) data = jsonutils.loads(res.read()) image = data['image'] return self.decrypt_metadata(image) 再往下就是glance-registry 接收到rest请求后对数据的操作。 不像nova-conductor通过Object Model进行db访问，glance-registry相对比较简单，直接通过glance db模块进行访问，就不再另写一篇文章介绍glance-registry,直接看下经过路由后的执行函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@utils.mutating def create(self, req, body): """Registers a new image with the registry. :param req: wsgi Request object :param body: Dictionary of information about the image :returns: The newly-created image information as a mapping, which will include the newly-created image's internal id in the 'id' field """ image_data = body['image'] # Ensure the image has a status set image_data.setdefault('status', 'active') # Set up the image owner if not req.context.is_admin or 'owner' not in image_data: image_data['owner'] = req.context.owner image_id = image_data.get('id') if image_id and not uuidutils.is_uuid_like(image_id): LOG.info(_LI("Rejecting image creation request for invalid image " "id '%(bad_id)s'"), &#123;'bad_id': image_id&#125;) msg = _("Invalid image id format") return exc.HTTPBadRequest(explanation=msg) if 'location' in image_data: image_data['locations'] = [image_data.pop('location')] try: image_data = _normalize_image_location_for_db(image_data) #调用glance db 模块进行数据库操作， #glance db 模块是对sqlalchemy 的封装 image_data = self.db_api.image_create(req.context, image_data) image_data = dict(image=make_image_dict(image_data)) LOG.info(_LI("Successfully created image %(id)s"), &#123;'id': image_data['image']['id']&#125;) return image_data except exception.Duplicate: msg = _("Image with identifier %s already exists!") % image_id LOG.warn(msg) return exc.HTTPConflict(msg) except exception.Invalid as e: msg = (_("Failed to add image metadata. " "Got error: %s") % encodeutils.exception_to_unicode(e)) LOG.error(msg) return exc.HTTPBadRequest(msg) except Exception: LOG.exception(_LE("Unable to create image %s"), image_id) raise OK,对metedata的分析到这，再回去看下对image chunk data的处理,先是方法_handle_source： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def _handle_source(self, req, image_id, image_meta, image_data): copy_from = self._copy_from(req) location = image_meta.get('location') sources = [obj for obj in (copy_from, location, image_data) if obj] if len(sources) &gt;= 2: msg = _("It's invalid to provide multiple image sources.") LOG.warn(msg) raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") if len(sources) == 0: return image_meta if image_data: #第一种情况 image_meta = self._validate_image_for_activation(req, image_id, image_meta) image_meta = self._upload_and_activate(req, image_meta) elif copy_from: #第二种情况，异步上传 msg = _LI('Triggering asynchronous copy from external source') LOG.info(msg) pool = common.get_thread_pool("copy_from_eventlet_pool") pool.spawn_n(self._upload_and_activate, req, image_meta) else: #第三种情况，不上传，修改metadata location,以及status if location: self._validate_image_for_activation(req, image_id, image_meta) image_size_meta = image_meta.get('size') if image_size_meta: try: image_size_store = store.get_size_from_backend( location, req.context) except (store.BadStoreUri, store.UnknownScheme) as e: LOG.debug(encodeutils.exception_to_unicode(e)) raise HTTPBadRequest(explanation=e.msg, request=req, content_type="text/plain") # NOTE(zhiyan): A returned size of zero usually means # the driver encountered an error. In this case the # size provided by the client will be used as-is. if (image_size_store and image_size_store != image_size_meta): msg = (_("Provided image size must match the stored" " image size. (provided size: %(ps)d, " "stored size: %(ss)d)") % &#123;"ps": image_size_meta, "ss": image_size_store&#125;) LOG.warn(msg) raise HTTPConflict(explanation=msg, request=req, content_type="text/plain") location_data = &#123;'url': location, 'metadata': &#123;&#125;, 'status': 'active'&#125; image_meta = self._activate(req, image_id, location_data) return image_meta 看上面这段代码其实就是之前描述的四种情况： 1. 如果是本地文件，直接上传，image status 为queued,上传成功，queued 更改为active。 2. 如果是外部文件（如在swift或ceph等），如果使用这个header:x-glance-api-copy-from,那么会异步上传，status同1. 3. 如果是外部文件，使用的header是x-image-meta-location，那么就不用上传了，但是image metedata 中的location 要更改为对应的外部地址。 4. image文件是not available的话，不上传，但image metedata 会有，image status 为queued。 参考文章openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--glanceApi 源代码解读(一)]]></title>
      <url>%2F2016%2F12%2F15%2Fglance-api-sourcecode-explain-1%2F</url>
      <content type="text"><![CDATA[目录结构：glanceApi 介绍 glanceApi 启动过程代码解读 glanceApi 介绍 glanceApi主要作用就是启动一个WSGI Server,接受restful请求，路由给对应V1/V2版本的不同执行函数。 因为版本不同，代码也可能不同，该版本为Openstack Liberty glanceApi 启动过程代码解读 首先看下glanceAPI 启动代码入口： 1234567891011121314151617181920212223242526272829def main(): try: #找到配置文件并读取配置文件中的值到config.CONF中 config.parse_args() #设置默认值 config.set_config_defaults() #设置协程调度使用epoll,若不支持，使用selects wsgi.set_eventlet_hub() logging.setup(CONF, 'glance') notifier.set_defaults() #osprofiler的使用与Ceilometer性能分析有关，此处略过 if cfg.CONF.profiler.enabled: _notifier = osprofiler.notifier.create("Messaging", oslo_messaging, &#123;&#125;, notifier.get_transport(), "glance", "api", cfg.CONF.bind_host) osprofiler.notifier.set(_notifier) osprofiler.web.enable(cfg.CONF.profiler.hmac_keys) else: osprofiler.web.disable() #初始化WSGI Server server = wsgi.Server(initialize_glance_store=True) #先加载paste文件，生成app,然后启动WSGI Serve server.start(config.load_paste_app('glance-api'), default_port=9292) #循环等待请求 server.wait() except KNOWN_EXCEPTIONS as e: fail(e) 看下WSGI Server初始化的过程： 123456789101112131415161718192021222324class Server(object): """Server class to manage multiple WSGI sockets and applications. This class requires initialize_glance_store set to True if glance store needs to be initialized. """ def __init__(self, threads=1000, initialize_glance_store=False): os.umask(0o27) # ensure files are created with the correct privileges self._logger = logging.getLogger("eventlet.wsgi.server") self.threads = threads self.children = set() self.stale_children = set() self.running = True # NOTE(abhishek): Allows us to only re-initialize glance_store when # the API's configuration reloads. #注意此处的参数会在start时用于初始化glance的后端存储 self.initialize_glance_store = initialize_glance_store self.pgid = os.getpid() try: # NOTE(flaper87): Make sure this process # runs in its own process group. os.setpgid(self.pgid, self.pgid) except OSError: self.pgid = 0 启动Server之前会加载paste文件，看下代码： 123456789101112131415161718192021222324252627282930def load_paste_app(app_name, flavor=None, conf_file=None): """ Builds and returns a WSGI app from a paste config file. """ # append the deployment flavor to the application name, # in order to identify the appropriate paste pipeline app_name += _get_deployment_flavor(flavor) if not conf_file: conf_file = _get_deployment_config_file() try: logger = logging.getLogger(__name__) logger.debug("Loading %(app_name)s from %(conf_file)s", &#123;'conf_file': conf_file, 'app_name': app_name&#125;) app = deploy.loadapp("config:%s" % conf_file, name=app_name) # Log the options used when starting if we're in debug mode... if CONF.debug: CONF.log_opt_values(logger, logging.DEBUG) return app except (LookupError, ImportError) as e: msg = (_("Unable to load %(app_name)s from " "configuration file %(conf_file)s." "\nGot: %(e)r") % &#123;'app_name': app_name, 'conf_file': conf_file, 'e': e&#125;) logger.error(msg) raise RuntimeError(msg) 不解释了，code explain everything. Server start 的过程： 123456789101112131415161718192021222324252627282930313233343536373839def start(self, application, default_port): """ Run a WSGI server with the given application. :param application: The application to be run in the WSGI server :param default_port: Port to bind to if none is specified in conf """ self.application = application self.default_port = default_port self.configure() self.start_wsgi()def configure(self, old_conf=None, has_changed=None): """ Apply configuration settings :param old_conf: Cached old configuration settings (if any) :param has changed: callable to determine if a parameter has changed """ eventlet.wsgi.MAX_HEADER_LINE = CONF.max_header_line self.client_socket_timeout = CONF.client_socket_timeout or None self.configure_socket(old_conf, has_changed) #初始化后端存储 if self.initialize_glance_store: initialize_glance_store()def start_wsgi(self): #根据配置文件指定worker数创建Server 进程 workers = get_num_workers() if workers == 0: # Useful for profiling, test, debug etc. self.pool = self.create_pool() self.pool.spawn_n(self._single_run, self.application, self.sock) return else: LOG.info(_LI("Starting %d workers"), workers) signal.signal(signal.SIGTERM, self.kill_children) signal.signal(signal.SIGINT, self.kill_children) signal.signal(signal.SIGHUP, self.hup) while len(self.children) &lt; workers: self.run_child() 再看下初始化后端存储部分，initialize_glance_store方法：12345def initialize_glance_store(): """Initialize glance store.""" glance_store.register_opts(CONF) glance_store.create_stores(CONF) glance_store.verify_default_store() 执行了三个glance_store导入函数，实现根据配置文件的指定，加载相应的后端存储。这里提一下glance-store项目，这个项目之前跟glance是在一块的，J版本之后就独立出来。看了一下github上的README,说该库提供的接口不是特别的稳定，有一些缺陷，可能最终会重写这部分，所以，就不详细展开了。 参考文章openstack 设计与实现 glance-api 服务启动流程(官方Kilo版本) glance-store END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--glance 源代码整体解读]]></title>
      <url>%2F2016%2F12%2F14%2Fglance-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：glance 介绍 glance API 详解 glance 整体代码解读 glance 介绍 openstack glance项目提供restful接口实现了虚拟机镜像的发现，注册，检索等功能。它本身不负责镜像的存储，提供driver接口，依赖于swift等其他项目完成镜像的存储。 因为版本不同，代码也可能不同，该版本为Openstack Liberty glance API 详解 glance api 的版本目前如下： v1版本逐渐弃用，目前v2版本用的比较多，v3版本正在开发中。v1与v2版的区别还是比较大的，不只是添加了很多API。v1版本中glance-api与glance-registry都是一个WSGI server,glance-api负责接收用户的restful请求，如果该请求是与metedata相关的，则将其转发给glance-registry，glance-registry会解析请求并与数据库交互，如果该请求是与image自身存取相关的，则直接转发给store backend。V2版本中，glance-registry服务的内容被整合进了glance-api中，采用责任链的模式实现API的处理流程。在下面结合实例分析的时候会更清晰。以下示例图来自网上，正确性不保证： 关于API 的详细介绍在这个页面check this glance 整体代码解读 看下setup.cfg中的启动脚本内容： 1234567891011121314[entry_points]console_scripts = glance-api = glance.cmd.api:main glance-cache-prefetcher = glance.cmd.cache_prefetcher:main glance-cache-pruner = glance.cmd.cache_pruner:main glance-cache-manage = glance.cmd.cache_manage:main glance-cache-cleaner = glance.cmd.cache_cleaner:main glance-control = glance.cmd.control:main glance-manage = glance.cmd.manage:main glance-registry = glance.cmd.registry:main glance-replicator = glance.cmd.replicator:main glance-scrubber = glance.cmd.scrubber:main glance-glare = glance.cmd.glare:main .................. 1，glance-cache-* :四个对image cache进行管理的工具，pruner用于执行一些周期性任务，cleaner用于清理cache文件释放空间。 2，glance-manage :执行对glance数据库的操作。 3，glance-replicator:实现镜像的复制功能 4，glance-scrubber:清理已经删除的image 5, glance-control:glance 提供了glance-api,glance-registry 两个WSGI Server,以及一个glance-scrubber后台服务进程，这里的glance-control 就是用于控制这三个服务进程（start,stop,restart）。 其实这里glance-api在初始时，还会导入一个叫做glance-store的项目，以便初始化后台的存储系统，glance通过glance-store这个框架所提供的接口，实现了对各种不同存储系统的支持。 glance的主要操作对象是image，image的metedata存储在数据库，真实的image数据存储在后端存储系统（如swift,ceph）。 image的主要metedata有：id,name,owner,size,location,disk_format,container_format(一般设置为bare),status。 image的访问权限： public 公共的：可以被所有的 tenant 使用。 private 私有的/项目的：只能被 image owner 所在的 tenant 使用。 shared 共享的：一个非共有的image 可以 共享给另外的 tenant，可通过member-* 操作来实现。 protected 受保护的：protected 的 image 不能被删除。 image的status变更： glance 中的task概念：在V2版本的API中提出了task的概念，关于为什么提出task的概念以可看这篇博客Getting started with Tasks API in Glance。 总结一下就是： 要给用户一个上传/下载镜像等操作的接口，但是目前的接口只是单纯地实现了功能，过程是不可控的，要是用户上传的不是镜像文件怎么办，甚至是恶意文件怎么办，执行过程中出现错误怎么办（没有反馈机制，用户是不可见的），本身镜像文件是非常大的，上传/下载占用太多时间怎么办。 因此针对这种耗时较长且不可控的操作，抽象出了task的概念。task是针对image的异步操作，本身具有一些属性如id,owner，status等。 task的status大致有：pending(task被创建，但未执行)，processing(task执行中),success(task成功结束),failure(执行失败)。 目前task支持的操作包括：import（用户上传镜像）,export（用户下载镜像），clone（region之间clone,注：紧紧跟随AWS啊）。具体的task api 可以看之前的参考页面。 参考文章openstack 设计与实现 openstack glance github Image Service API Getting started with Tasks API in Glance wiki–glance-task-api END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--novaCompute 源代码解读]]></title>
      <url>%2F2016%2F12%2F09%2FnovaComputer-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：novacompute 介绍 以nova boot 为例解读代码 novacompute 整体讲解 novacompute 介绍 novacomputer 完成的主要任务是虚拟机的生命周期管理。 目前支持的虚拟化包括libvirt (KVM, Xen, LXC and more), Hyper-V, VMware, XenServer 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 首先我们先把之前解读novaconductor时发送rpc消息给novacompute的代码拿过来nova/conductor/manager.py.ComputeTaskManager.build_instances： 1234567891011#发送异步消息给nova-compute，完成instance的bootself.compute_rpcapi.build_and_run_instance(context, instance=instance, host=host['host'], image=image, request_spec=request_spec, filter_properties=local_filter_props, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=bdms, node=host['nodename'], limits=host['limits']) 可以看到novaconductor 引入novacomputer中的rpcapi.py文件，调用ComputeAPI.build_and_run_instance方法，看下该方法： 12345678910111213141516171819def build_and_run_instance(self, ctxt, instance, host, image, request_spec, filter_properties, admin_password=None, injected_files=None, requested_networks=None, security_groups=None, block_device_mapping=None, node=None, limits=None): version = '4.0' #创建一个指定host的RPCClient cctxt = self.router.by_host(ctxt, host).prepare( server=host, version=version) #发送异步rpc请求 cctxt.cast(ctxt, 'build_and_run_instance', instance=instance, image=image, request_spec=request_spec, filter_properties=filter_properties, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=block_device_mapping, node=node, limits=limits) 由之前分析可知，rpc的请求被服务端接收后，真正执行函数的地方在manager.py这个文件里，找到执行函数： 12345678910111213141516171819202122232425262728@wrap_exception() #捕获所有exception的装饰器@reverts_task_state #一旦build instance failure 会还原 task-state@wrap_instance_fault #捕获所有instance相关的异常def build_and_run_instance(self, context, instance, image, request_spec, filter_properties, admin_password=None, injected_files=None, requested_networks=None, security_groups=None, block_device_mapping=None, node=None, limits=None): #保证某一instance的任务是同步执行的 @utils.synchronized(instance.uuid) def _locked_do_build_and_run_instance(*args, **kwargs): # NOTE(danms): We grab the semaphore with the instance uuid # locked because we could wait in line to build this instance # for a while and we want to make sure that nothing else tries # to do anything with this instance while we wait. with self._build_semaphore: #最终调用该函数 self._do_build_and_run_instance(*args, **kwargs) # NOTE(danms): We spawn here to return the RPC worker thread back to # the pool. Since what follows could take a really long time, we don't # want to tie up RPC workers. #利用eventlet新建一个线程执行_locked_do_build_and_run_instance函数 utils.spawn_n(_locked_do_build_and_run_instance, context, instance, image, request_spec, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, node, limits) 看下最后执行的_do_build_and_run_instance函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@hooks.add_hook('build_instance')@wrap_exception()@reverts_task_state@wrap_instance_event(prefix='compute')@wrap_instance_faultdef _do_build_and_run_instance(self, context, instance, image, request_spec, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, node=None, limits=None): try: LOG.debug('Starting instance...', instance=instance) instance.vm_state = vm_states.BUILDING instance.task_state = None #更新task_stat instance.save(expected_task_state= (task_states.SCHEDULING, None)) except exception.InstanceNotFound: msg = 'Instance disappeared before build.' LOG.debug(msg, instance=instance) return build_results.FAILED except exception.UnexpectedTaskStateError as e: LOG.debug(e.format_message(), instance=instance) return build_results.FAILED # b64 decode the files to inject: decoded_files = self._decode_files(injected_files) if limits is None: limits = &#123;&#125; if node is None: #由具体实现的driver（如libvirtDriver）获取nodename,因为hypervisor不一样，有的hypervisor其实就一个node(如libvirt),而有的则是多个node. node = self.driver.get_available_nodes(refresh=True)[0] LOG.debug('No node specified, defaulting to %s', node, instance=instance) try: #设定一个时间检测，调用_build_and_run_instance with timeutils.StopWatch() as timer: self._build_and_run_instance(context, instance, image, decoded_files, admin_password, requested_networks, security_groups, block_device_mapping, node, limits, filter_properties) LOG.info(_LI('Took %0.2f seconds to build instance.'), timer.elapsed(), instance=instance) return build_results.ACTIVE except exception.RescheduledException as e: retry = filter_properties.get('retry') if not retry: # no retry information, do not reschedule. LOG.debug("Retry info not present, will not reschedule", instance=instance) self._cleanup_allocated_networks(context, instance, requested_networks) compute_utils.add_instance_fault_from_exc(context, instance, e, sys.exc_info(), fault_message=e.kwargs['reason']) self._nil_ou t_instance_obj_host_and_node(instance) self._set_instance_obj_error_state(context, instance, clean_task_state=True) return build_results.FAILED .................................... 接着往下看self._build_and_run_instance函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def _build_and_run_instance(self, context, instance, image, injected_files, admin_password, requested_networks, security_groups, block_device_mapping, node, limits, filter_properties): #image是一个包含镜像信息的字典，‘name’是镜像的名字 image_name = image.get('name') self._notify_about_instance_usage(context, instance, 'create.start', extra_usage_info=&#123;'image_name': image_name&#125;) self._check_device_tagging(requested_networks, block_device_mapping) try: # #获取/创建ResourceTracker实例，为后续的资源申请做准备（此处涉及的resource Tracker Claim机制后面后详细讲） rt = self._get_resource_tracker(node) with rt.instance_claim(context, instance, limits): # NOTE(russellb) It's important that this validation be done # *after* the resource tracker instance claim, as that is where # the host is set on the instance. #group_policy包含两种affinity 和 anti-affinity，在创建server_group的时候指定。 #创建虚拟机时指定server_group,会根据group_policy倾向于创建在同一台host或不同host self._validate_instance_group_policy(context, instance, filter_properties) image_meta = objects.ImageMeta.from_dict(image) #为云主机申请网络资源，完成块设备验证及映射 with self._build_resources(context, instance, requested_networks, security_groups, image_meta, block_device_mapping) as resources: instance.vm_state = vm_states.BUILDING instance.task_state = task_states.SPAWNING # NOTE(JoshNang) This also saves the changes to the # instance from _allocate_network_async, as they aren't # saved in that function to prevent races. #更新实例状态 instance.save(expected_task_state= task_states.BLOCK_DEVICE_MAPPING) block_device_info = resources['block_device_info'] network_info = resources['network_info'] LOG.debug('Start spawning the instance on the hypervisor.', instance=instance) with timeutils.StopWatch() as timer: #调用hypervisor的spawn方法启动instance，如果使用libvirt的话，会调用nova/virt/libvirt/driver.py/libvirtDriver.spawn self.driver.spawn(context, instance, image_meta, injected_files, admin_password, network_info=network_info, block_device_info=block_device_info) LOG.info(_LI('Took %0.2f seconds to spawn the instance on ' 'the hypervisor.'), timer.elapsed(), instance=instance) except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e: 看下nova/virt/libvirt/driver.py/libvirtDriver.spawn 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def spawn(self, context, instance, image_meta, injected_files, admin_password, network_info=None, block_device_info=None): #根据模拟器类型，获取块设备及光驱的总线类型 #默认使用kvm，所以：块设备默认使用virtio；光驱默认使用ide；并且根据block_device_info设置设备映射 #最后返回包含&#123;disk_bus,cdrom_bus,mapping&#125;的字典 disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type, instance, image_meta, block_device_info) gen_confdrive = functools.partial(self._create_configdrive, context, instance, admin_pass=admin_password, files=injected_files, network_info=network_info) #从glance下载镜像（如果本地_base目录没有的话），然后上传到后端存储 self._create_image(context, instance, disk_info['mapping'], network_info=network_info, block_device_info=block_device_info, files=injected_files, admin_pass=admin_password) # Required by Quobyte CI self._ensure_console_log_for_instance(instance) #生成libvirt xml文件（libvirt创建虚拟机使用） xml = self._get_guest_xml(context, instance, network_info, disk_info, image_meta, block_device_info=block_device_info) #调用libvirt启动实例 self._create_domain_and_network( context, xml, instance, network_info, disk_info, block_device_info=block_device_info, post_xml_callback=gen_confdrive) LOG.debug("Instance is running", instance=instance) def _wait_for_boot(): """Called at an interval until the VM is running.""" state = self.get_info(instance).state if state == power_state.RUNNING: LOG.info(_LI("Instance spawned successfully."), instance=instance) raise loopingcall.LoopingCallDone() #等待实例创建结果（通过libvirt获取云主机状态判断） timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot) timer.start(interval=0.5).wait() 关于具体创建系统磁盘的代码_create_image，生成libvirt xml，调用libvirt启动实例的代码不再分析，可以参考这篇文章check this。 novacompute 整体讲解 novacomputer 代码目录如下： Resource Tracker:nova-computer 会维护数据库中一个computer_nodes的表用来存储主机的资源使用情况，以便nova-scheduler获取作为选择主机的依据，这就要求每次创建，迁移，删除虚拟机时都要对数据库中的的computer_node进行更新。nova-computer会为每一个主机创建一个ResourceTracker对象，更新computernode对象。目前是有两种更新机制：一种是ResourceTracker的claim机制，一种是使用周期性任务（Periodic Task） Claim机制：当一台主机被多个nova-scheduler同时选中并发出创建虚拟机的请求时，该主机并不一定有足够的资源满足创建要求。所以Claim机制就是在创建虚拟机之前先测试下是否满足新建需求，满足，则更新数据库，并将虚拟机申请的资源从主机可用的资源减掉，如果后来创建失败，或者虚拟机删除时，会通过claim加上之前减掉的部分。 @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE) def instance_claim(self, context, instance, limits=None): if self.disabled: # compute_driver doesn't support resource tracking, just # set the 'host' and node fields and continue the build: self._set_instance_host_and_node(instance) return claims.NopClaim() # sanity checks: if instance.host: LOG.warning(_LW("Host field should not be set on the instance " "until resources have been claimed."), instance=instance) if instance.node: LOG.warning(_LW("Node field should not be set on the instance " "until resources have been claimed."), instance=instance) #允许超配创建虚拟机 # get the overhead required to build this instance: overhead = self.driver.estimate_instance_overhead(instance) LOG.debug("Memory overhead for %(flavor)d MB instance; %(overhead)d " "MB", {'flavor': instance.flavor.memory_mb, 'overhead': overhead['memory_mb']}) LOG.debug("Disk overhead for %(flavor)d GB instance; %(overhead)d " "GB", {'flavor': instance.flavor.root_gb, 'overhead': overhead.get('disk_gb', 0)}) pci_requests = objects.InstancePCIRequests.get_by_instance_uuid( context, instance.uuid) #如果Claim返回none表示主机的资源满足不了新建虚拟机的需求， #会调用__exit__()方法将占用的资源返还到主机可用资源中。 claim = claims.Claim(context, instance, self, self.compute_node, pci_requests, overhead=overhead, limits=limits) # self._set_instance_host_and_node() will save instance to the DB # so set instance.numa_topology first. We need to make sure # that numa_topology is saved while under COMPUTE_RESOURCE_SEMAPHORE # so that the resource audit knows about any cpus we've pinned. instance_numa_topology = claim.claimed_numa_topology instance.numa_topology = instance_numa_topology #通过Object model，若不在同一节点则调用ConductorAPI 更新instance的host,node等属性。 self._set_instance_host_and_node(instance) if self.pci_tracker: # NOTE(jaypipes): ComputeNode.pci_device_pools is set below # in _update_usage_from_instance(). self.pci_tracker.claim_instance(context, pci_requests, instance_numa_topology) # Mark resources in-use and update stats #根据新建虚拟机的需求计算主机可用资源。 self._update_usage_from_instance(context, instance) elevated = context.elevated() # persist changes to the compute node: #根据上面的计算结果更新数据库。 self._update(elevated) return claim 使用Periodic Task:在类nova.compute.manager.ComputeManager中有个周期性的任务update_available_resource()用于更新主机可用资源。顺便提一点，nova-compute在启动的时候会启动两个周期任务，一个用于更新主机可用资源update_available_resource，一个用于汇报本机nova-compute的服务状态给数据库（report state）。 #根据配置文件周期性的调用该函数。 #该周期性的任务用于同步数据库内可用资源与hypervisor保持一致 @periodic_task.periodic_task(spacing=CONF.update_resources_interval) def update_available_resource(self, context): """See driver.get_available_resource() Periodic process that keeps that the compute host's understanding of resource availability and usage in sync with the underlying hypervisor. :param context: security context """ compute_nodes_in_db = self._get_compute_nodes_in_db(context, use_slave=True) nodenames = set(self.driver.get_available_nodes()) #更新所有主机数据库中的资源数据。 for nodename in nodenames: self.update_available_resource_for_node(context, nodename) self._resource_tracker_dict = { k: v for k, v in self._resource_tracker_dict.items() if k in nodenames} # Delete orphan compute node not reported by driver but still in db #清除inactive但仍在数据库中存有数据的计算节点信息 for cn in compute_nodes_in_db: if cn.hypervisor_hostname not in nodenames: LOG.info(_LI("Deleting orphan compute node %s"), cn.id) cn.destroy() 这两种机制并不冲突，一种是在新建或迁移虚拟机等涉及计算节点资源操作时，在当前数据库中数据的基础上更新，，保证数据库里的资源及时更新，以便为nova-scheduler 提供最新数据。周期性任务是保证hypervisor获取的资源数据与数据库中的数据保持一致。 关于resource tracker 如何更新数据库的代码分析 nova-compute Periodic tasks 机制 参考文章openstack 设计与实现 Openstack liberty源码分析 之 云主机的启动过程3 nova-compute Periodic tasks 机制 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--novaScheduler 源代码解读]]></title>
      <url>%2F2016%2F12%2F07%2FnovaScheduler-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：novaScheduler 介绍 以nova boot 为例解读代码 novascheduler 整体讲解 Filtering 讲解 Weighting 讲解 novaScheduler 介绍 关于nova不再赘述，之前文章由介绍。 nova-scheduler的主要作用就是就是根据各种规则为虚拟机选择一个合适的主机。 同nova-volume被单独剥离为cinder项目一样，社区也致力于将nova-scheduler剥离为gantt,作为一个通用的调度服务。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 由之前解读novaConductor时，我们把牵扯到调度novascheduler的部分拿过来： 12345678910def _schedule_instances(self, context, request_spec, filter_properties): scheduler_utils.setup_instance_group(context, request_spec, filter_properties) # TODO(sbauza): Hydrate here the object until we modify the # scheduler.utils methods to directly use the RequestSpec object spec_obj = objects.RequestSpec.from_primitives( context, request_spec, filter_properties) #调用scheduler_client,获取可用host列表 hosts = self.scheduler_client.select_destinations(context, spec_obj) return hosts 找到scheduler_client.select_destinations函数 1234#有一个装饰器，当发送失败的时候retry@utils.retry_select_destinationsdef select_destinations(self, context, spec_obj): return self.queryclient.select_destinations(context, spec_obj) 最终调用rpcapi中的select_destinations方法： 12345678910111213def select_destinations(self, ctxt, spec_obj): version = '4.3' msg_args = &#123;'spec_obj': spec_obj&#125; if not self.client.can_send_version(version): del msg_args['spec_obj'] msg_args['request_spec'] = spec_obj.to_legacy_request_spec_dict() msg_args['filter_properties' ] = spec_obj.to_legacy_filter_properties_dict() version = '4.0' #生成 RPCClient cctxt = self.client.prepare(version=version) #发送rpc call 同步请求 return cctxt.call(ctxt, 'select_destinations', **msg_args) 接下来看下novascheduler 接收到’select_destinations’ 的rpc请求之后的动作，之前说Novaconductor的时候说到接受请求执行的函数都在manager文件中。 12345678910111213141516@messaging.expected_exceptions(exception.NoValidHost)def select_destinations(self, ctxt, request_spec=None, filter_properties=None, spec_obj=_sentinel): """Returns destinations(s) best suited for this RequestSpec. The result should be a list of dicts with 'host', 'nodename' and 'limits' as keys. """ if spec_obj is self._sentinel: spec_obj = objects.RequestSpec.from_primitives(ctxt, request_spec, filter_properties) #调用driver中的方法，该driver即典型的stevedore中的driver插件导入 dests = self.driver.select_destinations(ctxt, spec_obj) return jsonutils.to_primitive(dests) 看下driver的相关代码： 12345678910def __init__(self, scheduler_driver=None, *args, **kwargs): if not scheduler_driver: scheduler_driver = CONF.scheduler.driver #根据nova配置文件选取scheduler_driver，利用stevedore的driver载入方式载入setup.cfg中对应scheduler_driver。 self.driver = driver.DriverManager( "nova.scheduler.driver", scheduler_driver, invoke_on_load=True).driver super(SchedulerManager, self).__init__(service_name='scheduler', *args, **kwargs) 一般setup.cfg 中对应的scheduler.driver有： 12345nova.scheduler.driver = filter_scheduler = nova.scheduler.filter_scheduler:FilterScheduler caching_scheduler = nova.scheduler.caching_scheduler:CachingScheduler chance_scheduler = nova.scheduler.chance:ChanceScheduler fake_scheduler = nova.tests.unit.scheduler.fakes:FakeScheduler 默认获取到的scheduler.driver是filter_scheduler，关于stevedore的用法，check this 找到filter_scheduler的select_destinations方法，该方法做一些参数的处理，然后调用_schedule方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _schedule(self, context, spec_obj): """Returns a list of hosts that meet the required specs, ordered by their fitness. """ elevated = context.elevated() config_options = self._get_configuration_options() #获取所有的host列表 hosts = self._get_all_host_states(elevated) selected_hosts = [] num_instances = spec_obj.num_instances # NOTE(sbauza): Adding one field for any out-of-tree need spec_obj.config_options = config_options #此处num_instances 猜测是新建虚拟机的个数，因为 boot 命令中有一个参数新建多个虚拟机 for num in range(num_instances): # Filter local hosts based on requirements ... #根据过滤条件筛选符合filter 的host，具体filter代码位于filters/目录下 hosts = self.host_manager.get_filtered_hosts(hosts, spec_obj, index=num) if not hosts: # Can't get any more locally. break LOG.debug("Filtered %(hosts)s", &#123;'hosts': hosts&#125;) #根据weight条件将host列表按权重排序，具体weight代码位于weights/目录下 weighed_hosts = self.host_manager.get_weighed_hosts(hosts, spec_obj) LOG.debug("Weighed %(hosts)s", &#123;'hosts': weighed_hosts&#125;) host_subset_size = max(1, CONF.filter_scheduler.host_subset_size) if host_subset_size &lt; len(weighed_hosts): weighed_hosts = weighed_hosts[0:host_subset_size] chosen_host = random.choice(weighed_hosts) LOG.debug("Selected host: %(host)s", &#123;'host': chosen_host&#125;) selected_hosts.append(chosen_host) # Now consume the resources so the filter/weights # will change for the next instance. chosen_host.obj.consume_from_request(spec_obj) if spec_obj.instance_group is not None: spec_obj.instance_group.hosts.append(chosen_host.obj.host) # hosts has to be not part of the updates when saving spec_obj.instance_group.obj_reset_changes(['hosts']) return selected_hosts 先看下获取所有的host列表的代码_get_all_host_states，直接调用host_manager.get_all_host_states， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def get_all_host_states(self, context): """Returns a list of HostStates that represents all the hosts the HostManager knows about. Also, each of the consumable resources in HostState are pre-populated and adjusted based on data in the db. """ #从数据库获取nova-computer服务的service列表 service_refs = &#123;service.host: service for service in objects.ServiceList.get_by_binary( context, 'nova-compute', include_disabled=True)&#125; # Get resource usage across the available compute nodes: #从数据库获取所有计算节点的资源使用情况 compute_nodes = objects.ComputeNodeList.get_all(context) seen_nodes = set() #更新计算节点的资源使用情况 for compute in compute_nodes: service = service_refs.get(compute.host) if not service: LOG.warning(_LW( "No compute service record found for host %(host)s"), &#123;'host': compute.host&#125;) continue host = compute.host node = compute.hypervisor_hostname state_key = (host, node) host_state = self.host_state_map.get(state_key) if not host_state: #获取host_state,如果本身维护的数据更新时间比数据库的早晚，就不用从数据库获取了。 host_state = self.host_state_cls(host, node, compute=compute) self.host_state_map[state_key] = host_state # We force to update the aggregates info each time a new request # comes in, because some changes on the aggregates could have been # happening after setting this field for the first time host_state.update(compute, dict(service), self._get_aggregates_info(host), self._get_instance_info(context, compute)) seen_nodes.add(state_key) # remove compute nodes from host_state_map if they are not active #去掉状态为inactive的node dead_nodes = set(self.host_state_map.keys()) - seen_nodes for state_key in dead_nodes: host, node = state_key LOG.info(_LI("Removing dead compute node %(host)s:%(node)s " "from scheduler"), &#123;'host': host, 'node': node&#125;) del self.host_state_map[state_key] #返回一个node的迭代器 return six.itervalues(self.host_state_map) 再看下根据filter过滤host的函数host_manager.get_filtered_hosts 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def get_filtered_hosts(self, hosts, spec_obj, index=0): """Filter hosts and return only ones passing all filters.""" #若filter_properties中指定了ignore_hosts,则排除相应host def _strip_ignore_hosts(host_map, hosts_to_ignore): ................................. ##若filter_properties中指定了forced_hosts,则排除不在forced_hosts中的host def _match_forced_hosts(host_map, hosts_to_force): ................................. #若filter_properties中指定了forced_nodes,则排除不在forced_nodes中的host def _match_forced_nodes(host_map, nodes_to_force): ................................. #若指定了requested_destination，则在指定requested_destination中filter host def _get_hosts_matching_request(hosts, requested_destination): .................................. ignore_hosts = spec_obj.ignore_hosts or [] force_hosts = spec_obj.force_hosts or [] force_nodes = spec_obj.force_nodes or [] requested_node = spec_obj.requested_destination if requested_node is not None: # NOTE(sbauza): Reduce a potentially long set of hosts as much as # possible to any requested destination nodes before passing the # list to the filters hosts = _get_hosts_matching_request(hosts, requested_node) if ignore_hosts or force_hosts or force_nodes: # NOTE(deva): we can't assume "host" is unique because # one host may have many nodes. name_to_cls_map = &#123;(x.host, x.nodename): x for x in hosts&#125; if ignore_hosts: _strip_ignore_hosts(name_to_cls_map, ignore_hosts) if not name_to_cls_map: return [] # NOTE(deva): allow force_hosts and force_nodes independently if force_hosts: _match_forced_hosts(name_to_cls_map, force_hosts) if force_nodes: _match_forced_nodes(name_to_cls_map, force_nodes) if force_hosts or force_nodes: # NOTE(deva): Skip filters when forcing host or node if name_to_cls_map: return name_to_cls_map.values() else: return [] #获得经过上述条件后满足条件的host列表 hosts = six.itervalues(name_to_cls_map) #调用配置文件中规定的filter对host列表进行过滤，筛选host return self.filter_handler.get_filtered_objects(self.enabled_filters, hosts, spec_obj, index) 再看下根据weight条件将host列表按权重排序host_manager.get_weighed_hosts方法，直接调用host_manager.get_weighed_hosts： 12345678910111213141516171819202122def get_weighed_objects(self, weighers, obj_list, weighing_properties): """Return a sorted (descending), normalized list of WeighedObjects.""" #获取WeighedObject对象 ，就是对host做一层抽象 weighed_objs = [self.object_class(obj, 0.0) for obj in obj_list] #如果只有一个WeighedObject，直接返回 if len(weighed_objs) &lt;= 1: return weighed_objs #逐一调用各权重过滤器。 for weigher in weighers: weights = weigher.weigh_objects(weighed_objs, weighing_properties) # Normalize the weights weights = normalize(weights, minval=weigher.minval, maxval=weigher.maxval) #累加权重值 for i, weight in enumerate(weights): obj = weighed_objs[i] obj.weight += weigher.weight_multiplier() * weight #根据权重值降序排列并返回 return sorted(weighed_objs, key=lambda x: x.weight, reverse=True) 到这里，Novascheduler的分析就基本结束了。 novascheduler 整体讲解 novascheduler 源码目录架构： nova scheduler实现了三种调度器：ChanceScheduler(随机调度器)，FilterScheduler(过滤调度器)，CachingScheduler(缓存调度器，在FilterScheduler的基础上将主机资源缓存在本地，通过后台定时任务从数据库更新缓存信息)。如果我们想自己实现定制的调度器，需要实现的接口在nova.scheduler.driver.Scheduler,继承类SchedulerDriver. 一般来说，scheduler的过程大致是这样的：从nova.scheduler.rpcapi.SchedulerAPI发送RPC请求到nova.scheduler.manager.SchedulerManager;从SchedulerManager到调度器（类SchedulerDriver）;从SchedulerDriver到Filters;从Filters到权重计算排序Weights。 Filtering 讲解 所有的filter都在/nova/scheduler/filters目录，继承自nova.scheduler.filters.BaseHostFilter。实现一个host_passes()函数。 1234567891011121314151617181920212223242526272829303132class DiskFilter(filters.BaseHostFilter): """Disk Filter with over subscription flag.""" def _get_disk_allocation_ratio(self, host_state, spec_obj): return host_state.disk_allocation_ratio #重点在实现该函数 def host_passes(self, host_state, spec_obj): """Filter based on disk usage.""" requested_disk = (1024 * (spec_obj.root_gb + spec_obj.ephemeral_gb) + spec_obj.swap) free_disk_mb = host_state.free_disk_mb total_usable_disk_mb = host_state.total_usable_disk_gb * 1024 # Do not allow an instance to overcommit against itself, only against # other instances. In other words, if there isn't room for even just # this one instance in total_usable_disk space, consider the host full. if total_usable_disk_mb &lt; requested_disk: LOG.debug("%(host_state)s does not have %(requested_disk)s " "MB usable disk space before overcommit, it only " "has %(physical_disk_size)s MB.", &#123;'host_state': host_state, 'requested_disk': requested_disk, 'physical_disk_size': total_usable_disk_mb&#125;) return False ............................... disk_gb_limit = disk_mb_limit / 1024 host_state.limits['disk_gb'] = disk_gb_limit return True Weighting 讲解 所有weighter位于nova/scheduler/weights目录，继承自weights.BaseWeighter; 1234567891011class RAMWeigher(weights.BaseHostWeigher): #可以设置maxval ,minval指明权重的最大最小值。 minval = 0 #权重的系数，最终排序时需要将各种weighter得到的权重乘上对应的系数，有多个weighter时才有意义，可以通过配置选项ram_weight_multiplier配置，默认为1.0 def weight_multiplier(self): """Override the weight multiplier.""" return CONF.filter_scheduler.ram_weight_multiplier #计算权重值，对于RAMWeighter直接返回可用内存大小即可 def _weigh_object(self, host_state, weight_properties): """Higher weights win. We want spreading to be the default.""" return host_state.free_ram_mb 参考文章openstack 设计与实现 Openstack liberty源码分析 之 云主机的启动过程2 学习Python动态扩展包stevedore END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--novaConductor 源代码解读]]></title>
      <url>%2F2016%2F12%2F06%2FnovaConductor-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：novaConductor 介绍 以nova boot 为例解读代码 novaConductor 介绍 关于nova不再赘述，之前文章由介绍。 根据wiki介绍，NovaConductor更像是一个编排工具，它会接受novaapi的请求，将创建虚拟机等任务交给novacompute处理，将寻找host的任务交给novascheduler，同时，会负责跟数据库的交互。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 先简略叙述下nova boot 在novaapi的过程，novaapi接收restful请求（post /servers）,根据apipaste配置文件,APIrouter,该请求最终路由到nova/api/openstack/servers.py.ServersController.create ,该方法完成参数的转换，解析，以及policy认证等工作后，调用nova/compute/api.py.API.create，再转给_create_instance 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687def _create_instance(self, context, instance_type, image_href, kernel_id, ramdisk_id, min_count, max_count, display_name, display_description, key_name, key_data, security_groups, availability_zone, user_data, metadata, injected_files, admin_password, access_ip_v4, access_ip_v6, requested_networks, config_drive, block_device_mapping, auto_disk_config, filter_properties, reservation_id=None, legacy_bdm=True, shutdown_terminate=False, check_server_group_quota=False): """Verify all the input parameters regardless of the provisioning strategy being performed and schedule the instance(s) for creation. """ # Normalize and setup some parameters if reservation_id is None: reservation_id = utils.generate_uid('r') security_groups = security_groups or ['default'] min_count = min_count or 1 max_count = max_count or min_count block_device_mapping = block_device_mapping or [] #获取image相关信息（metadata） if image_href: image_id, boot_meta = self._get_image(context, image_href) else: image_id = None boot_meta = self._get_bdm_image_metadata( context, block_device_mapping, legacy_bdm) self._check_auto_disk_config(image=boot_meta, auto_disk_config=auto_disk_config) #生成instance配置 base_options, max_net_count, key_pair = \ self._validate_and_build_base_options( context, instance_type, boot_meta, image_href, image_id, kernel_id, ramdisk_id, display_name, display_description, key_name, key_data, security_groups, availability_zone, user_data, metadata, access_ip_v4, access_ip_v6, requested_networks, config_drive, auto_disk_config, reservation_id, max_count) # max_net_count is the maximum number of instances requested by the # user adjusted for any network quota constraints, including # consideration of connections to each requested network if max_net_count &lt; min_count: raise exception.PortLimitExceeded() elif max_net_count &lt; max_count: LOG.info(_LI("max count reduced from %(max_count)d to " "%(max_net_count)d due to network port quota"), &#123;'max_count': max_count, 'max_net_count': max_net_count&#125;) max_count = max_net_count #确定块设备映射 block_device_mapping = self._check_and_transform_bdm(context, base_options, instance_type, boot_meta, min_count, max_count, block_device_mapping, legacy_bdm) # We can't do this check earlier because we need bdms from all sources # to have been merged in order to get the root bdm. self._checks_for_create_and_rebuild(context, image_id, boot_meta, instance_type, metadata, injected_files, block_device_mapping.root_bdm()) instance_group = self._get_requested_instance_group(context, filter_properties) #创建instance对象，并写入数据库 instances = self._provision_instances(context, instance_type, min_count, max_count, base_options, boot_meta, security_groups, block_device_mapping, shutdown_terminate, instance_group, check_server_group_quota, filter_properties, key_pair) #更新instance状态为create for instance in instances: self._record_action_start(context, instance, instance_actions.CREATE) #调用conductor api，之后会通过conductor rpc将请求转发给conductor manager self.compute_task_api.build_instances(context, instances=instances, image=boot_meta, filter_properties=filter_properties, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=block_device_mapping, legacy_bdm=False) return (instances, reservation_id) -nova/conductor/api.py.ComputeTaskAPI.build_instance 直接将请求转发给nova/conductor/rpcapi.py.ComputeTaskAPI.build_instance。 123456789101112131415161718192021222324252627282930def build_instances(self, context, instances, image, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, legacy_bdm=True): image_p = jsonutils.to_primitive(image) version = '1.10' if not self.client.can_send_version(version): version = '1.9' if 'instance_type' in filter_properties: flavor = filter_properties['instance_type'] flavor_p = objects_base.obj_to_primitive(flavor) filter_properties = dict(filter_properties, instance_type=flavor_p) kw = &#123;'instances': instances, 'image': image_p, 'filter_properties': filter_properties, 'admin_password': admin_password, 'injected_files': injected_files, 'requested_networks': requested_networks, 'security_groups': security_groups&#125; if not self.client.can_send_version(version): version = '1.8' kw['requested_networks'] = kw['requested_networks'].as_tuples() if not self.client.can_send_version('1.7'): version = '1.5' bdm_p = objects_base.obj_to_primitive(block_device_mapping) kw.update(&#123;'block_device_mapping': bdm_p, 'legacy_bdm': legacy_bdm&#125;) #生成RPCClient对象，准备发送 cctxt = self.client.prepare(version=version) #发送异步rpc消息到消息队列，conductor-manager会收到该消息 cctxt.cast(context, 'build_instances', **kw) 顺便说下这个生成的cctxt其实是一个RPCClient对象，在oslo.messaging项目中，调用的call,cast方法就是RPCClient对象对应的方法。 在讲conductor-manage之前，先说一下rabbitmq服务端的创建过程，该过程在服务启动的时候创建。其实server创建的主要目的就是监听端口，而openstack的服务主要通过两种方式调用：AMQP和restful。所以server的创建也主要有两种：一种是监听AMQP请求（Service），一种是监听restful请求（WSGIService）。大致看下nova-conductor的启动过程。 12345678910111213141516def main(): config.parse_args(sys.argv) #解析配置文件 logging.setup(CONF, "nova") # 日志模块 utils.monkey_patch() #eventlet模块的补丁 objects.register_all() # object模块注册（与数据库访问有关） objects.Service.enable_min_version_cache() gmr.TextGuruMeditation.setup_autorun(version) #生成一个server对象 server = service.Service.create(binary='nova-conductor', topic=CONF.conductor.topic, manager=CONF.conductor.manager) workers = CONF.conductor.workers or processutils.get_worker_count() #启动service service.serve(server, workers=workers) service.wait() 看下server对象的创建过程service.Service.create： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_enable=None, periodic_fuzzy_delay=None, periodic_interval_max=None, db_allowed=True): """Instantiates class and passes back application object. :param host: defaults to CONF.host :param binary: defaults to basename of executable :param topic: defaults to bin_name - 'nova-' part :param manager: defaults to CONF.&lt;topic&gt;_manager :param report_interval: defaults to CONF.report_interval :param periodic_enable: defaults to CONF.periodic_enable :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay :param periodic_interval_max: if set, the max time to wait between runs """ # 下面的参数解释见上面的英文注释 if not host: host = CONF.host if not binary: binary = os.path.basename(sys.argv[0]) #topic就是rabbitmq中连接queue 的依据 if not topic: topic = binary.rpartition('nova-')[2] #真正执行代码的manager类 if not manager: manager_cls = ('%s_manager' % binary.rpartition('nova-')[2]) manager = CONF.get(manager_cls, None) if report_interval is None: report_interval = CONF.report_interval if periodic_enable is None: periodic_enable = CONF.periodic_enable if periodic_fuzzy_delay is None: periodic_fuzzy_delay = CONF.periodic_fuzzy_delay debugger.init() #传参，完成初始化 service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_enable=periodic_enable, periodic_fuzzy_delay=periodic_fuzzy_delay, periodic_interval_max=periodic_interval_max, db_allowed=db_allowed) return service_obj 再看下service.serve()完成的工作,该函数直接调用oslo-service的launch()方法: 12345678910111213def launch(conf, service, workers=1, restart_method='reload'): if workers is not None and workers &lt;= 0: raise ValueError(_("Number of workers should be positive!")) #只起一个进程来运行服务 if workers is None or workers == 1: launcher = ServiceLauncher(conf, restart_method=restart_method) else: #起woker个进程运行服务 launcher = ProcessLauncher(conf, restart_method=restart_method) launcher.launch_service(service, workers=workers) return launcher 进入launcher.launch_service,调用一个add方法，看下代码（此处为起一个进程的launcher为例）： 12345678def add(self, service): """Add a service to a list and create a thread to run it. :param service: service to run """ #将一个service加入service list,并起一个thread去执行 self.services.append(service) self.tg.add_thread(self.run_service, service, self.done) add_thread会起一个green thread，执行callback这个回调，在这里就是server.start(): 123456789101112131415161718def start(self): ....................................... target = messaging.Target(topic=self.topic, server=self.host) endpoints = [ self.manager, baserpc.BaseRPCAPI(self.manager.service_name, self.backdoor_port) ] endpoints.extend(self.manager.additional_endpoints) serializer = objects_base.NovaObjectSerializer() #获取rpcserver并监听对应端口 self.rpcserver = rpc.get_server(target, endpoints, serializer) self.rpcserver.start() self.manager.post_start_hook() ................................... 接下来看conductor-manage的处理过程。nova-conductor收到rpc请求，根据路由映射，该请求会交给对应manager的函数处理，nova/conductor/manager.py.ComputeTaskManager.build_instances 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def build_instances(self, context, instances, image, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping=None, legacy_bdm=True): if (requested_networks and not isinstance(requested_networks, objects.NetworkRequestList)): requested_networks = objects.NetworkRequestList.from_tuples( requested_networks) # flavor = filter_properties.get('instance_type') if flavor and not isinstance(flavor, objects.Flavor): # Code downstream may expect extra_specs to be populated since it # is receiving an object, so lookup the flavor to ensure this. flavor = objects.Flavor.get_by_id(context, flavor['id']) filter_properties = dict(filter_properties, instance_type=flavor) request_spec = &#123;&#125; try: # check retry policy. Rather ugly use of instances[0]... # but if we've exceeded max retries... then we really only # have a single instance. #生成发送给scheduler的信息 request_spec = scheduler_utils.build_request_spec( context, image, instances) scheduler_utils.populate_retry( filter_properties, instances[0].uuid) #发送同步消息给nova-scheduler，选取用于创建云主机的主机(留给下篇文章) hosts = self._schedule_instances( context, request_spec, filter_properties) except Exception as exc: updates = &#123;'vm_state': vm_states.ERROR, 'task_state': None&#125; #设置虚拟机state,taskstate for instance in instances: self._set_vm_state_and_notify( context, instance.uuid, 'build_instances', updates, exc, request_spec) try: # If the BuildRequest stays around then instance show/lists # will pull from it rather than the errored instance. self._destroy_build_request(context, instance) except exception.BuildRequestNotFound: pass self._cleanup_allocated_networks( context, instance, requested_networks) return ............................. #发送异步消息给nova-compute，完成instance的boot(下篇文章) self.compute_rpcapi.build_and_run_instance(context, instance=instance, host=host['host'], image=image, request_spec=request_spec, filter_properties=local_filter_props, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=bdms, node=host['nodename'], limits=host['limits']) 从上面的源码可以看出，build_instances方法主要实现过滤参数的组装，然后通过客户端scheduler_client发送rpc请求到scheduler完成host的选取，最后发送rpc请求到选取的host上，由nova-compute完成云主机的启动。 对nova boot在 Novaconductor的分析就完成了。简单从整体源码层面分析一下novaconductor,源码目录结构如下： 一般来说，rpcapi.py都是与rpc相关的，别的服务只需导入该模块便可以使用它提供的远程调用Novaconductor的服务，Novaconductor注册的RPCServer接收到rpc请求，然后由manager.py中的ConductorManager真正完成数据库的访问。由于数据库访问的特殊性，api.py又对rpc.api做了一层封装，所以其他模块需要导入的是api.py，api.py的封装主要是区分访问数据库是否需要通过RPC,如果数据库就在本地host,那么无需通过RPC调用。所以api.py中包含了四个类：LocalAPI,API,LocalComputerTaskAPI,ComputerTaskAPI。前两个类是Novaconductor访问数据库的接口，后两个是TaskaAPI接口，TaskaAPI接口主要包含耗时较长的任务，比如新建虚拟机，迁移虚拟机等。如果Novacompute与Novaconductor模块在同一台机器上，那么不用通过RPC,直接通过LocalAPI访问数据库，同理TaskAPI也不需要RPC,使用LocalTaskAPI。看下nova/conductor/init.py 1234567891011121314151617def API(*args, **kwargs): #根据配置选项use_local判断Novacompute与novaconductor是否在同一节点，决定使用哪一类API use_local = kwargs.pop('use_local', False) if CONF.conductor.use_local or use_local: api = conductor_api.LocalAPI else: api = conductor_api.API return api(*args, **kwargs)def ComputeTaskAPI(*args, **kwargs): #分析同上 use_local = kwargs.pop('use_local', False) if CONF.conductor.use_local or use_local: api = conductor_api.LocalComputeTaskAPI else: api = conductor_api.ComputeTaskAPI return api(*args, **kwargs) 接下来看下novaconductor与数据库交互的部分，在manager.py文件中有两个类，一个ConductorManager,一个ComputerTaskManager,对应两种API。与数据库交互的主要是ConductorManager。但当我打开这个类时瞬间懵圈了： 类里面不是我们期待的数据库操作方法，而是关于object的一些方法。这里要讲下object Model。 Object Model算是nova访问数据库的分水岭。之前，对某一个表的操作都是放在一个同名文件中,例如flavor.py,使用时直接调用文件中的函数操作数据库。Object Model引入后，新建flavor对象与flavor表对应，将对flavor 的操作封装在flavor对象中。这么做的原因大致是：1，nova-computer 与数据库升级时的版本问题。2，减少写入数据库的数据量。3，数据库的传值类型问题。看下图Object Model 工作流程： novaComputer 与novaConductor不在同一个节点时，虚线为引入ObjectModel之前NovaComputer访问数据库的流程。实现表示引入objectModel后的流程。可以看到，Novacomputer需要做数据库的操作时，将通过ObjectModel调用nova.conductor.rpcapi.ConductorAPI提供的RPC接口，novaConductor 接受到RPC请求后，通过本地ObjectModel完成数据库的更新。ObjectModel的代码位于nova/objects目录，里面的每一个类对应数据库中的一个表，如下instance.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 @obj_base.NovaObjectRegistry.registerclass Network(obj_base.NovaPersistentObject, obj_base.NovaObject, obj_base.NovaObjectDictCompat): # Version 1.0: Initial version # Version 1.1: Added in_use_on_host() # Version 1.2: Added mtu, dhcp_server, enable_dhcp, share_address VERSION = '1.2' #基类base.NovaObject 会记录变化的字段，更新数据库时只更新这些变化的字段。 #字典field是ComputerNode对象维护的信息，该字典的值不一定包含ComputerNode表内所有信息，每一个值的类型都是nova.object.fields模块定义的类型，若数据类型不匹配，就会抛出异常。 fields = &#123; 'id': fields.IntegerField(), 'label': fields.StringField(), 'injected': fields.BooleanField(), 'cidr': fields.IPV4NetworkField(nullable=True), 'cidr_v6': fields.IPV6NetworkField(nullable=True), 'multi_host': fields.BooleanField(), 'netmask': fields.IPV4AddressField(nullable=True), 'gateway': fields.IPV4AddressField(nullable=True), 'broadcast': fields.IPV4AddressField(nullable=True), 'netmask_v6': fields.IPV6AddressField(nullable=True), 'gateway_v6': fields.IPV6AddressField(nullable=True), 'bridge': fields.StringField(nullable=True), 'bridge_interface': fields.StringField(nullable=True), 'dns1': fields.IPAddressField(nullable=True), 'dns2': fields.IPAddressField(nullable=True), 'vlan': fields.IntegerField(nullable=True), 'vpn_public_address': fields.IPAddressField(nullable=True), 'vpn_public_port': fields.IntegerField(nullable=True), 'vpn_private_address': fields.IPAddressField(nullable=True), 'dhcp_start': fields.IPV4AddressField(nullable=True), 'rxtx_base': fields.IntegerField(nullable=True), 'project_id': fields.UUIDField(nullable=True), 'priority': fields.IntegerField(nullable=True), 'host': fields.StringField(nullable=True), 'uuid': fields.UUIDField(), 'mtu': fields.IntegerField(nullable=True), 'dhcp_server': fields.IPAddressField(nullable=True), 'enable_dhcp': fields.BooleanField(), 'share_address': fields.BooleanField(), &#125; ...................................... @obj_base.remotable_classmethod def get_by_id(cls, context, network_id, project_only='allow_none'): db_network = db.network_get(context, network_id, project_only=project_only) return cls._from_db_object(context, cls(), db_network) @obj_base.remotable_classmethod def get_by_uuid(cls, context, network_uuid): db_network = db.network_get_by_uuid(context, network_uuid) return cls._from_db_object(context, cls(), db_network) nova.object.base 中有定义两个重要的修饰函数：remotable_classmethod 和remotable,前者用于修饰类的方法，后者用于修饰实例的方法。如果novacomputer 与novaconductor不在同一节点，novacomputer 初始化时会将novaObject.indirection_api初始化为nova.conductor.rpcapi.ConductorAPI,此时调用remotable_classmethod修饰的函数时，会通过rpc交给Novaconductor处理，rpc请求中包含了该函数名，novaconductor接收到该rpc请求会调用相应的object model 函数完成数据库操作。 参考文章nova wiki Openstack liberty源码分析 之 云主机的启动过程1 openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--novaApi 源代码解读]]></title>
      <url>%2F2016%2F12%2F01%2FnovaApi-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：novaApi 介绍 以nova list 为例解读代码 novaApi 介绍 关于nova不再赘述，之前文章由介绍。 之前说的novaclient的主要作用就是将命令行最终转化为restful请求去novaApi查找。 novaApi的执行过程主要分为两个阶段：接收到restful请求后，根据PasteDeploy 将请求的路由到具体的WSGI Application ,之后Routers将请求路由到具体函数并执行。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova list 为例解读代码 首先看下novaapi的启动过程，进入nova项目的setup.cfg文件 123456789101112131415161718192021222324252627282930313233343536console_scripts = nova-all = nova.cmd.all:main # nova 所有service的启动脚本，已弃用 nova-api = nova.cmd.api:main #此处为Nova-api的启动脚本 nova-api-metadata = nova.cmd.api_metadata:main nova-api-os-compute = nova.cmd.api_os_compute:main nova-cells = nova.cmd.cells:main nova-cert = nova.cmd.cert:main nova-compute = nova.cmd.compute:main nova-conductor = nova.cmd.conductor:main nova-console = nova.cmd.console:main nova-consoleauth = nova.cmd.consoleauth:main nova-dhcpbridge = nova.cmd.dhcpbridge:main nova-idmapshift = nova.cmd.idmapshift:main nova-manage = nova.cmd.manage:main nova-network = nova.cmd.network:main nova-novncproxy = nova.cmd.novncproxy:main nova-policy = nova.cmd.policy_check:main nova-rootwrap = oslo_rootwrap.cmd:main nova-rootwrap-daemon = oslo_rootwrap.cmd:daemon nova-scheduler = nova.cmd.scheduler:main nova-serialproxy = nova.cmd.serialproxy:main nova-spicehtml5proxy = nova.cmd.spicehtml5proxy:main nova-xvpvncproxy = nova.cmd.xvpvncproxy:mainwsgi_scripts = nova-placement-api = nova.api.openstack.placement.wsgi:init_applicationnova.api.v21.extensions = admin_actions = nova.api.openstack.compute.admin_actions:AdminActions admin_password = nova.api.openstack.compute.admin_password:AdminPassword agents = nova.api.openstack.compute.agents:Agents aggregates = nova.api.openstack.compute.aggregates:Aggregates assisted_volume_snapshots = nova.api.openstack.compute.assisted_volume_snapshots:AssistedVolumeSnapshots attach_interfaces = nova.api.openstack.compute.attach_interfaces:AttachInterfaces availability_zone = nova.api.openstack.compute.availability_zone:AvailabilityZone baremetal_nodes = nova.api.openstack.compute.baremetal_nodes:BareMetalNodes block_device_mapping = nova.api.openstack.compute.block_device_mapping:BlockDeviceMapping 简单解释下console_scripts中值得注意的几个服务： nova-api:包括两种API服务：nova-api-metadata ,nova-api-os-compute。根据配置文件中的enable_apis确定启动哪种服务。 nova-api-metadata:接受虚拟机的metadata相关的请求。只有采用nova-network部署时才使用。该工作目前已由neutron项目完成。 nova-api-os-compute:主要的Openstack Compute API服务。 nova-cells:主要适用于openstack增强横向扩展能力，check this nova-rootwrap:在openstack运行时以root身份运行某些shell命令 setup.cfg文件中nova.api.v21.extensions的字段中的每一项对应一个API，novaapi 启动时会用stevedore据此动态加载。 首先进入main函数 1234567891011121314151617181920212223242526def main(): config.parse_args(sys.argv) logging.setup(CONF, "nova") utils.monkey_patch() objects.register_all() if 'osapi_compute' in CONF.enabled_apis: # NOTE(mriedem): This is needed for caching the nova-compute service # version which is looked up when a server create request is made with # network id of 'auto' or 'none'. objects.Service.enable_min_version_cache() log = logging.getLogger(__name__) gmr.TextGuruMeditation.setup_autorun(version) # 生成Guru report的相关代码 launcher = service.process_launcher() started = 0 for api in CONF.enabled_apis: #根据配置文件中的配置创建server should_use_ssl = api in CONF.enabled_ssl_apis try: server = service.WSGIService(api, use_ssl=should_use_ssl) #创建WSGI server ,创建过程中paste deploy参与进来，加载相应application launcher.launch_service(server, workers=server.workers or 1) #根据配置文件中worker个数启动server started += 1 except exception.PasteAppNotFound as ex: log.warning( _LW("%s. ``enabled_apis`` includes bad values. " "Fix to remove this warning."), ex) ............................ 看下service.WSGIService()创建时发生了什么： 12345678910111213141516171819202122232425262728293031323334353637383940class WSGIService(service.Service): """Provides ability to launch API from a 'paste' configuration.""" def __init__(self, name, loader=None, use_ssl=False, max_url_len=None): """Initialize, but do not start the WSGI server. :param name: The name of the WSGI server given to the loader. :param loader: Loads the WSGI application using the given name. :returns: None """ self.name = name # NOTE(danms): Name can be metadata, os_compute, or ec2, per # nova.service's enabled_apis self.binary = 'nova-%s' % name self.topic = None self.manager = self._get_manager() self.loader = loader or wsgi.Loader() #从paste配置文件加载api对应的wsgi application self.app = self.loader.load_app(name) # inherit all compute_api worker counts from osapi_compute if name.startswith('openstack_compute_api'): wname = 'osapi_compute' else: wname = name self.host = getattr(CONF, '%s_listen' % name, "0.0.0.0") self.port = getattr(CONF, '%s_listen_port' % name, 0) self.workers = (getattr(CONF, '%s_workers' % wname, None) or processutils.get_worker_count()) ............................ # 指定IP，port监听socket，server的实现再用了eventlet进行封装，监听到http请求时，不会新建一个线程，而是用协程。 self.server = wsgi.Server(name, self.app, host=self.host, port=self.port, use_ssl=self.use_ssl, max_url_len=max_url_len) # Pull back actual port used self.port = self.server.port self.backdoor_port = None ................................ 当路径为/v21/servers/detail请求过来时，WSGI server监听到请求，根据paste配置文件路由到特定的WSGI application。 12345678[composite:openstack_compute_api_v21] use = call:nova.api.auth:pipeline_factory_v21 #调用pipeline_factory_v21函数，根据nova配置文件中的auth_strategy决定使用noauth2或keystonenoauth2 = compute_req_id faultwrap sizelimit noauth2 osapi_compute_app_v21keystone = compute_req_id faultwrap sizelimit authtoken keystonecontext osapi_compute_app_v21 ...................... [app:osapi_compute_app_v21]paste.app_factory = nova.api.openstack.compute:APIRouterV21.factory .................... 据此，我们便找到最终调用的函数APIRouterV21.factory，第一阶段完成，进入第二阶段 第二阶段主要是找到最终调用的函数，nova封装了route模块实现该路由功能。进入上次找到的路由函数APIRouterV21.factory。 1234567891011121314151617181920212223242526272829303132333435363738394041class APIRouterV21(base_wsgi.Router): """Routes requests on the OpenStack v2.1 API to the appropriate controller and method. """ @classmethod def factory(cls, global_config, **local_config): """Simple paste factory, :class:`nova.wsgi.Router` doesn't have one.""" return cls() @staticmethod def api_extension_namespace(): return 'nova.api.v21.extensions' #该类主要实现利用stevedore加载位于setup.cfg中nova.api.v21.extensions下所有资源，并利用check_func()函数进行检查 def __init__(self, init_only=None): def _check_load_extension(ext): return self._register_extension(ext) self.api_extension_manager = stevedore.enabled.EnabledExtensionManager( namespace=self.api_extension_namespace(), check_func=_check_load_extension, invoke_on_load=True, invoke_kwds=&#123;"extension_info": self.loaded_extension_info&#125;) mapper = ProjectMapper() self.resources = &#123;&#125; # NOTE(cyeoh) Core API support is rewritten as extensions # but conceptually still have core #这部分做的工作就是对所有封装的资源进行注册（对资源的属性进行一些扩展），并使用mapper对象建立路由规则（与***controllers类建立）。 if list(self.api_extension_manager): # NOTE(cyeoh): Stevedore raises an exception if there are # no plugins detected. I wonder if this is a bug. self._register_resources_check_inherits(mapper) self.api_extension_manager.map(self._register_controllers) LOG.info(_LI("Loaded extensions: %s"), sorted(self.loaded_extension_info.get_extensions().keys())) super(APIRouterV21, self).__init__(mapper) #打印出该mapper对象便可找到路由关系 ..................... nova中每种资源都被封装成一个nova.api.openstack.wsgi.Resource资源，并封装为一个WSGI application。 最后APIRouterV21将mapper传给父类Router做最后的工作. 123456789101112131415161718192021222324252627282930313233343536373839class Router(object): """WSGI middleware that maps incoming requests to WSGI apps.""" #使用routes模块将mapper与_dispatch()关联起来 def __init__(self, mapper): """Create a router for the given routes.Mapper. Each route in `mapper` must specify a 'controller', which is a WSGI app to call. You'll probably want to specify an 'action' as well and have your controller be an object that can route the request to the action-specific method. """ self.map = mapper self._router = routes.middleware.RoutesMiddleware(self._dispatch, self.map) #根据mapper将请求路由到适当的WSGI应用，即资源上。 @webob.dec.wsgify(RequestClass=Request) def __call__(self, req): """Route the incoming request to a controller based on self.map. If no match, return a 404. """ return self._router @staticmethod @webob.dec.wsgify(RequestClass=Request) def _dispatch(req): """Dispatch the request to the appropriate controller. Called by self._router after matching the incoming request to a route and putting the information into req.environ. Either returns 404 or the routed WSGI app's response. """ match = req.environ['wsgiorg.routing_args'][1] if not match: return webob.exc.HTTPNotFound() app = match['controller'] return app 至此，所有阶段便完成了，GET /v21/servers/detail最终会调用的函数是setup.cfg中的server对应的controller的detail函数。 123456789101112131415161718192021222324252627282930313233343536373839404142@extensions.expected_errors((400, 403)) def detail(self, req): """Returns a list of server details for a given user.""" context = req.environ['nova.context'] context.can(server_policies.SERVERS % 'detail') try: servers = self._get_servers(req, is_detail=True) #调用下面的_get_servers函数 except exception.Invalid as err: raise exc.HTTPBadRequest(explanation=err.format_message()) return servers ..............def _get_servers(self, req, is_detail): """Returns a list of servers, based on any search options specified.""" search_opts = &#123;&#125; search_opts.update(req.GET) context = req.environ['nova.context'] remove_invalid_options(context, search_opts, self._get_server_search_options(req)) # Verify search by 'status' contains a valid status. # Convert it to filter by vm_state or task_state for compute_api. # For non-admin user, vm_state and task_state are filtered through # remove_invalid_options function, based on value of status field. # Set value to vm_state and task_state to make search simple. search_opts.pop('status', None) ............ #最终调用函数在这里compute_api.get_all() try: instance_list = self.compute_api.get_all(elevated or context, search_opts=search_opts, limit=limit, marker=marker, expected_attrs=expected_attrs, sort_keys=sort_keys, sort_dirs=sort_dirs) except exception.MarkerNotFound: msg = _('marker [%s] not found') % marker raise exc.HTTPBadRequest(explanation=msg) except exception.FlavorNotFound: LOG.debug("Flavor '%s' could not be found ", search_opts['flavor']) instance_list = objects.InstanceList() 参考文章openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--novaClient 源代码解读]]></title>
      <url>%2F2016%2F11%2F30%2Fnovaclient-sourcecode-explain%2F</url>
      <content type="text"><![CDATA[目录结构：novaclient 介绍 以nova list 为例解读代码 novaclient 介绍 novaclient是Python写的nova API服务的客户端. 提供了两种使用形式：命令行形式和Python API形式（命令行形式其实最终也是调用的Python API）。 novaclient 本身不是类似于nova-api这种的service，没有一个守护进程在运行，而是在我们输入命令的之后根据入口main函数去相应的执行。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova list 为例解读代码 进入novaclient的根目录，找到setup.cfg文件，找到入口函数 其实我们也可以在安装了novaclient的机器中执行 which nova 找到对应入口。 进入novaclient.shell 的main函数，截取部分代码如下： 123456789101112131415161718192021def main(): try: argv = [encodeutils.safe_decode(a) for a in sys.argv[1:]] OpenStackComputeShell().main(argv) #重点在这个函数 except Exception as exc: logger.debug(exc, exc_info=1) if six.PY2: message = encodeutils.safe_encode(six.text_type(exc)) else: message = encodeutils.exception_to_unicode(exc) print("ERROR (%(type)s): %(msg)s" % &#123; 'type': exc.__class__.__name__, 'msg': message&#125;, file=sys.stderr) sys.exit(1) except KeyboardInterrupt: print(_("... terminating nova client"), file=sys.stderr) sys.exit(130)if __name__ == "__main__": main() 找到OpenStackComputeShell().main函数, 123456789101112131415161718192021def main(self, argv): # Parse args once to find version and debug settings parser = self.get_base_parser(argv) (args, args_list) = parser.parse_known_args(argv) self.setup_debugging(args.debug) self.extensions = [] do_help = ('help' in argv) or ( '--help' in argv) or ('-h' in argv) or not argv # bash-completion should not require authentication skip_auth = do_help or ( 'bash-completion' in argv) if not args.os_compute_api_version: api_version = api_versions.get_api_version( DEFAULT_MAJOR_OS_COMPUTE_API_VERSION) else: api_version = api_versions.get_api_version( args.os_compute_api_version) ......................... 这个函数很长，大体做的工作如下： 1,解析命令行，判定是否需要做认证操作（如help命令不需要） 2,需要做认证操作的话，判定认证方法（一般为keystone） 1234567891011if use_session: # Not using Nova auth plugin, so use keystone with utils.record_time(self.times, args.timings, 'auth_url', args.os_auth_url): keystone_session = ( loading.load_session_from_argparse_arguments(args)) keystone_auth = ( loading.load_auth_from_argparse_arguments(args)) else: # set password for auth plugins os_password = args.os_password 3,创建一个Client用于确定API版本 1234567891011121314151617181920212223242526272829# This client is just used to discover api version. Version API needn't# microversion, so we just pass version 2 at here.self.cs = client.Client( api_versions.APIVersion("2.0"), os_username, os_password, os_project_name, tenant_id=os_project_id, user_id=os_user_id, auth_url=os_auth_url, insecure=insecure, region_name=os_region_name, endpoint_type=endpoint_type, extensions=self.extensions, service_type=service_type, service_name=service_name, auth_token=auth_token, volume_service_name=volume_service_name, timings=args.timings, bypass_url=bypass_url, os_cache=os_cache, http_log_debug=args.debug, cacert=cacert, timeout=timeout, session=keystone_session, auth=keystone_auth, logger=self.client_logger)if not skip_auth: if not api_version.is_latest(): if api_version &gt; api_versions.APIVersion("2.0"): if not api_version.matches(novaclient.API_MIN_VERSION, novaclient.API_MAX_VERSION): raise exc.CommandError( _("The specified version isn't supported by " "client. The valid version range is '%(min)s' " "to '%(max)s'") % &#123; "min": novaclient.API_MIN_VERSION.get_string(), "max": novaclient.API_MAX_VERSION.get_string()&#125; ) api_version = api_versions.discover_version(self.cs, api_version) 4,根据发现的API版本创建一个client 1234567891011121314# Recreate client object with discovered version.self.cs = client.Client( api_version, os_username, os_password, os_project_name, tenant_id=os_project_id, user_id=os_user_id, auth_url=os_auth_url, insecure=insecure, region_name=os_region_name, endpoint_type=endpoint_type, extensions=self.extensions, service_type=service_type, service_name=service_name, auth_token=auth_token, volume_service_name=volume_service_name, timings=args.timings, bypass_url=bypass_url, os_cache=os_cache, http_log_debug=args.debug, cacert=cacert, timeout=timeout, session=keystone_session, auth=keystone_auth) 5,根据args调用指定版本API 1args.func(self.cs, args) 6,看下最终调用的V2版本下的shell.py(此处为do_list()函数) 12345678910111213141516171819202122 @utils.arg( '--not-tags-any', dest='not-tags-any', metavar='&lt;not-tags-any&gt;', default=None, help=_("Only the servers that do not have at least one of the given tags" "will be included in the list result. Boolean expression in this " "case is 'NOT(t1 OR t2)'. Tags must be separated by commas: " "--not-tags-any &lt;tag1,tag2&gt;"), start_version="2.26")def do_list(cs, args): """List active servers.""" imageid = None flavorid = None if args.image: imageid = _find_image(cs, args.image).id if args.flavor: flavorid = _find_flavor(cs, args.flavor).id # search by tenant or user only works with all_tenants if args.tenant or args.user: args.all_tenants = 1 ....................... 比较重要的语句如下： 1234567servers = cs.servers.list(detailed=detailed, search_opts=search_opts, sort_keys=sort_keys, sort_dirs=sort_dirs, marker=args.marker, limit=args.limit) ....................... 7,调用v2 下client中的servers 中的list方法， 123456789101112password = kwargs.pop('password', api_key)self.projectid = project_idself.tenant_id = tenant_idself.user_id = user_idself.flavors = flavors.FlavorManager(self)self.flavor_access = flavor_access.FlavorAccessManager(self)self.images = images.ImageManager(self)self.glance = images.GlanceManager(self)self.limits = limits.LimitsManager(self)self.servers = servers.ServerManager(self) # 调用此处的serversself.versions = versions.VersionManager(self)....................... 8,终于到最后一步 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def list(self, detailed=True, search_opts=None, marker=None, limit=None, sort_keys=None, sort_dirs=None): if search_opts is None: search_opts = &#123;&#125; qparams = &#123;&#125; for opt, val in six.iteritems(search_opts): if val: if isinstance(val, six.text_type): val = val.encode('utf-8') qparams[opt] = val detail = "" if detailed: detail = "/detail" result = base.ListWithMeta([], None) while True: if marker: qparams['marker'] = marker if limit and limit != -1: qparams['limit'] = limit # Transform the dict to a sequence of two-element tuples in fixed # order, then the encoded string will be consistent in Python 2&amp;3. if qparams or sort_keys or sort_dirs: # sort keys and directions are unique since the same parameter # key is repeated for each associated value # (ie, &amp;sort_key=key1&amp;sort_key=key2&amp;sort_key=key3) items = list(qparams.items()) if sort_keys: items.extend(('sort_key', sort_key) for sort_key in sort_keys) if sort_dirs: items.extend(('sort_dir', sort_dir) for sort_dir in sort_dirs) new_qparams = sorted(items, key=lambda x: x[0]) query_string = "?%s" % parse.urlencode(new_qparams) else: query_string = "" servers = self._list("/servers%s%s" % (detail, query_string), "servers") #重点在这 result.extend(servers) result.append_request_ids(servers.request_ids) if not servers or limit != -1: break marker = result[-1].id return result 找到最终调用的_list方法如下： 1234567def _list(self, url, response_key, obj_class=None, body=None): if body: resp, body = self.api.client.post(url, body=body) else: resp, body = self.api.client.get(url) ....................... 即最终通过restful api 去查找对应的数据,此处路径为（/servers/detail ）self.api.client是novaclient.client.SessionClient类的对象，但是在SessionClient类中没有找到get函数，在其父类adapter.LegacyJsonAdapter中发现该函数，而adapter文件是在keystoneclient中，所以self.api.client.get(url)调用的是keystoneclient/adapter.py中的LegacyJsonAdapter类的get()函数,最终找到Adapter类中的get()函数。 12def get(self, url, **kwargs): return self.request(url, 'GET', **kwargs) 之后会调用sessionClient的request()函数。 1234567891011121314151617181920212223def request(self, url, method, **kwargs): kwargs.setdefault('headers', kwargs.get('headers', &#123;&#125;)) api_versions.update_headers(kwargs["headers"], self.api_version) # NOTE(jamielennox): The standard call raises errors from # keystoneauth1, where we need to raise the novaclient errors. raise_exc = kwargs.pop('raise_exc', True) with utils.record_time(self.times, self.timings, method, url): resp, body = super(SessionClient, self).request(url, method, raise_exc=False, **kwargs) # 该函数 # if service name is None then use service_type for logging service = self.service_name or self.service_type _log_request_id(self.logger, resp, service) # TODO(andreykurilin): uncomment this line, when we will be able to # check only nova-related calls # api_versions.check_headers(resp, self.api_version) if raise_exc and resp.status_code &gt;= 400: raise exceptions.from_response(resp, body, url, method) return resp, body 调用adapter.LegacyJsonAdapter类中的request方法 123456789101112131415161718192021222324252627282930class LegacyJsonAdapter(Adapter): """Make something that looks like an old HTTPClient. A common case when using an adapter is that we want an interface similar to the HTTPClients of old which returned the body as JSON as well. You probably don't want this if you are starting from scratch. """ def request(self, *args, **kwargs): headers = kwargs.setdefault('headers', &#123;&#125;) headers.setdefault('Accept', 'application/json') try: kwargs['json'] = kwargs.pop('body') except KeyError: # nosec(cjschaef): kwargs doesn't contain a 'body' # key, while 'json' is an optional argument for Session.request pass resp = super(LegacyJsonAdapter, self).request(*args, **kwargs) body = None if resp.text: try: body = jsonutils.loads(resp.text) except ValueError: # nosec(cjschaef): return None for body as # expected pass return resp, body 不往下看了，再往下代码比较复杂。 参考文章openstack-L版源码解析之novaclientpythonnovaclient END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux系列--关于 linux 日志]]></title>
      <url>%2F2016%2F11%2F20%2Flinux-log%2F</url>
      <content type="text"><![CDATA[目录结构：Linux日志概述 常见日志文件 常用命令 Linux日志概述 Linux 系统的日志主要分为两种类型：一种是进程所属日志，另一种是syslog的信息，即系统syslog记录的日志。 Linux 日志系统可以划分为三个子系统：1，连接时间用户登录的日志，一般写入到/var/log/wtmp,/var/log/utmp两个文件中。2，进程统计日志。3错误日志，通过syslogd写入文件/var/log/message。 关于syslogd:Linux 内核由很多子系统组成，包括网络、文件访问、内存管理等。子系统需要给用户传送一些消息，这些消息内容包括消息的来源及其重要性等。所有的子系统都要把消息送到一个可以维护的公用消息区，于是就有了一个叫 syslog 的程序。它的配置文件通常位于/etc/log/syslog.conf。 关于logrotate:logrotate程序用来帮助用户管理日志文件，它以自己的守护进程工作。logrotate 周期性地旋转日志文件，可以周期性地把每个日志文件重命名成一个备份名字，然后让它的守护进程开始使用一个日志文件的新的拷贝。配置文件通常位于/etc/logrotate.conf。 常见日志文件直接引自云服务器 ECS Linux 系统中常见的日志文件介绍 /var/log/cron可以在 cron 文件中检查 crontab 任务调度是否实际执行，执行过程是否发生错误，以及 /etc/crontab 文件是否有语法错误或编写错误。 /var/log/dmesg可以在 dmesg 文件中检查系统在开机时候内核检测过程所产生的各项信息，包括系统的设备信息，以及在启动和操作过程中系统记录的任何错误和问题的信息。通过 dmesg 文件可以判断某些硬件设备（比如磁盘）在系统启动过程中是否被正确识别。 /var/log/lastlog可以在 lastlog 文件中检查系统上面所有账号的最后一次登录系统时的相关信息。 /var/log/messages可以在 messages 文件中检查到绝大多数系统发生的错误信息，如果系统发生一些未知的错误，建议客户首先检查一下 messages 文件。可以通过 tail -f /var/log/messages 实时查看系统内的变化情况。iptables 的默认日志就是 /var/log/messages。 /var/log/secure所有涉及需要输入账号密码的软件或程序，在登陆时（无论登录成功或失败）的信息都会被记录到 secure 文件，比如系统的 login 程序；su、sudo 命令；ssh、telnet、pop3、ftp 等程序的登录信息。 /var/log/wtmp可以在 wtmp 文件中检查到正确登陆系统的账户信息。由于 wtmp 文件已经被编码过的，所以需要使用 last 指令来取出文件的内容，用 cat 等命令无法直接查看此文件。 常用命令 w查看当前系统登录的用户以及用户在干什么 history当前登录用户的执行命令历史 last查看当前用户登录历史 who /var/log/wtmp查看用户登录系统历史 参考文章云服务器 ECS Linux 系统中常见的日志文件介绍 Linux系统日志简介 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--设置nfs作为cinder backend]]></title>
      <url>%2F2016%2F11%2F10%2Fmake-nfs-as-cinder-backend%2F</url>
      <content type="text"><![CDATA[目录结构：nfs 设置 修改cinder 配置文件 nfs 设置首先对nfs进行设置，大体以下几个步骤： 磁盘分区，格式化磁盘在格式化磁盘的时候出现如下错误：搜了很长时间没找到答案，看提示应该是系统还有什么进程在使用这块磁盘，后请教别人后发现该磁盘之前做过ceph的osd盘，虽然已经删除了对应的关联，但是并没有删除完全，必须将/var/lib/ceph/osd/下对应的ceph包删除。然后，格式化成功。 新建共享目录，mount 磁盘 nfs安装 先查看有没有安装 配置nfs共享文件路径并启动nfsserver. 修改cinder 配置文件 首先修改cinder配置文件cinder.conf，指定nfsshare文件路径(图中注释部分为之前的ceph配置) 添加nfsshare 文件，只需要指出nfs暴露的接口即可。 重启cinder-volume 发现cinder volume报错，发现是cinder对挂载的nfs没有权限，修改nfs挂载文件的user,group 为cinder，或者直接chmod 777 (不安全) 重启cinder-volume，成功。 查看mount验证下。 参考文章磁盘分区，格式化与检验 CentOS 6.5下NFS安装配置Configure an NFS storage back end END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[devops系列-- 运维利器SaltStalk介绍]]></title>
      <url>%2F2016%2F11%2F03%2F2016-11-3-intro-to-salt%2F</url>
      <content type="text"><![CDATA[SaltStalk是什么salt官网给出的介绍简洁明了： 一个配置管理系统，能够维护预定义状态的远程节点(比如，确保指定的报被安装，指定的服务在运行) 一个分布式远程执行系统，用来在远程节点（可以是单个节点，也可以是任意规则挑选出来的节点）上执行命令和查询数据。开发其的目的是为远程执行提供最好的解决方案，并使远程执行变得更好，更快，更简单。 其实，除了saltstack,还有其他的同类型软件供我们选择，比如puppet、chef、ansible、fabric等,关于其中的区别以及优劣势可以参考这里，Ansible vs. Chef vs. Fabric vs. Puppet vs. SaltStack稍微总结下：chef,puppet 是出现时间比较早的系统，对于比较重视成熟性和稳定性的公司来说比较合适。ansible与saltstack比较适合快速灵活的生产部署环境，且适合没有太多额外要求，生产环境操作系统一致的情况下。fabric比较适合规模比较小的环境，属于入门级别的配置管理系统。 这里有一个视频更细粒度的（可用性，互操作性，扩展性等）比较了一下，Chef vs Puppet vs Ansible vs SaltStack | Configuration Management Tools Comparison SaltStalk 安装安装比较简单，check this,不再赘述。 SaltStalk 配置saltstalk是典型的CS架构，主要分两种角色，master和minions,master负责minions的配置管理以及远程执行。一般来说，master的配置文件位于/etc/salt/master，minion的配置文件在相应机器的/etc/salt/minion,只需在minion配置文件中指定master指向即可运行。 master的配置项非常多，大致包括以下几项： 主要配置：包括网络接口，提供服务的端口，master id,最大打开文件数，工作线程个数，返回端口，缓存目录，各种缓存配置（minions数据缓存，作业缓存等） master 安全相关配置：主要是针对PKI验证的一些配置。 master模块管理相关配置 master state system 相关配置 pillar 相关配置 日志相关配置 windows软件源相关配置 minion的配置项跟master大致类似，不再赘述。 其他： 新版saltstack 中minion有一个minion_blackout的配置，该选项设为true后，该minion不会执行除saltutil.refresh_pillar之外的其他所有命令。 saltstack有一个访问控制系统对于非admin用户进行细粒度的访问控制。check this job管理器：可以实现对minion job的发送信号，定时调度等 job 返回数据管理：系统默认返回数据存储在Job cache（默认是在/var/cache/salt/master/jobs目录）中，还有其他两种存储模式：一种是External Job Cache，如下图 另一种是Master Job Cache，如下图： returner: 我们可以在master端执行远程指令时指定returner,这样minion返回的数据不仅会返回到master，也会返回到我们指定的returner。而且我们也可以按照要求实现一个returner，来替换我们的Default Job Cache。 SaltStalk 一些概念解释sls 文件salt state 文件的简称，saltstalk实现配置管理的核心部分，描述了系统的目标状态，一般遵循yaml格式，参考YAML 语言教程。 首先是配置管理的入口文件top.sls，默认在/srv/salt/目录下。 1234567891011base: # 默认环境变量 '*': # 通过正则进行匹配minion - apache # 需要自己写的state.sls模块名 my_group: #通过分组名去进行匹配 须定义match:nodegroup - match: nodegroup - nginx 'os:CentOs': #通过grains模块去匹配，须定义match:grain - match: grain - nginx 再看下需要自己写的webserver.sls文件，默认位于/srv/salt/apache.sls。当然，还有其他组织形式，常用的还有/srv/salt/apache/init.sls 形式。 1234567891011apache: # 标签定义 pkg: # 状态定义，这里使用（pkg state module） - installed # 安装nginx（yum安装） service.running: # 保持服务是启动状态 - enable: True - reload: True - require: - file: /etc/init.d/httpd - watch: #检测下面配置文件，有变动，立马执行上述/etc/init.d/httpd 命令reload操作 - file: /etc/apache/httpd.conf - pkg: nginx state 之间的逻辑关系一般有三种： require :依赖某个state，在运行此state前，先运行依赖的state，依赖可以有多个 watch :在某个state变化时运行此模块，watch除具备require功能外，还增了关注状态的功能. order：优先级比require和watch低，有order指定的state比没有order指定的优先级高 一般对某个minion执行具体state的时候，我们可以执行1$ salt minion-1 state.sls apache 当然，如果是执行所有的sts文件，则是如下命令： 1$ salt minion-1 state.highstate # test=True 参数可以测试安装，并不真正安装 grain vs pillar vs mine如果说sls文件是配置管理的骨架或框架的话，那么grain与pillar就是填充骨架的血与肉。其实他们就是saltstack自己定义的一些数据，通过这些数据定制我们的系统配置。 grains 存储在minion一端，包括minion自己生成的一些信息，比如操作系统，内存，磁盘信息，cpu架构等等。当然我们也可以自己定制minion 的grains信息。通常做法是在默认/etc/salt/grains中定义。 pillar 是存储 在master端，完全是用户自定义的一些动态数据。一般存储在master端/srv/pillar 目录下。组织形式类似salt state。 两者区别：一般来说，变化较少或不变的数据存储在grains中，一些敏感数据（如各种密码），易变数据，以及涉及到minion 配置的数据一般存储在pillar中。 mine 更像是以上两种形式的结合，它是定期从minions 收集的数据，传给master并存储在master端，这样，所有的minions都能获取到。mine数据是有时效性的，每隔一段时间便会更新。mine可以存储在mine配置文件中，不过更多的是将它放在pillar中，只需定义mine_functions 关键字即可。官网示例参考EXAMPLE 两者的常用操作如下： 12345678910111213$ salt '*' grains.items # 查看所有 grains键值对$ salt '*' grains.get os # 查看 grains中的 os对应的值$ salt '*' saltutil.sync_grains # 同步所有 grains数据$ salt '*' pillar.items # 查看所有 pillar键值对$ salt '*' pillar.get data # 查看 pillar中的 data对应的值$ salt '*' saltutil.refresh_pillar # 刷新所有 pilllar数据$ salt '*' state.apply my_sls_file pillar='&#123;"hello": "world"&#125;' # 命令行更改或增加pillar execution modules也就是运程执行时需要 minions执行的函数，通常salt已经封装好了大量的modules,可以通过 salt ‘*’ sys.doc 查看所有modules的doc。当然也可以自己写modules，下文会给出示例。 runnerrunner 本质是可定制化的Python 脚本，与execution modules 类似，不过是运行在master节点，可以使用salt-run 命令使用，非常方便（最新版的salt-run也可以运行modules 了，命令：salt-run salt.cmd test.ping ,）。直接举个栗子：首先指定runner脚本的位置，在master配置文件中指定：runner_dirs: [/srv/runner/]然后就可以在指定目录下写Python 脚本了。 minions.py ：1234567891011# Import salt modulesimport salt.clientdef up(): ''' Print a list of all of the minions that are up ''' client = salt.client.LocalClient(__opts__['conf_file']) minions = client.cmd('*', 'test.ping', timeout=1) for minion in sorted(minions): print minion 使用命令行： salt-run minions.up 即可调用。注：有两种模式，默认为同步模式，即有数据返回后才显示，还有一种异步模式，即立即返回不显示数据，可以指定returner将数据返回到指定容器中。 salt enginessalt engines 是一个利用salt 并独立长时间运行的进程。有以下特点： 可以获取到salt的各种配置项，modules以及runner 以独立进程运行，由salt监视，一旦挂掉后由salt负责重启。 可以运行在master和minion上。 SaltStalk 使用远程执行saltstack 最基本的功能之一，在master节点实现对minions节点的远程控制与执行，命令格式如下： 1$ salt '&lt;target&gt;' &lt;function&gt; [arguments] target 指定 minions,可以有多种方式匹配minions，正则匹配，group匹配，grains匹配，pillar匹配等。 function 就是前面讲的execution modules ，后面可以加参数。 下面介绍下execution modules 的定制开发。首先确定一下modules的位置，默认位于/srv/salt/_modules目录下，可以通过以下命令实现modules的同步。12345$ salt '*' saltutil.sync_modules$ salt '*' saltutil.sync_all$ salt '*' state.apply 通常modules的脚本会使用zip压缩文件，官网示例 脚本中可以调用salt本身的module以及调用grains数据。同时注意str 类型 与unicode的转换。 配置管理配置管理功能是在远程执行的基础上建立起来的，Salt 状态系统的核心是就是上面提到过的SLS文件。SLS表示系统将会是什么样的一种状态（比如安装什么服务，服务是否启动，服务配置等等），而且是以一种很简单的格式来包含这些数据。下面是一个写SLS文件的示例： 首先是在master配置文件中定义三个环境： 12345678910file_roots: base: - /srv/salt/prod qa: - /srv/salt/qa - /srv/salt/prod dev: - /srv/salt/dev - /srv/salt/qa - /srv/salt/prod 简单解释下：这里定义了三个环境，base环境就是生产环境，它只有一个salt根目录，qa环境可以获取到两个salt根目录下的配置文件，按照上下顺序优先采用 /srv/salt/qa 目录下的配置文件，dev环境同理。通常我们开发时，会首先在开发环境部署，新的SLS文件会存储在 /srv/salt/dev 环境中，然后将改变push到对应的开发机器。开发完成后，将新的SLS文件 复制到/srv/salt/qa 目录，push到对应的测试机器以供测试。最后测试完成后，才会把SLS文件复制到/srv/salt/prod 目录，push到生产环境的机器。 接下来开始编写SLS文件了，首先编写各环境根目录下的top文件,因为 /srv/salt/prod目录下的文件是所有环境都能获取到的,所以只需在该目录下编写top文件即可。 123456789101112base: 'roles:prod': - match: grain - apacheqa: 'roles:qa': - match: grain - apachedev: 'roles:dev': - match: grain - apache 这里我们是通过在 grain 中添加roles 键值对，然后通过grain进行匹配，当然也可以通过其他方法匹配（pillar,group，正则等）。apache是之后我们需要编写SLS文件，不过这在这之前，先在各minions 添加grain roles键值对。 在对应minions 中添加grain roles键值对，默认编辑/etc/salt/grains ,添加 roles: prod 即可。添加完后，master 执行 salt ‘*’ saltutil.sync_grains 同步命令。 接下来开始编写apache的sls 文件。 /srv/salt/prod/apache/init.sls12345678910111213apache: pkg.installed: - name: httpd service.running: - name: httpd - require: - pkg: apache/var/www/html/index.html: file.managed: - source: salt://apache/index.html - require: - pkg: apache index.html 是我们的一个测试网页，放在/srv/salt/prod/apache/目录下。 接下来在 master端执行 salt ‘*’ state.apply 命令 即可完成对应minions 端apache的安装，启动等工作。浏览器输入minion ip,可以查看到测试网页。 如果该过程中出现错误，多半是SLS文件写错了，可以查看 minion日志，如果日志不详细，可以先关掉salt-minion，然后运行 salt-minion -l debug ，再复现一次错误，便可查看到更详细日志。 event/reactorevent/reactor 是saltstack 中的两个系统。两者结合我们可以定制一些自动化的功能，比如，minion端的salt-minion服务重启后，master端立马同步grains，pillars信息。event系统是一个本地的ZeroMQ PUB接口, 用于产生salt events.这个event总线是一个开放的系统,用于发送给Salt和其他系统发送关于操作的通告信息.event系统产生event有一个严格的标准. 每一个event有一个 tag . event tags用于快速过滤events. 每一个event有一个数据结构附加在tag后. 这个数据结构是个字典, 包含关于本event的信息。reactor 系统会结合sls文件在master端去匹配event tags，匹配成功后执行相应的sls文件。 暂时还没有碰到此类需求，回头再看吧。 SaltStalk 架构关于架构部分，内容比较繁杂，通常会涉及到multimaster,multimaster with failover,salt-syndic等，后续有需求了再研究吧。 参考SALTSTACK ARCHITECTURE 参考文章saltstalk doc Saltstack SLS文件解读 Saltstack自动化（五）sls文件使用 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux shell系列-- 如何在系统中保证只有一个实例]]></title>
      <url>%2F2016%2F11%2F01%2F2016-11-1-keep-one-instance-in-a-machine%2F</url>
      <content type="text"><![CDATA[目录结构：如何在系统中保证只有一个实例 写 pid 方式 文件锁的方式 独占端口的方式 dbus API 写成service并交由第三方托管 如何在系统中保证只有一个实例从这发现的这个问题，如何确保系统中某个程序只存在一个实例 从评论区中汇总几种可行的方案并实验，记录如下，。 写 pid 方式这种方式应该是最直观，最容易想到的，类似于算法中的暴力破解法。原理就是在程序执行之前将PID写入某个约定好的文件，每次程序启动前通过检测该PID对应的程序是否存在来判断。它的伪代码如下： Invariant: File xxxxx will exist if and only if the program is running, and the contents of the file will contain the PID of that program. On startup: If file xxxxx exists: If there is a process with the PID contained in the file: Assume there is some instance of the program, and exit Else: Assume that the program terminated abnormally, and overwrite file xxxx with the PID of this program Else: Create file xxxx, and save the current PID to that file. On termination (typically registered via atexit): Delete file xxxxx 这种方式还要注意假如两个程序同时读取文件的时候这种情况，可以再加一个文件锁（其实就是下面的方式了）。 文件锁的方式对文件锁的概念其实一直比较陌生，但其实与我们还是挺接近的。我们数据库中经常出现锁表或者锁记录的情况，就是利用文件锁实现的。关于文件锁的理论知识可以参考如下一篇文章：Linux 2.6 中的文件锁 一个简单的demo如下： 123456789101112131415161718#!/usr/bin/env pythonimport fcntlimport os, timeFILE = "flag-file.txt"if not os.path.exists(FILE): file = open(FILE,"w") file.write(""+os.getpid()) file.close()file = open(FILE,"r+")try: fcntl.flock(file.fileno(),fcntl.LOCK_EX | fcntl.LOCK_NB) print "acquire lock, i am running" time.sleep(10) except IOError: print "another programme is running" 效果如下： 使用这种方式有个地方要注意：你不知道什么时候别人可能把你作为flag的文件给删除了。当然可以通过逻辑判断的方式来解决，但当程序比较复杂的时候可能还会出现意料外的问题。所以我们可以用端口独占的方式来避免这种情况。 独占端口的方式同上，通过独占端口的方式来达到互斥的效果，demo如下： 1234567891011121314151617#!/usr/bin/env pythonimport socketimport os, timePORT = 888def check_port(ip,port): s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) try: s.bind((ip,int(port))) print "port is open,i'm running" time.sleep(10) except: print "another programme is running"if __name__ == '__main__': check_port("127.0.0.1",PORT) dbus API首先简单介绍下什么是 dbus,直接引用wiki： D-Bus是一个进程间通信及远程过程调用机制，可以让多个不同的计算机程序（即进程）在同一台电脑上同时进行通信[4]。D-Bus作为freedesktop.org项目的一部分，其设计目的是使Linux桌面环境（如GNOME与KDE等）提供的服务标准化。 dbus 的Python库主要有几下几个 1,GDbus and QtDbus are wrappers over the C/C++ APIs of GLib and Qt 2,pydbus is a modern, pythonic API with the goal of being as invisible a layer between the exported API and its user as possible 3,dbus-python is a legacy API, built with a deprecated dbus-glib library, and involving a lot of type-guessing (despite “explicit is better than implicit” and “resist the temptation to guess”). 4,txdbus is a native Python implementation of the D-Bus protocol for the Twisted networking framework. demo代码如下，因为对dbus这一块不熟，好像有个地方参数不对，后续再调吧 12345678910111213141516171819202122#!/usr/bin/env pythonimport dbusimport dbus.serviceimport dbus.mainloop.glibimport os, timedef check_dbus(bus_name): dbus.mainloop.glib.DBusGMainLoop(set_as_default = True) #bus = dbus.SessionBus() # s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) try: my_bus_name = dbus.service.BusName(bus_name, bus, allow_replacement = False, replace_existing = True, do_not_queue = True) #s.bind((ip,int(port))) print "i'm running" time.sleep(10) except: print "another programme is running"if __name__ == '__main__': check_dbus("name") 写成service并交由第三方托管这个就没什么好说的了，将Python文件打包并执行，由supevisor等第三方工具来管理。 参考文章v2ex stackoverflow-Preventing multiple process instances on Linux Linux 编程中的文件锁之 flock Linux 2.6 中的文件锁 Python wiki END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列-- benchmarking工具rally]]></title>
      <url>%2F2016%2F10%2F31%2Fopenstack-rally%2F</url>
      <content type="text"><![CDATA[目录结构：ralley是什么 rally 安装以及快速引导 rally 任务启动配置文件 rally 插件 ralley是什么一句话概括，rally是一个测试openstack 性能的工具。它的存在主要回答这样一个问题：How does OpenStack work at scale? 意译一下就是：openstack 在负载比较大的规模下运转情况怎么样？rally以一种插件式的形式工作，对openstack是无侵入的。通常会作为CI的一种功能集成到CI中去。 tips:随着rally项目的不断发展，rally又衍生出了其他的一些功能，比如与tempest结合进行openstack的功能测试等。如下图 deploy(部署)功能并非是另一种部署方式，只是与devstack等部署方式以插件形式结合来简化工作而已。 rally 安装以及快速引导rally 安装比较简单，还可以跟Devstack 部署时一块安装，以及docker 安装check this! 安装完成之后我们就可以进行性能测试了，大致顺序如下： 创建一个openstack 的rally测试环境：如果是有现成的openrc文件，那么直接source 一下，然后执行： rally deployment create –fromenv –name=existing 就创建完成了。也可以将openrc文件中的内容写入一个json文件，假设取名为existing.json,执行如下命令： rally deployment create –file=existing.json –name=existing 。 执行rally deployment list 即可查看我们创建的deployment 以及哪一个正在被激活（正在使用）。通过 rally deployment check 可查看检测到的openstack服务： 创建完一个deployment之后我们就可以进行测试了，我们需要指定一个json或yaml文件 来说明测试的内容， 在rally安装目录samples/tasks/scenarios 中有很多示例文件，比如samples/tasks/scenarios/nova 目录下就有nova对应的情景测试文件，例如boot-and-delete.json，就是启动虚拟机再删除操作，内容如下： 1234567891011121314151617181920212223242526&#123; "NovaServers.boot_and_delete_server": [ &#123; "args": &#123; "flavor": &#123; "name": "m1.tiny" &#125;, "image": &#123; "name": "^cirros.*uec$" &#125;, "force_delete": false &#125;, "runner": &#123; "type": "constant", "times": 10, "concurrency": 2 &#125;, "context": &#123; "users": &#123; "tenants": 3, "users_per_tenant": 2 &#125; &#125; &#125; ]&#125; 利用此文件启动任务： rally task start samples/tasks/scenarios/nova/boot-and-delete.json还可以利用rally查看images ，flavors列表等。 rally提供了多种方式进行结果的查看，最直观的就是以html形式在浏览器中展现（需翻墙）：rally task report –out=report1.html –open tips:执行任务时指定的json 文件格式如下： 1234&#123; "&lt;ScenarioName1&gt;": [&lt;benchmark_config&gt;, &lt;benchmark_config2&gt;, ...] "&lt;ScenarioName2&gt;": [&lt;benchmark_config&gt;, ...]&#125; 其中， 格式如下： 1234567&#123; "args": &#123; &lt;scenario-specific arguments&gt; &#125;, "runner": &#123; &lt;type of the runner and its specific parameters&gt; &#125;, "context": &#123; &lt;contexts needed for this scenario&gt; &#125;, "sla": &#123; &lt;different SLA configs&gt; &#125;&#125; rally 任务启动配置文件rally任务启动配置文件就是上文中提到过的json/yaml文件，其实可以通过此配置文件实现很多功能，比如：多任务配置，设置SLA(Service-Level Agreement,就是一个成功与否的基准，例如：max_seconds_per_iteration”: 10)，利用jinja模板传参数进来，或利用jinja模板进行简单的逻辑控制等，check this,不再赘述。 rally 插件rally插件就是我们在json文件中配置的ScenarioName，通过这些插件的组合可以完成我们自己定义的要求。几个命令如下： 关于rally与tempest的结合使用还不是很成熟，感兴趣的可以去试下，check this! 参考文章openstack rally wiki SDN使用 Rally 来实现 Openstack Tempest 测试 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列 --关于keystone配置详解]]></title>
      <url>%2F2016%2F10%2F24%2Fopenstack-keystone-config%2F</url>
      <content type="text"><![CDATA[目录结构keystone 配置文件综述 keystone 配置文件详解 keystone 配置文件综述关于keystone的概述以及整体架构我们已经在之前的一篇文章中做过介绍，不再赘述。这里具体介绍下keystone的配置方面的知识。 首先，keystone的配置文件主要有两个，一个是通常位于/etc/keystone/或/etc/目录下的keystone.conf，另一个是位于keystone安装根目录下的keystone-paste.ini。其中，后者其实就是paste deploy 文件，我们这里不需要关注，主要讨论keystone.conf文件。 tips:keystone默认端口是35357，因此我们最好将35357端口从临时端口范围中删除，以免该端口被用作临时端口。命令如下： sysctl -w ‘net.ipv4.ip_local_reserved_ports=35357’ 如果是想重启后仍然有效，在/etc/sysctl.conf 或/etc/sysctl.d/keystone.conf文件后追加： net.ipv4.ip_local_reserved_ports = 35357 keystone 配置文件详解keystone.conf文件中所有的配置项如下： 接下来我们从其中挑选几个值得注意的配置项详细说明下： 关于TOKEN 对于token的持久化有两种形式：一种是以key/value对的形式存储，另一种是存储在SQL数据库中。需要在[token] 配置项中的driver选项进行具体配置。 token provider 大致有以下四种形式： UUID:长度固定为 32 Byte 的随机字符串,UUID token 简单美观,验证流程如下： 由于每当 OpenStack API 收到用户请求，都需要向 Keystone 验证该 token 是否有效。随着集群规模的扩大，Keystone 需处理大量验证 token 的请求，在高并发下容易出现性能问题。为了杜绝keystone成为瓶颈，引出了下面的几种。 PKI:验证流程如下：可以看出，PKI token携带更多用户信息的同时还附上了数字签名，以支持本地认证，从而避免了步骤 4。因为 PKI token 携带了更多的信息，这些信息就包括 service catalog，随着 OpenStack 的 Region 数增多，service catalog 携带的 endpoint 数量越多，PKI token 也相应增大，很容易超出 HTTP Server 允许的最大 HTTP Header(默认为 8 KB)，导致 HTTP 请求失败。 PKIZ:顾名思义，就是对PKI token进行压缩，但压缩效果有限，无法良好的处理 token size 过大问题。原理同上，无须赘述。 FERNET:前三种 token 都会持久性存于数据库，与日俱增积累的大量 token 引起数据库性能下降，所以用户需经常清理数据库的 token（ 命令为：keystone-manage token_flush）。为了避免该问题，社区提出了 Fernet token，它携带了少量的用户信息，大小约为 255 Byte，采用了对称加密，无需存于数据库中。 几种token provider比较如下： Caching Layer 启动keystone的的cache功能，首先要在[cache]配置项，设置为enabled,指定backend等等。然后再在其他具体配置项进行设置。支持cache的地方有以下几个地方： token resource role具体配置根据backend的不同也不尽相同。 Service Catalogkeystone 提供了两种配置选项。 SQL-based Service Catalog：配置选项如下： 12[catalog]driver = sql 可以通过查阅以下命令进行相关操作： 123openstack --helpopenstack help service createopenstack help endpoint create File-based Service Catalog (templated.Catalog)：就是基于一个配置模板文件，配置选项如下： 123[catalog]driver = templatedtemplate_file = /opt/stack/keystone/etc/default_catalog.templates 模板文件示例： 其他 其实，除了sql数据库，我们也可以把数据存在文件中，以 LDAP形式，不过用的不多，不再赘述。 参考文章openstack doc 理解 Keystone 的四种 Token END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git系列--关于github 的碎碎念]]></title>
      <url>%2F2016%2F10%2F17%2Fgithub-tips%2F</url>
      <content type="text"><![CDATA[目录结构github 快捷键 github trending &amp;&amp; github subscribe 为开源项目贡献代码 git 常用命令 github 快捷键逛v2ex的时候逛到了这个帖子，录了几个有关 Github 的视频，我觉得你也应该知道这些,很有意思,把视频看了一遍，都不是很长，强烈推荐。顺手记录下这些信息。首先是github的一些快捷键： 项目中搜索含有某个关键字的文件：在github 项目的code目录按下 “t”,即可进入搜索页面，如下图： 列出github的快捷键：在项目目录按下 “？” 即可出现： 进入 issue页面：按下“g” 与 “i”键，就是goto issue 的缩写 进入code页面：按下“g”与“c”键，goto code 其他的按下“？”自己探索吧。 github trending &amp;&amp; github subscribe github trending:进入这个页面github trending就可以看到最近按照stars数排序靠前的比较优秀的项目，可以按照语言分类。 github subscribe:进入这个页面github subscribe,我们可以订阅我们关注的大牛，他们的动态会按周/月发送到你的邮箱。 为开源项目贡献代码关于这个，强烈推荐看下视频吧，直观易懂。这里简短记录下过程： fork and clone github remote -add upstream ssh@xxxxxxxx.git :标示我们fork的上级，master分支要与upstream分支保持一致 github checkout -b feature/bug-fixed : 创建并切换分支，在此分支上编码。 git add &amp;&amp; commit git checkout master &amp;&amp; git pull upstream master：在push之前我们先要保证master与upstream一致。 git checkout feature/bug-fixed &amp;&amp; git rebase master：将在bug-fixed上的commit log 打到master分支的最上面。 git push origin featue/bug-fixed git pull request: 现在可以向开源作者pull request了 git 常用命令 切换分支后，清理本地目录：git reset –hard HEAD &amp;&amp; git clean -i 不详细写了，以后用得着直接去这个页面找吧：git-tips 参考文章v2ex git-tips END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph系列--ceph存储介绍]]></title>
      <url>%2F2016%2F09%2F29%2Fceph-intro%2F</url>
      <content type="text"><![CDATA[目录结构：关于分布式存储 ceph 是什么 ceph 架构 what makes ceph unique 关于分布式存储在了解ceph之前，我们最好还是先了解一下分布式存储的一些概念。分布式存储是随着存储容量的需求变大而出现的概念。由于存储需求变大，出现了两种解决方案，一种是scale up,采用容量更大，价格更昂贵的存储机器。 另一种是scale out,利用众多的普通存储机器（PC机）横向扩展成一个大的存储集群。 第二种就是分布式存储的概念了。相对采用昂贵的专用商业存储设备来说，这种存储的性价比更高，但也因此引出了分布式存储几个比较棘手的问题，比如数据如何组织存储，如何保证高可用性，如何保证冗余数据的一致性，是否支持分布式事务，如何避免单点失败等等，这些概念可以参考这两篇文章，分布式存储系统 知识体系,An Introduction to Distributed Systems。 ceph 是什么以下篇目是基于这场presentation，Ceph Intro &amp; Architectural Overview. 我们首先看一下ceph 的设计哲学： 由此我们也可以看出ceph的一些特点：开源，社区驱动，可扩展，无单点故障，软件层面，自我管理。 那具体提供什么服务呢？看下图 对应过来就是对象存储，块存储，文件系统存储。 ceph 架构再从技术层面看下ceph架构 我们从底往上一层层的介绍，首先是RADOS,我们从上图的介绍中也可以看出，RADOS是一系列的节点，这些节点上跑着两种程序，跑着OSD程序的我们称为OSD节点（storage node），跑着MON程序的我们称为MON节点（monitor node）。 我们再详细看下OSD节点。 每个OSD节点内，有多个磁盘disk,这些磁盘上是对应的文件系统（官网推荐 xfs），在往上就是我们的逻辑存储单元OSD，每个磁盘对应一个OSD。我们的数据就是存储在OSD里。OSD与MON节点各自功能如下： 可以看出MON节点只负责集群状态的维护，具体存储交给OSD处理。 再看下LIBRADOS的作用。 简而言之，就是为Application的调用提供了多种语言版本的接口。通信机制为SOCKET。 再往上是RADOSGW. 简而言之，就是在LIBRADOS提供的API的基础上进行封装对外提供对象存储的REST服务。 与之并列的是RBD，也就是ceph提供的块存储服务。我们最常用的就是attach到某台虚拟机上（如下图），Ross Turk还提到了另外两种用法，感兴趣可以去看一下。 关于ceph提供的文件系统因为用的不多，不再详述。 what makes ceph unique第一个就是crush算法，一种计算寻址而非查找寻址的方法。 图中的placement group 有很重要的作用，有了它，我们底层添加节点或节点崩溃，ceph都会自动更新，而对用户是透明的。 其次是 rbdlayering ,参考这里RBD LAYERING 参考文章Ceph Intro &amp; Architectural Overview Ceph Intro and Architectural Overview by Ross Turk 理解 OpenStack + Ceph （2）：Ceph 的物理和逻辑结构 [Ceph Architecture] ceph docEND]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python系列--Python中的生成器以及协程的相关知识]]></title>
      <url>%2F2016%2F09%2F26%2Fpython-yield-and%20coroutine%2F</url>
      <content type="text"><![CDATA[目录结构：迭代器与生成器 yield, send 关键字解析 协程的一些知识 迭代器与生成器在说生成器之前我们得先讨论一下迭代器的概念。 我们知道，对于一个可迭代的对象，我们可以通过 for in 的语句来进行迭代输出。这个可迭代的对象可以是一个list，string，文件等。其实我们深入这个可迭代的对象会发现，这些可迭代对象都实现了两个函数：iter() 和 next()。在我们调用list等这些可迭代对象的时候，需要把整个list数据全部读到内存里。这样就存在一个问题：list数据量小还可以，一旦变得特别大，内存就有可能被占满而导致运行缓慢甚至崩溃。这个时候我们想到，能不能只把我们需要的那个数据读入内存，调用next()的时候再把下一个数据读入内存呢。这就是生成器了。 一般来说，含有 yield 语句的函数就可叫做生成器。 1234567891011def g(): yield 0 yield 1 yield 2ge = g()type(ge) #ge 的类型为 'generator'dir(ge) #列出ge包含的函数我们发现有__iter__() 和 next()ge.next()ge.next()ge.next() 生成器也是一种迭代器，但我们只可以读取它一次，因为它并非把所有数据都存入内存中，而是实时地生成数据。上述例子当我们再次读取ge.next()后就会报错。 此处再略提一下生成器推导式：我们知道Python中有各种集合的推导式，如下： 列表推导式：my_list = [ f(x) for x in sequence if cond(x) ] 字典推导式：my_dict = { k(x): v(x) for x in sequence if cond(x) } 集合推导式：my_set = { f(x) for x in sequence if cond(x) } 相应的，我们也可以用推导式来生成生成器，跟列表推导式类似，只需要将[]改为()：my_generator = ( f(x) for x in sequence if cond(x) ) yield, send 关键字解析由上文可以看到，yield关键字是理解生成器的关键。那么yield什么意思呢？我们来详细解释一下。yield 关键字有点类似我们平常写程序用到的return。但是程序运行到return的时候就会返回，运行完毕。而运行到yield的时候也会返回，但会保存上下文之后再返回，等到下次再次唤醒该程序的时候恢复上下文，继续从此运行，直到碰到下一次yield。 123mygenerator = (x*x for x in range(3))for i in mygenerator : print(i) #输出0,1,4 首先我们创建了一个生成器mygenerator，注意，此时程序并不执行，只是创建了一个生成器。接着是一个for循环,fou循环其实就是执行了一个next()函数，此时进入生成器获取第一个i就是0乘以0，也就是0。返回并输出。再次循环，进入生成器，以此类推。这个例子可能不是特别直接，只是说明一下运行的逻辑顺序，下文中我们有一个更为直接的实例。 我们先看一下send()函数再将两个函数放在一个例子中说明一下。 在调用生成器时，除了next()方法，我们还可以用send()方法唤醒生成器，而且send(args),在唤醒生成器的同时会把参数 args传给指定的数据。如下例： 1234567891011121314def f(): #定义一个生成器函数 while True: val = yield # val接受send()传过来的的参数并赋值 yield val*10 g = f() #新建一个生成器g.next() #触发生成器，生成器执行到val=yield ，保存上下文退出，等待传值g.send(1) # 触发生成器，生成器继续执行，赋值val = 1，执行到yield val*10，保存上下文，,返回10，退出。g.next() #同上g.send(10)g.next()g.send(0.5) 注意，我们在用send()之前必须保证生成器已经执行到yield,也就是说生成器已经被触发过一次，我们可以用send(none)来实现。其实，next()跟send(none)效果是一样的。 接下来我们看一个比较复杂的例子： 1234567891011121314151617181920212223242526#!/usr/bin/env python# -*- coding: utf-8 -*-# @Date : 2016-09-22 16:40:36# @Author : Zhang Chen (pekingzcc@gmail.com)# @Link : zhangchenchen.github.ioimport osdef countdown(n): print "Into genaration :counting down from", n while n &gt;= 0: print "Genaration: ###loops from here###" newvalue = (yield n) print "Gerenation: newvalue is",newvalue # If a new value got sent in, reset n with it if newvalue is not None: n = newvalue else: n -= 1if __name__=='__main__': c = countdown(5) print "Main function begin from here " for x in c: print "Main function: x is", x if x == 5: c.send(3) 这个例子看明白了，yield与send的用法就理解的差不多了。我们首先看一下执行结果： 接下来分析一下：我们从main 函数开始，先创建了一个生成器（此时并未执行），接着输出main函数开始的语句。接着进入for循环，注意，遇到for循环就相当于执行了一次next(),所以进入生成器输出“Into genaration :counting down from 5”，继续运行，进入生成器的for循环，输出“Genaration: ###loops from here###”，继续往下，碰到yield n ，保存上下文，退出并返回n(此时是5)到main函数，主函数输出“Main function: x is 5”，进入条件语句，c.send(5)触发生成器，再次进入生成器，赋值newvalue为3，接着输出“Gerenation: newvalue is 3” ，赋值n = newvalue =3 ,继续循环输出“Genaration: ###loops from here###”，碰到yield n ,返回main函数，注意，此时main函数又进入到for循环，所以再次进入生成器，但是没有send()数据传过来，也可以理解为send(none),所以输出“Gerenation: newvalue is none”,接着执行n-1 ,n变为2。继续循环，输出“Genaration: ###loops from here###”。。。。。 协程的一些知识在了解协程之前，我们需要先从进程，线程说起。我们知道，进程的出现是为了并发，在一台机器上同时运行多个程序（当然，内部实现可能是多CPU并行，也有可能是单CPU时间分片，但在外部看来就是多个程序一起运行），进程的切换需要陷入内核，由OS来进行切换。一切换进程得反复进入内核，置换掉一大堆状态，这样，进程数一高，就会吃掉很多的系统资源。为了解决这个问题，就出现了线程的概念。 一个进程里可以有多个线程，这样就能处理多个逻辑，当某个线程阻塞的时候，可以切换线程到另一个线程。因为线程是共享附属进程的资源的，它们件的切换相对要比进程间的切换消耗的资源要少很多。但之后，问题又来了，操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还需要做这些数据的恢复操作，所以线程多了之后，它们之间的切换也非常耗性能。 协程的概念就来了，既然线程切换费资源，那我们干脆自己做逻辑流的切换，不用交给OS处理。注意，这里经常提到的是切换，具体到应用场景就是IO密集型场景。在cpu密集型的场景中协程的意义就没有那么大了。在IO处理时，我们有同步，异步两种处理方式，关于IO模型，可以check this 。在异步处理IO时，我们需要写回调函数来实现异步，这种写法是比较反人类的，可读性比较差。协程可以很好解决这个问题。比如 把一个IO操作 写成一个协程。当触发IO操作的时候就自动让出CPU给其他协程。协程的切换很轻，消耗资源少。协程通过这种对异步IO的封装既保留了性能也保证了代码的 容易编写和可读性。 其实协程不是一个新生的事物，它在很早之前就出现了，只不过最近因为在一些动态语言的世界里大放异彩。对其历史感兴趣的check this。 再简单说一下Python中的协程，其实Python中的协程跟yield关键字是分不开的。只要含有yield的函数都可以认为是一个协程，利用协程实现的生产者消费者如下： 12345678910111213141516171819202122232425262728293031323334import random def get_data(): """返回0到9之间的3个随机数，模拟异步操作""" return random.sample(range(10), 3) def consume(): """显示每次传入的整数列表的动态平均值""" running_sum = 0 data_items_seen = 0 while True: print('Waiting to consume') data = yield data_items_seen += len(data) running_sum += sum(data) print('Consumed, the running average is &#123;&#125;'.format(running_sum / float(data_items_seen))) def produce(consumer): """产生序列集合，传递给消费函数（consumer）""" while True: data = get_data() print('Produced &#123;&#125;'.format(data)) consumer.send(data) yield if __name__ == '__main__': consumer = consume() consumer.send(None) producer = produce(consumer) for _ in range(10): print('Producing...') next(producer) 利用协程的Python库比较常见的是Greenlet库，它是以C扩展模块形式接入Python的轻量级协程，将一些原本同步运行的网络库以mockey_patch的方式进行了重写。Greenlets全部运行在主程序操作系统进程的内部，它们被协作式地调度。 参考文章A Curious Course on Coroutines and Concurrency 提高你的Python: 解释‘yield’和‘Generators（生成器） (译)Python关键字yield的解释(stackoverflow) Python 中的进程、线程、协程、同步、异步、回调 什么是Python中的生成器推导式？ 生成器 聊聊 Python 中生成器和协程那点事儿 GENERATOR.SEND() WITH YIELD 了解协程（Coroutine）/) 协程的好处是什么？ 为什么觉得协程是趋势？ 编程珠玑番外篇-Q 协程的历史，现在和未来 利用python yielding创建协程将异步编程同步化 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列 --keystone]]></title>
      <url>%2F2016%2F09%2F16%2Fopenstack-keystone%2F</url>
      <content type="text"><![CDATA[目录结构keystone 是什么 keystone 的架构 token 的生成方式 可信计算部分介绍 keystone 是什么keystone 是 openstack 的认证服务模块（identy service）。 nova,glance,swift,cinder等其他服务通过keystone注册其服务的endpoint，针对这些服务的任何调用都需要经过keystone的身份认证，并获得服务的endpoint进行访问。 keystone 提供的服务可以概括为以下四个方面： Identity:对用户身份进行验证。用户的身份凭证通常是用户名和密码。 Token:Identity确认完用户身份后，会给用户提供一个token以请求后续的资源。而keystone也会提供针对token的验证。token大致有两类：一类是与Tenant(也就是project)无关的token，通过这个token，可以向keystone获取Tenant列表，用户选择要访问的Tenant,然后可以获取与该Tenant绑定的token,只有通过与某个特定Tenant绑定的token才能访问此Tenant中的资源。token有自己的过期时间，如果删除某个用户的访问权限，只要删除对应token即可。 Catalog:Catalog服务对外提供一个服务的查询目录，即可访问的endpoint列表。 Policy:一个基于规则的身份验证引擎。通过配置文件来定义各种动作与用户角色的匹配关系。该部分已作为Oslo的一部分进行开发维护。 以创建虚拟机为例，keystone 的大致工作流程如下(注：以下内容摘自&lt;&gt;)： 用户Alice发送自己的凭证到keystone，keystone认证通过后，返回给Alice一个token以及服务目录。 Alice通过token请求keystone查询他所拥有的Tenant列表。（如果已经知道Tenant，略过以上两步） Alice选择一个Tenant，发送自己的凭证给keystone申请token，keystone验证后，返回token2。 Alice选择endpoint并发送token2请求创建虚拟机，keystone验证token2(包括该token是否有效，是否有权限创建虚拟机等)成功后，把请求转发给Nova，并创建虚拟机。 keystone 的架构 keystone还涉及另外一个子项目，keystonemiddleware,它提供了对token合法性进行验证的中间件。比如：客户端访问keystone提供的资源时用的是PKI类型的token，为了不必每次都需要keystone介入token的验证，我们通常会在本地节点上缓存相关证书和密钥，利用keystonemiddleware对token进行验证。 Keystone项目本身，除了后台的数据库，主要包括了一个处理restful请求的API服务进程。这些API涵盖了Identity,Token,Catalog,Policy等服务。这些不同的服务提供的功能则分别由后端Driver实现。 token 的生成方式在openstack F版本之前，token的生成方式只有UUID这一种方式（即由程序随机生成一段序列），但是在大规模的集群中，大量客户端并发请求的情况下，keystone的性能存在瓶颈（该版本还未引入keystonemiddleware）,所有的请求都要跟keystone交互。于是PKI系统在随后的版本中引入，就可以做到本地检验而不用与keystone频繁交互。 PKI详细介绍，check this!此处简单介绍几个概念： Certificate Auth：即CA,认证中心，数字签证的签发机构，是PKI应用中权威的，可信任的第三方机构。 CA私钥：CA签发的非对称加密算法中的私钥。 CA公钥证书：包含CA的公钥信息。 签名私钥 签名公钥证书 下图为UUID的token验证流程： 下图为PKI的token验证流程： 可信计算部分介绍在云计算环境中，可能会有成千上万个计算节点部署在不同的地方，可能有些云租户对安全的要求比较高，要求应用或虚拟机必须运行在验证为可信的节点上。openstack 在E版本时为此引入了可信计算池的概念。可信计算池的实现位于Nova项目。在FilterScheduler中加入了一个新的TrustedFilter,经过该filter的即认为是可信计算的节点。 那么可信节点是如何判定的呢？计算节点采用基于Intel TXT 的TBoot进行可信启动，对主机的BIOS,VMM和操作系统进行完整性度量，并在得到来自认证服务的请求时将度量数据发送给认证服务。认证服务器部署基于OpenAttestation的认证服务，通过将来自主机的度量值与白名单数据库进行比对确定主机的可信状态。 参考文章openstack 设计与实现 Understanding OpenStack Authentication: Keystone PKI END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[安全系列 --简单加固 ubuntu 服务器]]></title>
      <url>%2F2016%2F09%2F16%2Fsecure-your-ubuntu-server%2F</url>
      <content type="text"><![CDATA[目录结构前言 花五分钟的时间加固 ubuntu server 其他 前言逛v2ex的时候看到了一篇讨论 在服务器上做什么加固策略的帖子，想起自己之前看过的一篇文章，My First 5 Minutes On A Server; Or, Essential Security for Linux Servers，顺手写下这篇文章，权当记录，本篇的绝大部分内容都是出自这篇文章，强烈推荐，如果觉得TLDR,就接着往下翻吧。 安全，在大部分程序员潜意识里总是处于一种low priority的状态（参看此知乎链接）。比如，我们搭建一个web server，会在第一时间去想用什么去部署，去实现，实现之后可能还会考虑高可用，可扩展等因素。而安全却总是会在最后才出现在我们的脑海里，甚至于直到出现安全事故才去关注。其实，我们拿到一台Linux服务器，只需要花些时间做一些安全方面的考量，做一些简单的措施就可以抵挡很大一部分攻击，提高攻击门槛。 那对于安全方面的措施，我比较推崇上文作者所说：简单有效。因为比较复杂的安全策略实施起来的增量收益可能会低于增加的大量人力物力资源。而且指不定还会引入其他的安全隐患。引用作者的话说：据我的经验来讲，大部分安全事故，要么是没有做足够的安全措施，要么是做了足够的安全措施，但没有更新维护导致出现漏洞。 Simplicity is the heart of good security. 下面就简单说下我们拿到一台服务器（或VPS）后需要做的一些安全加固措施。 花五分钟的时间加固 ubuntu server准备本文中用的Linux发行版为 14.04。其他实验环境作者没有实践。首先更改你的 root 密码为强类型密码。然后执行如下命令做一下升级： 12apt-get updateapt-get upgrade 尽量不要使用root用户新增一个用户，当需要root权限时再用sudo暂时提升权限。新建用户的密码也要是强类型密码。 12adduser example_user #新建 用户adduser example_user sudo #给新建用户添加sudo权限 使用SSH密钥认证登录利用ssh登录到Linux服务器有很多种方案，比较常用的就是用户名密码登录和SSH密钥认证的方式。相对用户名密码登陆方式，后者是一种更安全的方案，因为前者容易受到暴力破解的威胁，而后者用暴力破解的方法不太实际。首先需要在客户端产生公约密钥对。Linux下是如下命令： 1ssh-keygen -t rsa 如果客户端是windows，可以利用putty等客户端实现。 创建完之后，我们会获取到两个文件，id_rsa.pub，和 id_rsa。也就是公钥文件和密钥文件。 接下来将公钥内容复制到服务器.ssh 目录下authorized_keys文件中。 关闭SSH 密码服务器登陆添加如下内容到 /etc/ssh/sshd_config 文件中。 123PermitRootLogin noPasswordAuthentication noAllowUsers deploy@(your-ip) deploy@(another-ip-if-any) # 添加允许登录IP 重启ssh服务 1service ssh restart 添加防火墙添加防火墙有很多种方法，最简单的就是利用ubuntu 提供的ufw。1234ufw allow from &#123;your-ip&#125; to any port 22 #限定IP的端口ufw allow 80 #打开80端口ufw allow 443 #打开443端口ufw enable 当然要根据你的实际要求来打开对应的端口。除了ufw，还有iptables,ip6tables,以及最新的nftables等都可以实现防火墙。 安装Fail2ban1apt-get install fail2ban Fail2ban 会在服务端扫描日志并根据一些策略屏蔽一些有可疑行为的用户（IP）。官方介绍说的比较清楚： Fail2ban scans log files (e.g. /var/log/apache/error_log) and bans IPs that show the malicious signs – too many password failures, seeking for exploits, etc. Generally Fail2Ban is then used to update firewall rules to reject the IP addresses for a specified amount of time, although any arbitrary other action (e.g. sending an email) could also be configured. Out of the box Fail2Ban comes with filters for various services (apache, courier, ssh, etc). Fail2Ban is able to reduce the rate of incorrect authentications attempts however it cannot eliminate the risk that weak authentication presents. Configure services to use only two factor or public/private authentication mechanisms if you really want to protect services. 设置安全自动升级123apt-get install unattended-upgradesvim /etc/apt/apt.conf.d/10periodic 更新如下内容： 1234APT::Periodic::Update-Package-Lists "1";APT::Periodic::Download-Upgradeable-Packages "1";APT::Periodic::AutocleanInterval "7";APT::Periodic::Unattended-Upgrade "1"; 再更新下这个文件/etc/apt/apt.conf.d/50unattended-upgrades 1234 Unattended-Upgrade::Allowed-Origins &#123; "Ubuntu lucid-security";// "Ubuntu lucid-updates";&#125;; 安装LogwatchLogwatch 是一个强大的日志解析和分析软件。它被设计用来给出一台服务器上所有的活动报告，可以以命令行的形式输出，或邮件发送。 123apt-get install logwatchvim /etc/cron.daily/00logwatch 添加如下内容： 1/usr/sbin/logwatch --output mail --mailto test@gmail.com --detail high 其他其实原文在hacking news 上引发了一系列的讨论。比如有人就提出，我们拿到一台服务器的时候，第一件事不是加固，而是应该先安装配置管理工具（Puppet or Chef），利用配置管理工具才更有条理。其实lz认为还是看实际场景吧，配置管理工具确实方便，尤其是对于一些大型的项目，而对于只搭一个blog的vps，确实没有必要。 参考文章My First 5 Minutes On A Server; Or, Essential Security for Linux Servers Securing Your Server Hacking news v2ex END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 系列 --cinder]]></title>
      <url>%2F2016%2F09%2F06%2Fopenstack-cinder%2F</url>
      <content type="text"><![CDATA[目录结构在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案. 本篇我们的主题是openstack的块存储项目cinder,那么第一个问题自然是 cinder是干什么用的？ 由这个问题出发，我们逐渐引出 cinder如何实现？ ，cinder的具体功能 。 cinder是干什么用的 cinder如何实现 cinder的具体功能 cinder是干什么用的openstack中创建虚拟机的时候是附带一块硬盘的，但这块硬盘随着虚拟机的destroy也相应的删除了。所以，openstack推出了两种应付持久存储的解决方案，一个是之前我们提过的对象存储swift，一个就是现在我们讲的块存储cinder。块存储,对象存储的概念我们之前已介绍过，不再赘述。需要说明的是，cinder并不是新开发的一个块设备存储系统，它更像是一个资源管理系统，对不同的存储后端进行封装，以统一的API形式向虚拟机提供持久块存储资源。对于不同的存储后端，它采用插件的形式，结合不同的后端存储的驱动提供块存储服务。 cinder如何实现首先看一下cinder的主要组件：简单介绍下这几个组件： cinder-api: 负责接受和处理外界的API请求，并将请求放入RabbitMQ队列，交由其他程序执行。 cinder-scheduler: 在多个存储节点的情况下，新建或迁移volume的时候由该程序依据指定的算法选出合适的节点。算法有过滤算法和权重算法两种。 cinder-volume： 运行在存储节点上，管理存储空间，处理cinder数据库维护状态的读写请求，通过消息队列和直接在块存储设备或软件上与其他进程交互。每个存储节点都有一个Volume Service，若干个这样的存储节点联合起来可以构成一个存储资源池。cinder-volume为后端的volume provider 定义了统一的 driver 接口，volume provider 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 接下来介绍下这种插件机制以及后端的具体存储：后端存储大致可以分为两类，一类是软件实现的存储系统，比如LVM,ceph,sheepdog等。另一类是商用的硬件支持的专业存储系统，比如IBM,HP,EMC等公司的专业存储系统。 以LVM 为例： 此处简单介绍下iSCSI协议,引自 wiki: iSCSI利用了TCP/IP的port 860 和 3260 作为沟通的渠道。透过两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置。iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换 SCSI 命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降。于是，iSCSI 常常被认为是光纤通道（Fiber Channel）的一个低成本替代方法，而光纤通道是需要专用的基础架构的。但是，基于以太网的光纤通道（FCoE）则不需要专用的基础架构。 其他商用存储系统： 两者对比，check this! cinder的具体功能 从代码层面了解具体实现的流程，check this! 参考文章探索 OpenStack 之（9）：深入块存储服务Cinder 每天五分钟玩转openstack openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python系列--python web 运行与部署]]></title>
      <url>%2F2016%2F09%2F05%2Fpython-web-intro%2F</url>
      <content type="text"><![CDATA[目录结构：在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案。 本篇我们的问题从 python web 是如何跑起来的？ 开始，一步步探究由此问题引起的其他内容。 python web 是如何跑起来的？ 探究 WSGI 探究 python web framework 探究 python web server 常见 python web生产部署环境 python web 是如何跑起来的？要知道python web 是如何跑起来的，我们首先要知道一个http请求的生命周期。一个http请求从浏览器发出，在服务端被http server接收，这个http server对请求进行解析，如果是一些图片，文本等静态资源，它就会根据路径去寻找对应的资源并返回。如果是请求的动态资源，比如jsp/php等，它就会把相应的http请求转发给一个web server,由web server对请求进行解析并处理，并最终返回一个 http response。这就是一个http请求的大致周期。接下来对应到python web中，当web server 接收到一个请求时，它当然是解析请求并把该请求映射到我们写的对应的处理逻辑中。我们的处理逻辑处理完成后，再将结果由web server 转发出去。那web werver 与 我们写的处理逻辑该怎样做才能实现上述效果呢，这就引出了 WSGI。 以下示例摘自廖雪峰的官方网站 WSGI:Web Server Gateway Interface.WSGI接口定义非常简单，它只要求Web开发者实现一个函数，就可以响应HTTP请求。 12345# hello.pydef application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return '&lt;h1&gt;Hello, web!&lt;/h1&gt;' 以上代码中application 函数接收两个参数，environ是一个包含所有HTTP请求信息的dict对象，start_response是一个发送HTTP响应的函数，该函数就是符合WSGI标准的一个HTTP处理函数。调用start_response()就发送了一个http header, http body 就是下文return 的数据。利用python自带的 web server 加载上面这个application. 1234567891011# server.py# 从wsgiref模块导入:from wsgiref.simple_server import make_server# 导入我们自己编写的application函数:from hello import application# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:httpd = make_server('', 8000, application)print "Serving HTTP on port 8000..."# 开始监听HTTP请求:httpd.serve_forever() 确保以上两个文件在同一个目录下，然后在命令行输入python server.py来启动WSGI服务器。 探究 WSGI以下内容摘自wsgi和tornado.md WSGI是为python语言定义的web服务器和web应用程序或框架之间的一种简单而实用的接口。wsgi是一个web组件的接口规范，它将web组件分为三类：server，middleware，application。接下来简单介绍下这三个组件： wsgi server :可以理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。 wsgi application :就是一个普通的callable对象，当有请求到来时，wsgi server会调用这个wsgi app。这个对象接收两个参数，通常为environ,start_response。environ可以理解为环境变量，跟一次请求相关的所有信息都保存在了这个环境变量中，包括服务器信息，客户端信息，请求信息。start_response是一个callback函数，wsgi application通过调用start_response，将response headers/status 返回给wsgi server。此外这个wsgi app会return 一个iterator对象 ，这个iterator就是response body。 wsgi middleware :可以简单地理解为对application的封装。通过封装实现一些公用的功能，如下示例用一个简单Dispatcher Middleware，用来实现URL 路由： 1234567891011121314151617181920212223242526272829303132333435363738from wsgiref.simple_server import make_serverURL_PATTERNS= ( ('hi/','say_hi'), ('hello/','say_hello'), )class Dispatcher(object): def _match(self,path): path = path.split('/')[1] for url,app in URL_PATTERNS: if path in url: return app def __call__(self,environ, start_response): path = environ.get('PATH_INFO','/') app = self._match(path) if app : app = globals()[app] return app(environ, start_response) else: start_response("404 NOT FOUND",[('Content-type', 'text/plain')]) return ["Page dose not exists!"]def say_hi(environ, start_response): start_response("200 OK",[('Content-type', 'text/html')]) return ["kenshin say hi to you!"]def say_hello(environ, start_response): start_response("200 OK",[('Content-type', 'text/html')]) return ["kenshin say hello to you!"]app = Dispatcher()httpd = make_server('', 8000, app)print "Serving on port 8000..."httpd.serve_forever() 以上代码就已经有点web framework 的意思啦。由这个实例我们可以看到写一个基于wsgi的python web framework其实就是实现wsgi application部分和wsgi middleware 部分。其实，写一个python web framework 也是一件非常简单的事，见廖雪峰的官方博客。下面我们就探究一下业界常用的python web framework。而wsgi server部分我们待会再谈。 探究 python web framework翻看python 的wiki，我们发现关于python的web framework 以井喷的速度发展，各式各样的框架。截两张图，分别是主流的全栈框架和非全栈框架： 主流的全栈python web框架: 这里要多说一句tornado框架，其实它不单是一个web框架，还是一个web服务器。作为Web框架，是一个轻量级的Web框架，类似于另一个Python web 框架Web.py，其拥有异步非阻塞IO的处理方式。作为Web服务器，Tornado有较为出色的抗负载能力，官方用nginx反向代理的方式部署Tornado和其它Python web应用框架进行对比，结果最大浏览量超过第二名近40%。见下表： 主流的非全栈python web框架： 探究 python web server Apache + modwsgi，配置安装比较繁琐，用的人越来越少。 Gunicorn，运行在UNIX平台上的python wsgi http server。 uWSGI,实现了uwsgi和WSGI两种协议的Web服务器，负责响应python 的web请求。此处提醒一下，uwsgi是一种uWSGI服务器自有的协议，它跟WSGI毛的关系都没有。关于他们的区别，看这 Tornado,上文已介绍。 附录一张图： 常见 python web生产部署环境 参考文章廖雪峰的官方网站 大家都是怎么部署python网站的 区分wsgi、uWSGI、uwsgi、php-fpm、CGI、FastCGI的概念 使用了Gunicorn或者uWSGI,为什么还需要Nginx？ fcgi vs. gunicorn vs. uWSGI What is the difference between uwsgi protocol and wsgi protocol? 理解Python WSGI Web Frameworks for Python END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--swift]]></title>
      <url>%2F2016%2F09%2F02%2Fopenstack-swift%2F</url>
      <content type="text"><![CDATA[目录结构：在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案。比如本篇我们学习openstack的对象存储模块swift，出现在脑海中的第一个问题就是 swift 是做什么用的？ 在解决了这个问题以后，我们知道是做对象存储的，那么， 它是如何进行存储的？ ，这就是第二个问题。在了解了它的存储机制后，我们还想问的就是 既然是存储，那么它的安全性机制是怎么做的？数据丢失了怎么办？ 。最后我们就要了解下 swift 的具体使用 以及 深入到代码层面 源码的组织架构 等。 swift什么用？ swift中数据如何存储？ swift中的安全机制 swift 如何使用？ swift 源码目录结构 swift 什么用？我们已经知道swift是openstack中负责对象存储的组件。自然而然的引出另一个问题，什么是对象存储？除了对象存储还有其他什么类型的存储，区别是什么？OK，接下来慢慢解决这些问题。 对象存储，有点类似于hashmap这种数据结构，我们可以通过key来获取/删除对应的value值。对象存储就是将对象数据作为一个逻辑单元存储起来，更多的操作是获取，删除。更新操作频率比较低。相应的，对象存储比较适合于存放静态数据或者更新频率比较低的大容量数据。比如一些多媒体数据，数据备份，虚拟机镜像等。 其他的存储类型还包括块存储，文件存储。 块存储通常提供的原始块设备，就是裸磁盘空间，比如磁盘阵列里面有2块硬盘，然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘，再采用映射的方式将这几个逻辑盘映射给主机使用。一些对实时性要求比较高的存储我们就得用块存储了，比如数据库。openstack中对应的组件为cinder。 文件存储就比较好理解了，我们常用的Windows操作系统就搭载了一套文件系统，提供文件存储。在分布式环境中，常用NFS协议，通过TCP/IP实现网络化存储，比如我们常用的NFS或者FTP服务器。 swift中数据如何存储？首先看下swift的整体架构： 整体架构主要分为两部分，访问层（Access Tier）和存储层（Storage Node）。访问层最主要的有两部分，Proxy node 主要负责Http请求的的转发，还有一个负责用户身份的认证（Authentication）。可选的是一个负载均衡设备。 当一个Restful请求到来时，Proxy node 负责接受用户请求并转发给认证服务进行处理，认证通过后再转发给存储层进行数据的操作。在转发给存储层之前，如果启用缓存的话，首先会去缓存服务器检查是否命中，命中就不用去数据层了。 接下来我们看下存储层，存储层由一系列的存储节点组成。为了便于组织以及故障隔离，这些存储节点在物理上做了一些划分，比如根据地理位置的不同划分region,一个 region相当于一个数据中心，每个rigion内部有多个zone，zone可以理解为一组独立的存储节点。一个zone包含多个存储节点（storage node）,一个node里有多个Device(可以理解为磁盘)，一个Device包含多个Partition(可以理解为磁盘中文件系统上的一个目录)。 在每个Storage node 上存储的对象在逻辑上又分为三层： 相应的，Storage node 中运行着三种对应的服务： Account Server :Account server 负责Account相关的一些服务，比如包含的Countainer列表，以及Account的元数据等。这些数据存储在一个SQLLite数据库中。 Countainer Server : 负责Countainer相关服务，比如包含的object列表，Countainer元数据。也存储在SQLlite数据库中。 Object Server : 提供对象数据的存取及相应的元数据服务，以二进制的形式存储在存储节点中，元数据以扩展属性的形式存储其中。因为对应的存储系统必须支持文件的扩展属性。 由以上信息我们得知swift对象最终以二进制的形式存储在存储节点中，存储节点中并没有“路径”的概念，那么它最终是如何与物理位置映射的呢？swift提出了ring的概念，ring其实就是记录了存储对象与物理存储节点的映射关系，object，countainer，account都有与之对应的ring。proxy server接收到http请求时，先查找操作实体（object，countainer，account）对应的ring,根据ring确定他们在对应的服务器集群中的具体位置，并将对应的http请求转发给对应的节点上的server。 此处注意的是，对象寻址并非是渐进式的（即寻找某个object先寻找account，再寻找下面的countainer,再寻找对应的object），而是直接寻找（即寻找object ,直接ring就可以找到，同理寻找某个account，countainer也是如此）。 为什么叫ring呢？其实这跟那个映射算法有关，这个将数据映射在相应服务器集群上某个节点的算法叫一致性hash算法，关于该算法，可以花5分钟的时间了解下这篇文章 至于swift 中是如何具体实现该算法的就不详细叙述了。下图为大致架构图： swift中的安全机制swift 中为了保证数据在损坏的情况下依然保证高可用，采用了增加副本的策略，每一个对象都会有若干个备份的副本（默认是三个），且存储在不同的zone里，这样，当某一个zone的对象数据不可用时，还可以启用其他zone里的副本。这样为了解决数据一致性的问题，swift开启了以下三个服务： Replicator: 负责检查数据与相应副本是否一致。如果出现数据不一致的情况，会将过时的数据更新，并负责将标记为删除的数据从磁盘上删除。 Auditor: 持续扫描磁盘检查Acount,Container和object数据的完整性，如果发现数据损坏，就会对相应的数据隔离，然后通过Replicator从其他节点上获取副本以恢复。 Updator: 在创建一个object的时候，需要对应的更新该object所在的container列表，以及再上层的account 列表，同理，创建container的时候也是如此。有时候这些更新操作因为相应的server繁忙而更新失败，swift会使用Updator服务继续处理这些失败的更新操作。 swift的使用swift的使用跟其他openstack的组件使用一样都是通过restful API来提供服务。swift API主要提供了以下几种功能： 对象存储 单个对象默认最大值是5GB，可以用户自己配置。 超过最大值对象的数据可以通过中间件进行上传，存储 对象压缩 对象删除 swift API 的执行过程大致如下：swiftClient 将用户的命令转换为标准HTTP请求（如果是用户请求本身就是HTTP请求那么没有这一步）;paste deploy将请求路由到proxy-server WSGI Application;根据请求内容调用对应的Controller(AccountController,ContainerController或者ObjectController),该controller会将请求转发到特定的存储节点上的WSGI Server(Account Server,Container Server或Object Server);这些server接收到http请求并处理。 swift 源码目录结构 bin: 主要是一些启动脚本，工具脚本。比如proxy-server负责启动proxy server，swift-ring-builder 用来创建ring。 swift：swift的核心代码。其子目录account,container,obj,proxy分别对应相应的服务的具体实现。common子目录是被多个组件共用的公共代码。 etc:配置文件模板 参考文章openstack 设计与实现 五分钟了解一致性hash swift github END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python库系列--requests 与 刷票]]></title>
      <url>%2F2016%2F08%2F31%2Fpython-requests%2F</url>
      <content type="text"><![CDATA[目录结构：关于刷票的若干思考 requests 引出 requests 基本用法 requests 高级用法 关于刷票的若干思考前两天同学让我帮他去一个网站帮他刷票，就顺手写了一个小程序，整理一下关于刷票的若干思考：其实应付普通的投票机制非常简单，只要懂点http的相关知识就可以搞定，原理就是构造相应的HTTP请求。一般来说，为了防止刷票，都会有以下几种防刷票策略： 限制IP : 相应的对策就是伪造IP。要想伪造IP的话首先要知道后端服务器程序是怎么获取IP的。至少有三个HTTP HEAD可以获取用户端IP，REMOTE_ADDR、HTTP_VIA、HTTP_FORWARDED_FOR 。REMOTE_ADDR是WEB服务器获取的用户IP值，也就是最终的外网IP，但它的重复值太多，所以不能作为唯一判断标志。 HTTP_VIA是倒数第二个代理服务器/网关的IP。 HTTP_FORWARDED_FOR是所有网关和你自己的IP列表。大部分服务端程序都是用HTTP_FORWARDED_FOR来获取IP。这样我们就可以通过修改HTTP_FORWARDED_FOR来达到伪造IP的目的。 限制网页来源 : 修改 http header中的refer。 限制 user-agent : 修改 http header中的 user-agent。 限制cookie : 修改 http header中的 cookie 限制登录后才可以投票 :模拟登录 验证码验证 : 简单的验证码可以通过OCR识别技术，增加干扰策略的验证码就比较复杂了，不予讨论。 当然，其实防刷票机制不只是上面简单的几种，一般来说比较安全的网站都会制定一些策略，比如IP可疑就增加验证码难度，设定黑白名单等。 requests 引出因为写上面的那个小程序用的是requests库。就简单的学习了一下这个库。python有自己的标准网络库，urllib,urllib2。但这两个库用着都不是特别的顺畅，requests 库的出现让人们眼前一亮，官网的declare:HTTP for Humans 。强调人性化。 requests 基本用法requests包要完成的内容无非就是发送和接收http请求。接下来我们就从发送请求和接收请求两方面讨论下它的基本使用。 发送请求 ：首先最简单的就是常用的get，post等发送HTTP请求类型，requests是这样发送的： 123456789import requestsr = requests.get('https://github.com/timeline.json')r = requests.post("http://httpbin.org/post")r = requests.put("http://httpbin.org/put")r = requests.delete("http://httpbin.org/delete")r = requests.head("http://httpbin.org/get")r = requests.options("http://httpbin.org/get") 对于 get 请求，我们可以通过添加params添加关键字参数： 1234import requestsparam = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get("http://httpbin.org/get", params=param) 对于post请求，我们可以通过data参数来模拟提交表单。 1234import requestsparam = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get("http://httpbin.org/get", data=param) 还可以通过 headers参数定制我们发送的请求： 123456789 import requests my_headers = &#123; 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36', 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Accept-Encoding' : 'gzip', 'Accept-Language' : 'zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4'&#125; r = requests.get("http://httpbin.org/get", headers=my_headers) 除此之外，还有timeout，cookies参数来设置响应时间，cookie。 相应内容 ： 任何时候调用 requests.*() 你都在做两件主要的事情。其一，你在构建一个 Request 对象， 该对象将被发送到某个服务器请求或查询一些资源。其二，一旦requests得到一个从服务器返回的响应就会产生一个 Response 对象。该响应对象包含服务器返回的所有信息， 也包含你原来创建的 Request 对象。 1234567891011import requestsr = requests.get('https://github.com/timeline.json')print(r.text) # 字符形式，可以自己设置编码print(r.content) # 二进制形式print(r.json()) # json形式,requests 中有一个内置的 JSON 解码器,处理 JSON 数据。print(r.raw()) # 原始套接字形式 高级用法 会话对象：http是无状态的，为了能够获取用户登录状态，一般服务器会用一个sessionId放在cookie中。用户发送的请求也包含这个sessionId，这些工作由浏览器来做。而requests也实现了类似功能。 12345678import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get("http://httpbin.org/cookies")print(r.text) #'&#123;"cookies": &#123;"sessioncookie": "123456789"&#125;&#125;' SSL 证书验证Requests 可以为 HTTPS 请求验证 SSL 证书，就像 web 浏览器一样。要想检查某个主机的 SSL 证书，可以使用 verify 参数: 123import requestsrequests.get('https://github.com', verify=True) 代理利用proxies 参数来配置请求 12345678import requestsproxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;requests.get("http://example.org", proxies=proxies) 参考文章requests quickstart requests advance Python HTTP 库：requests 快速入门 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[shell脚本系列--set 与 unset]]></title>
      <url>%2F2016%2F08%2F29%2Fshell-set%2F</url>
      <content type="text"><![CDATA[目录结构：命令用途 常用命令参数 使用示例 命令用途set 主要是显示系统中已经存在的shell变量，以及设置shell变量的新变量值。使用set更改shell特性时，符号”+”和”-“的作用分别是打开和关闭指定的模式。set命令不能够定义新的shell变量。如果要定义新的变量，可以使用declare命令以 变量名=值 的格式进行定义即可。 unset 用于删除已定义的shell变量（包括环境变量）和shell函数。unset命令不能够删除具有只读属性的shell变量和环境变量。 常用命令参数set常用参数 -a 标示已修改的变量，以供输出至环境变量。 -b 使被中止的后台程序立刻回报执行状态。 unset 常用参数 -f 仅删除函数 -v 仅删除变量 使用示例set 示例： 123456declare mylove='Visual C++' #定义新环境变量set -a mylove #设置为环境变量env | grep mylove #显示环境变量值 unset 示例： 12unset -v mylove #删除指定的环境变量 参考文章Linux unset linux set END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[shell脚本系列--declare 与 typeset]]></title>
      <url>%2F2016%2F08%2F26%2Fshell-declare-typeset%2F</url>
      <content type="text"><![CDATA[目录结构：命令用途 常用命令参数 使用示例 命令用途declare 与 typeset 命令是bash的内建命令，两者是完全一样的，用来声明shell变量，设置变量的属性。 常用命令参数 -r 设置变量为只读 -i 设置变量为整数 -a 设置变量为数组array -f 如果后面没有参数的话会列出之前脚本定义的所有函数，如果有参数的话列出以参数命名的函数 -x 设置变量在脚本外也可以访问到 使用示例12345678910111213141516171819202122232425262728293031#!/bin/bashfunc1 ()&#123; echo This is a function.&#125;declare -f # Lists the function above.echodeclare -i var1 # var1 is an integer.var1=2367echo "var1 declared as $var1"var1=var1+1 # Integer declaration eliminates the need for 'let'.echo "var1 incremented by 1 is $var1."# Attempt to change variable declared as integer.echo "Attempting to change var1 to floating point value, 2367.1."var1=2367.1 # Results in error message, with no change to variable.echo "var1 is still $var1"echodeclare -r var2=13.36 # 'declare' permits setting a variable property #+ and simultaneously assigning it a value.echo "var2 declared as $var2" # Attempt to change readonly variable.var2=13.37 # Generates error message, and exit from script.echo "var2 is still $var2" # This line will not execute.exit 0 # Script will not exit here. 参考文章Advanced Bash-Scripting Guide linux bash shell之declare END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--Nova]]></title>
      <url>%2F2016%2F08%2F22%2Fopenstack-nova%2F</url>
      <content type="text"><![CDATA[目录结构：概述 体系架构 VM实例的生命周期管理 动态迁移实例 代码结构与部署 概述 OpenStack Nova provides a cloud computing fabric controller, supporting a wide variety of virtualization technologies, including KVM, Xen, LXC, VMware, and more. In addition to its native API, it includes compatibility with the commonly encountered Amazon EC2 and S3 APIs. 也就是说，Nova 是一个控制器，它支持利用各种虚拟化技术（KVM,XEN等）完成云计算平台的CPU,内存，硬盘的虚拟化。云中的实例instance生命周期的所有活动都是由Nova控制。Nova自身并没有提供任何虚拟化能力，它使用libvirt API来与被支持的Hypervisors交互。Nova 通过一个与Amazon Web Services（AWS）EC2 API，S3 API兼容的web services API来对外提供服务。 体系架构首先放一张 Openstack 的整体概念结构图： 第二张图是逻辑结构图(图是13年的，现在的Network Service已经变更为Neutron) 图中可以看出，Nova处于Openstack的一个核心位置，其他组件都为 Nova 提供支持，Glance 为 VM 提供 image ，Cinder 和 Swift 分别为 VM 提供块存储和对象存储，Neutron 为 VM 提供网络连接。 接下来一张就是Nova的架构图： 这些组件以子服务（后台 deamon 进程）的形式运行。总的来说，nova的各个组件是以数据库和队列为中心进行通信的，下面对其中的几个组件做一个简单的介绍： nova-api: 接收和响应客户的 API 调用。 除了提供 OpenStack 自己的API，nova-api 还支持 Amazon EC2 API。 nova-scheduler: 虚机调度服务，负责决定在哪个计算节点上运行虚机 。 nova-compute: 管理虚机的核心服务，通过调用 Hypervisor API 实现虚机生命周期管理 。 Hypervisor: 计算节点上跑的虚拟化管理程序，虚机管理最底层的程序。 不同虚拟化技术提供自己的 Hypervisor。 常用的 Hypervisor 有 KVM，Xen， VMWare 等 。 nova-conductor: nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor。 Database: Nova 会有一些数据需要存放到数据库中，一般使用 MySQL。 数据库安装在控制节点上。 Nova 使用命名为 “nova” 的数据库。 Message Queue: 为解耦各个子服务，Nova 通过 Message Queue 作为子服务的信息中转站。OpenStack 默认是用 RabbitMQ 作为 Message Queue。 VM实例的生命周期管理 有的操作功能比较类似，也有各自的适用场景，简单介绍下上述几个重要的操作： 常规操作： 常规操作中，Launch、Start、Reboot、Shut Off 和 Terminate 都很好理解。 下面几个操作重点回顾一下： Resize: 通过应用不同的 flavor 调整分配给 instance 的资源。 Lock/Unlock: 可以防止对 instance 的误操作。 Pause/Suspend/Resume: 暂停当前 instance，并在以后恢复。 Pause 和 Suspend 的区别在于 Pause 将 instance 的运行状态保存在计算节点的内存中，而 Suspend 保存在磁盘上。 Pause 的优点是 Resume 的速度比 Suspend 快；缺点是如果计算节点重启，内存数据丢失，就无法 Resume 了，而 Suspend 则没有这个问题。 Snapshot: 备份 instance 到 Glance。产生的 image 可用于故障恢复，或者以此为模板部署新的 instance。 故障处理: 故障处理有两种场景：计划内和计划外。计划内是指提前安排时间窗口做的维护工作，比如服务器定期的微码升级，添加更换硬件等。 计划外是指发生了没有预料到的突发故障，比如强行关机造成 OS 系统文件损坏，服务器掉电，硬件故障等。 计划内故障处理:对于计划内的故障处理，可以在维护窗口中将 instance 迁移到其他计算节点。 涉及如下操作： Migrate: 将 instance 迁移到其他计算节点。 迁移之前，instance 会被 Shut Off，支持共享存储和非共享存储。 Live Migrate： 与 Migrate 不同，Live Migrate 能不停机在线地迁移 instance，保证了业务的连续性。也支持共享存储和非共享存储（Block Migration）。 Shelve/Unshelve： Shelve 将 instance 保存到 Glance 上，之后可通过 Unshelve 重新部署。 Shelve 操作成功后，instance 会从原来的计算节点上删除。 Unshelve 会重新选择节点部署，可能不是原节点。 计划外故障处理： 划外的故障按照影响的范围又分为两类：Instance 故障和计算节点故障 。 Instance 故障:Instance 故障只限于某一个 instance 的操作系统层面，系统无法正常启动。 可以使用如下操作修复 instance。 Rescue/Unrescue: 用指定的启动盘启动，进入 Rescue 模式，修复受损的系统盘。成功修复后，通过 Unrescue 正常启动 instance。 Rebuild: 如果 Rescue 无法修复，则只能通过 Rebuild 从已有的备份恢复。Instance 的备份是通过 snapshot 创建的，所以需要有备份策略定期备份。 计算节点故障: Instance 故障的影响范围局限在特定的instance，计算节点本身是正常工作的。如果计算节点发生故障，OpenStack 则无法与节点的 nova-compute 通信，其上运行的所有 instance 都会受到影响。这个时候，只能通过 Evacuate 操作在其他正常节点上重建 Instance。 Evacuate: 利用共享存储上 Instance 的镜像文件在其他计算节点上重建 Instance。 所以提前规划共享存储是关键。 动态迁移实例注意：以下内容大多摘自每天五分钟玩转openstack，强烈推荐，如果觉得TLDR,以下为部分节选。 接下来以一个instance 动态迁移的示例来看下Nova的具体执行流程。 Migrate 操作会先将 instance 停掉，也就是所谓的“冷迁移”。而 Live Migrate 是“热迁移”，也叫“在线迁移”，instance不会停机。 Live Migrate 分两种： 源和目标节点没有共享存储，instance 在迁移的时候需要将其镜像文件从源节点传到目标节点，这叫做 Block Migration（块迁移） 源和目标节点共享存储，instance 的镜像文件不需要迁移，只需要将 instance 的状态迁移到目标节点。 源和目标节点需要满足一些条件才能支持 Live Migration： 源和目标节点的 CPU 类型要一致。 源和目标节点的 Libvirt 版本要一致。 源和目标节点能相互识别对方的主机名称，比如可以在 /etc/hosts 中加入对方的条目。 在源和目标节点的 /etc/nova/nova.conf 中指明在线迁移时使用 TCP 协议。 Instance 使用 config driver 保存其 metadata。在 Block Migration 过程中，该 config driver 也需要迁移到目标节点。由于目前 libvirt 只支持迁移 vfat 类型的 config driver，所以必须在 /etc/nova/nova.conf 中明确指明 launch instance 时创建 vfat 类型的 config driver。 源和目标节点的 Libvirt TCP 远程监听服务得打开 接下来以非共享存储Block Migration 为例简述下Nova的工作流程： 向 nova-api 发送请求：通过dashboard 发送live migrate消息，指定源节点，目标节点。 nova-api 发送消息:nova-api 向 Messaging（RabbitMQ）发送了一条消息：“Live Migrate 这个 Instance” 源代码在 /opt/stack/nova/nova/compute/api.py，方法是 live_migrate。 nova-compute 执行操作： 目标节点执行迁移前的准备工作，首先将instance的数据迁移过来，主要包括镜像文件、虚拟网络等资源，日志在 devstack-controller:/opt/stack/logs/n-cpu.log。 源节点启动迁移操作，暂停 instance。 在目标节点上 Resume instance 在源节点上执行迁移的后处理工作，删除 instance。 在目标节点上执行迁移的后处理工作，创建 XML，在 Hypervisor 中定义 instance，使之下次能够正常启动。 代码结构与部署 未完待续 参考文章大家谈OpenStack-Nova组件理解 OpenStack Grizzly Architecture OpenStack Hacker养成指南 每天5分钟玩转 OpenStack openstack doc openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--neutron]]></title>
      <url>%2F2016%2F08%2F22%2Fopenstack-neutron%2F</url>
      <content type="text"><![CDATA[概述In short，neutron就是openstack为实现云平台上networking as a service而开发的项目。可以简单地理解为网络部分的虚拟化由neutron实现。 核心概念解释 1, Tenant networks 首先解释下network,一个network就是一个二层独立广播域。 由租户在project中创建的network.默认创建的network是完全独立的，不与其他project共享。neutron提供的tenant networks包括如下几种类型： Flat : 所有的虚拟机实例都在同一个network中，没有分隔策略,都在一个广播域。基本不用这种类型。 Vlan : 提供对Vlan的支持。 GRE and VXLAN ：利用隧道技术封装二层协议帧在L3或L4传输，实现两个虚拟Vlan的二层联通。 2, Provider networks 由openstack administrator 创建的network，在数据中心中映射物理网络。通常的类型为flat或Vlan 3, Subnet subnet属于网络中的三层概念，指定一段IPV4或者IPV6地址并描述其相关的配置信息。它附加在一个二层network 上指明属于这个network的虚拟机可使用的IP范围。 4, Port port 是一个设备的连接点，例如一台虚拟机的虚拟网卡。同时描述了相关的网络配置，例如该端口的MAC地址与IP地址。 5, Vlan VS Vxlan neutron 提供的服务 1, 基础的网络虚拟化（core service） 上文提到的network，subnet等都是一些基础的网络虚拟化服务。 2, DHCP 服务， 路由服务 这两个服务 都需要network namespace 技术来实现。 3, 安全组服务 通过 Linux Iptables实现。 4, LBaas 它允许租户动态的在自己的网络创建一个负载均衡设备。 5, FWaas 防火墙一般放在网关上，用来隔离子网之间的访问。因此，防火墙即服务（FireWall as a Service）也是在网络节点上（具体说来是在路由器命名空间中）来实现。 目前，OpenStack 中实现防火墙还是基于 Linux 系统自带的 iptables，所以大家对于其性能和功能就不要抱太大的期望了。 一个可能混淆的概念是安全组（Security Group），安全组的对象是虚拟网卡，由L2 Agent来实现，比如neutron_openvswitch_agent 和 neutron_linuxbridge_agent，会在计算节点上通过配置 iptables 规则来限制虚拟网卡的进出访问。防火墙可以在安全组之前隔离外部过来的恶意流量，但是对于同个子网内部不同虚拟网卡间的通讯不能过滤（除非它要跨子网）。 可以同时部署防火墙和安全组实现双重防护。 6, DVR(分布式路由) 按照 Neutron 原先的设计，所有网络服务都在网络节点上进行，这意味着大量的流量和处理，给网络节点带来了很大的压力。 这些处理的核心是路由器服务。任何需要跨子网的访问都需要路由器进行路由。 很自然，能否让计算节点上也运行路由器服务？这个设计思路无疑是更为合理的，但具体实施起来需要诸多细节上的技术考量。 为了降低网络节点的负载，同时提高可扩展性，OpenStack 自 Juno 版本开始正式引入了分布式路由（Distributed Virtual Router，DVR）特性（用户可以选择使用与否），来让计算节点自己来处理原先的大量东西向流量和非 SNAT 南北流量（有 floating IP 的 vm 跟外面的通信）。 这样网络节点只需要处理占到一部分的 SNAT （无 floating IP 的 vm 跟外面的通信）流量，大大降低了负载和整个系统对网络节点的依赖。很自然的，FWaaS 也可以跟着放到计算节点上。 DHCP 服务、VPN 服务目前仍然需要集中在网络节点上进行。 7, VPNaas 架构与原理 Neutron，其实和其他的OpenStack组件差不多，他都是一个中间层，自己基本不干具体的活，通过插件的机制，调用第三方的组件来完成相关的功能。 参考文章Openstack Neutron: Introduction OpenStack Neutron网络的肤浅理解 Overview and components 深入理解Neutron – OpenStack 网络实现 openstack doc openstack 设计与实现 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--Openstack 技术栈总览]]></title>
      <url>%2F2016%2F08%2F22%2Fopenstack-technology-stack%2F</url>
      <content type="text"><![CDATA[END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--网络虚拟化基础知识]]></title>
      <url>%2F2016%2F08%2F15%2Fopenstack-network-virtualization%2F</url>
      <content type="text"><![CDATA[网络发展历程详见：SDN软件定义网络从入门到精通》导论课 总结一下目前还常用的硬件设备： 集线器(hub)：属于物理层产品，对信息进行中继和放大，从任意接口收到的数据会往其他所有接口广播。 交换机(switch):链路层产品，记录终端主机的mac地址并生成mac表，根据mac表转发主机之间的数据流。能够进行VLAN隔离。 路由器(router):网络层产品，基于IP寻址，采用路由表实现数据转发。 网络虚拟化与SDNSDN(Software Define Network)是一种集中控制的网络架构，可将网络划分为数据层面和控制层面,它是实现网络虚拟化的一种解决方案。通常来说，只要网络硬件可以集中式软件管理，可编程化，控制转发层面分开，则可以认为这个网络是一个SDN网络。openstack 的neutron组件就是一种SDN 架构(其实，SDN的创始人就是Openstack Quantum,即后来的neutron 的发起者之一)。SDN大体架构如下(图片来自SDN与网络虚拟化的起源与现状)： 网络虚拟化可以认为是一种网络技术，它可以在物理网络上虚拟多个相互隔离的虚拟网络，从而使得不同用户之间使用独立的网络资源切片，从而提高网络资源利用率，实现弹性的网络。网络虚拟化是云计算和SDN发展到一定阶段的产物，因此可以认为网络虚拟化是新一代的SDN。延伸阅读：SDN与网络虚拟化的起源与现状 openstack相关的网络虚拟化知识 Linux Bridge:Linux Bridge 是 Linux 上用来做 TCP/IP二层协议交换的设备，其功能大家可以简单的理解为是一个二层交换机或者Hub。多个网络设备可以连接到同一个 Linux Bridge，当某个设备收到数据包时，Linux Bridge 会将数据转发给其他设备。 如上图，两个虚拟机，各自分配一个虚拟网卡vnet0,vnet1.他们连接在一个叫做br0 的Linux bridge 上。宿主机的网卡eth0分配到br0下，这样VM1与VM2可以相互通信，与外网也可以通信。 LAN 与VLAN : LAN: 表示 Local Area Network，本地局域网，通常使用 Hub 和 Switch 来连接 LAN 中的计算机。一个 LAN 表示一个广播域。 其含义是：LAN 中的所有成员都会收到任意一个成员发出的广播包。 VLAN: Virtual LAN。一个带有 VLAN 功能的switch 能够将自己的端口划分出多个 LAN。计算机发出的广播包可以被同一个 LAN 中其他计算机收到，但位于其他 LAN 的计算机则无法收到。 简单地说，VLAN 将一个交换机分成了多个交换机，限制了广播的范围，在二层将计算机隔离到不同的 VLAN 中。 VLAN的实现：一种方法是使用两个交换机，不同VLAN组的机器分别接到一个交换机。 另一种方法是使用一个带 VLAN 功能的交换机，将不同VLAN组的机器分别放到不同的 VLAN 中。请注意，VLAN 的隔离是二层上的隔离，不同VLAN无法相互访问指的是二层广播包（比如 arp）无法跨越 VLAN 的边界。但在三层上（比如IP）是可以通过路由器让 不同VLAN互通的。 KVM环境下VLAN的实现： eth0 是宿主机上的物理网卡，有一个命名为 eth0.10 的子设备与之相连。 eth0.10 就是 VLAN 设备了，其 VLAN ID 就是 VLAN 10。 eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚机 VM1 的虚拟网卡 vent0 也挂在 brvlan10 上。这样，宿主机用软件实现了一个交换机（当然是虚拟的），上面定义了一个 VLAN10。 eth0.10，brvlan10 和 vnet0 都分别接到 VLAN10 的 Access口上。而 eth0 就是一个 Trunk 口。VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。brvlan20 同理。 也可以利用 OpenVSwitch实现（官方推荐） OpenVSwitch: 简称OVS,是一个高质量的、多层虚拟交换机，使用开源Apache2.0许可协议，由Nicira Networks开发，主要实现代码为可移植的C代码。它的目的是让大规模网络自动化可以通过编程扩展,同时仍然支持标准的管理接口和协议（例如NetFlow, sFlow, SPAN, RSPAN, CLI, LACP, 802.1ag）。此外,它被设计位支持跨越多个物理服务器的分布式环境，类似于VMware的vNetwork分布式vswitch或Cisco Nexus 1000 V。Open vSwitch支持多种linux 虚拟化技术，包括Xen/XenServer， KVM和irtualBox。 Linux nameSpace: Linux 提供的一种内核级别环境隔离的方法。类似chroot系统调用（通过修改根目录把用户监禁到一个特定目录下，chroot内部的文件系统无法访问外部的内容），Linux Namespace在此基础上，提供了对UTS、IPC、mount、PID、network、User等的隔离机制。在openstack neutron 中就利用network namespace 进行网络的隔离。 参考文章SDN与网络虚拟化的起源与现状 SDN软件定义网络从入门到精通》导论课 Neutron 理解 (1): Neutron 所实现的虚拟化网络 动手实践 Linux VLAN - 每天5分钟玩转 OpenStack（13） openstack doc OVS初级教程：使用open vswitch构建虚拟网络 Introducing Linux Network Namespaces Docker基础技术：Linux Namespace（上） Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack系列--服务器虚拟化知识]]></title>
      <url>%2F2016%2F08%2F15%2Fopenstack-hypevisor%2F</url>
      <content type="text"><![CDATA[概念与认识直接引用wiki: 在计算机技术中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括计算能力和数据存储。 关于虚拟化的大致分类，参考虚拟化分类 目前比较常用的虚拟化技术包括KVM,XEN,vmware workstation等，本文重点介绍下KVM技术。 KVM 技术KVM 简介KVM（Kernel-based Virtual Machine） 技术是在x86硬件平台（包含虚拟化扩展如Intel VT或AMD-V）上的一种完全虚拟化解决方案。效率可达到物理机的80％以上。它包含一个为处理器提供底层虚拟化可加载的核心模块kvm.ko（kvm-intel.ko 或 kvm-AMD.ko)。kvm还需要一个经过修改的QEMU软件（qemu-kvm），作为虚拟机上层控制和界面。kvm能在不改变linux或windows镜像的情况下同时运行多个虚拟机，（ps：它的意思是多个虚拟机使用同一镜像）并为每一个虚拟机配置个性化硬件环境（网卡、磁盘、图形适配器……）。在主流的linux内核，如2.6.20以上的内核均包含了kvm核心。 图片来自wikipedia 名词解释：KVM QEMU qemu-kvm LibvirtKVM:一种虚拟化技术 QEMU:一种模拟器，它向Guest OS模拟CPU和其他硬件，Guest OS认为自己和硬件直接打交道，其实是同Qemu模拟出来的硬件打交道，Qemu将这些指令转译给真正的硬件。 qemu-kvm:kvm负责cpu虚拟化+内存虚拟化，实现了cpu和内存的虚拟化，但kvm不能模拟其他设备。qemu模拟IO设备（网卡，磁盘等），kvm加上qemu之后就能实现真正意义上服务器虚拟化。因为用到了上面两个东西，所以称之为qemu-kvm。 Libvirt:目前使用最为广泛的一种对虚拟机进行管理的工具和API。Libvirtd是一个daemon进程，可以被本地的virsh调用，也可以被远程的virsh调用，Libvirtd调用qemu-kvm操作虚拟机。 更多Libvirt介绍 KVM 与 Docker)docker优势： 更轻量，启动和关闭速度更快 性能比虚拟化来说具有优势 docker劣势： 安全性 相对虚拟化来说生态环境还未成熟 推荐文章 参考文章OpenStack设计与实现（一）虚拟化 kvm vs docker 虚拟机已死 “容器”才是未来？ END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[opensatck 系列--RabbitMQ 快速入门]]></title>
      <url>%2F2016%2F08%2F08%2Fintroduce-to-RabbitMQ%2F</url>
      <content type="text"><![CDATA[简介RabbitMQ 是一个消息队列框架，一个message broker，它的主要工作就是接收并转发消息。类似生产者/消费者模型，它的三个主要角色也是生产者（产生消息的角色），消费者（处理消息的角色），消息队列（缓存消息），接下来介绍下RabbitMQ的几个经典的应用模式。 准备工作RabbitMQ 安装 封装了AMQP的python第三方库Pika 发布/订阅 模式所谓发布/订阅是指：producer 发布消息，订阅的consumer都会接收到此消息，类似广播。示例给出一个简单的日志系统，产生的日志信息会广播到所有的consumer。 producer 代码如下，解释在注释中： 1234567891011121314151617import pika # 引入pika包import sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host='localhost')) #指定主机hostchannel = connection.channel() #创建channelchannel.exchange_declare(exchange='logs', type='fanout') #声明exchange，以及exchange的type，解释见下文#channel.queue_declare(queue='hello') #创建一个名为hello的queue，因为此处是用的rabbitmq自动创建模式，所以注释掉message = ' '.join(sys.argv[1:]) or "info: Hello World!" #要发送的消息channel.basic_publish(exchange='logs', routing_key='', body=message) #绑定exchange，消息等print(" [x] Sent %r" % message)connection.close() # 关闭连接，关闭前会确定queue中是否还有消息，没有则关闭 Exchanges 解释：你可以把exchange看作是producer与queue之间的一个分发器，它会按照指定的策略把消息分发到指定queue中。 exchange 的类型有四种， direct 与routing_key 结合使用，指定发送到某个queue topic 按照某种主题模式，指定发送到满足该模式的多个queue headers RPC模式 fanout 广播模式 consumer 代码如下： 1234567891011121314151617181920212223242526import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters( host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='logs', type='fanout')result = channel.queue_declare(exclusive=True) #没有指定名字所以创建一个临时queue,而且consume断开后该queue会被删除queue_name = result.method.queuechannel.queue_bind(exchange='logs', queue=queue_name) # 绑定exchange与queueprint(' [*] Waiting for logs. To exit press CTRL+C')def callback(ch, method, properties, body): # 被调用的处理逻辑函数 print(" [x] %r" % body)channel.basic_consume(callback, queue=queue_name, no_ack=True) #指定callback函数，queue,no_ack解释见下文channel.start_consuming() ACK(消息确认机制)在rabbitmq运行过程中，如果某个consumer挂掉，那么分配到该consumer的所有其他任务也就无法完成。为了防止这种情况，默认ACK机制开启，consumer完成某个任务后会返回queue一个ack,表示当前任务已完成，可以分配下一个任务。如果指定no_ack=True，那么就会关闭ack机制。 其余message分发机制routing 分发 topic 分发 RPC调用 参考文章RabbitMQ doc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 系列--Pecan 简介]]></title>
      <url>%2F2016%2F08%2F07%2Fintroduce-to-Pecan%2F</url>
      <content type="text"><![CDATA[简介Pecan是一个轻量级的python web框架，它专注于HTTP，提供对象路由的寻址方式，没有提供针对session或者数据库的封装功能。尽管是轻量级的，也提供了一些可扩展的特性： 对象路由 REST风格的controller支持 可扩展的安全框架 可扩展的模板语言支持 可扩展的json支持 基于python文件的配置 安装About installation 基于Pecan创建第一个应用Quick start 对象路由介绍所谓对象路由就是将Http请求路径map到相应的controller对象的方法。具体操作就是先将URL路径拆分，然后从root controller开始一层层的往下寻找对应的方法，示例如下： 12345678910111213141516171819202122232425262728from pecan import exposeclass BooksController(object): @expose() def index(self): return "Welcome to book section." @expose() def bestsellers(self): return "We have 5 books in the top 10."class CatalogController(object): @expose() def index(self): return "Welcome to the catalog." books = BooksController()class RootController(object): @expose() def index(self): return "Welcome to store.example.com!" @expose() def hours(self): return "Open 24/7 on the web." catalog = CatalogController() @expose()是一个注解的装饰函数，只有注解了该函数才能进行对象路由，其参数常用于指定返回值，模板。一个request：http://xxxx/catalog/books/bestsellers ,首先会找到RootController，接着找到对应的catalog,也就是CatalogController,再接着找BookController,以此类推。除此之外，pecan还提供了pecan.route()指定路由匹配，以及 _lookup(), _default(), _route()函数。详细说明见官方文档。 Restful 风格的controller为了兼容 TurboGears2，Pecan也提供了一套rest风格的controller，RestController. 12345678910111213from pecan import exposefrom pecan.rest import RestControllerfrom mymodel import Bookclass BooksController(RestController): @expose() def get(self, id): book = Book.get(id) if not book: abort(404) return book.title 默认的URL mapping 如下： 配置文件当我们新建一个Pecan项目时，默认配置文件如下： 1234567891011server = &#123; 'port' : '8080', 'host' : '0.0.0.0'&#125;app = &#123; 'root' : None, 'modules' : [], 'static_root' : 'public', 'template_path' : ''&#125; 首先是server的配置，就是端口与所在机器的IP.第二个是app配置，Pecan利用app配置选项将你的应用最终打包成一个wsgi的应用，一个典型的配置如下： 1234567app = &#123; 'root' : 'project.controllers.root.RootController', 'modules' : ['project'], 'static_root' : '%(confdir)s/public', 'template_path' : '%(confdir)s/project/templates', 'debug' : True&#125; root: 应用的rootcontroller。modules:就是pecan去找寻你的应用的入口目录，该目录下必须包含一个app.py,该文件必须包含setup_app() 函数。static_root:静态文件所在根目录。template_path：模板文件所在目录。debug：是否开启debug模式。除了这两个主要配置，还有其他配置，不再赘述。 Pecan Hooks利用wsgi中间件访问Pecan内部是一件比较难的事情，Pecan提供了hooks机制来实现类似wsgi中间件的功能。Hooks提供了如下函数让你在request周期内的一些节点上做操作。 on-route(): 在Pecan将一个请求路由给对应的controller前调用。 before():路由完成，代码执行前调用。 after(): 代码执行完后调用 on_error():一个请求出现异常时调用。 实现一个Pecan Hook 12345678910from pecan.hooks import PecanHookclass SimpleHook(PecanHook): def before(self, state): print "\nabout to enter the controller..." def after(self, state): print "\nmethod: \t %s" % state.request.method print "\nresponse: \t %s" % state.response.status 接下来就是将Pecan hook attached 到 controller，有两种，如果是attached到整个项目所有的controller，可以在配置文件中指定，如下： 12345app = &#123; 'root' : '...' # ... 'hooks': lambda: [SimpleHook()]&#125; 如果是指定到某一个controller，那么可以用hooks做如下操作： 123456789101112from pecan import exposefrom pecan.hooks import HookControllerfrom my_hooks import SimpleHookclass SimpleController(HookController): __hooks__ = [SimpleHook()] @expose('json') def index(self): print "DO SOMETHING!" return dict() 参考文章pecan doc END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[shell脚本系列--Zsh简介与使用]]></title>
      <url>%2F2016%2F08%2F05%2Fintroduce-to-Zsh%2F</url>
      <content type="text"><![CDATA[Zsh同bash一样，是一款功能强大的终端（shell）软件，只不过bash是大部分Linux发行版默认的shell，而Zsh需要手动安装（mac默认已安装）。关于Zsh的更多介绍请看 wiki)。 Zsh相比于bash有很多优势，两者的对比请看 why zsh is cooler than your shell概括一下就是：高效，自动补全更优秀，可定制型高。借用官网上一句话：if you want you hand dirty, this is definitely your choice! 而且随着开源项目 oh my zsh的火爆，zsh的各种主题以及插件也很齐全，适应一段时间后用起来得心应手。 当然，如果生产环境是分布式的情况下，还是建议规矩的用bash，毕竟来回切换的成本也是不小的。 接下来开始简单说下zsh的安装以及试用。 准备工作 Unix-based 机器 已安装 curl或者wget ,git 安装Zsh 两种安装方法：借用包管理工具（如：apt-get install zsh），源码安装（略） 安装完成后输入 zsh –version 出现相应的版本号即安装成功 输入 chsh -s $(which zsh) 更改默认shell 为zsh 退出之后再次登录即进入Zsh 安装Oh My Zsh（两种方法） 第一种，通过curl: 1sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)" 第二种，通过wget: 1sh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" Oh My Zsh使用简介 插件：zsh的配置文件为 ~/.zshrc 相应的插件配置选项为 plugins=(git bundler osx rake ruby) 主题：在配置文件中的配置选项为： ZSH_THEME=”robbyrussell” 插件与主题都是可以配置的，可供配置的插件以及列表在这Extern themes , External-plugins 参考文章Zsh wiki) oh my zsh wiki github readme END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[思考系列--如何学习一门新技术]]></title>
      <url>%2F2016%2F08%2F01%2Fthe-way-to-learn-a-new-technology%2F</url>
      <content type="text"><![CDATA[目录结构：技术认知的三个层次 技术需求的定位 不同层次的学习实施方法 文章主要是以学习一门计算机编程相关技术为目的，其他的知识学习方法类似，具体实施细节可能会不同。 技术认知的三个层次为了之后描述方便，首先给出技术认知的三个大致层次：简洁明了。再简单介绍下我所理解的这三个层次： 了解：知道这门技术为什么会出现，所解决的问题，大体框架，以及相关的技术栈。In short, 吹牛逼的时候能轻易地拽出这个词，但还没有付诸实践。 熟悉：已经将该技术落实于至少一个项目之上，知道如何使用以及使用过程中会有哪些常见的坑,但内在原理还不甚了解。 精通：读过源码，落实过至少三个项目，原理通透，知晓相比业界其他类似解决方案的pros and cons。这三个层次也可以理解为渐进式的学习。 技术需求的定位技术永远是为业务而服务的，我们学习一门技术时最好先找准自己对技术需求的定位。找到自己的定位就容易理清主线，不至于产生“乱”的感觉。技术需求也可以根据技术认知的三个层次划分。– 接触到一项新技术，比较感兴趣或者只想知道该技术的用途以及它所解决的痛点，对技术的认知止于了解层面即可。比较适于技术偏产品的职业。– 会用即可，不必了解深层次实现。适用于大部分程序员。– 对技术有更高层次的追求，知其所以然，适用于部分程序员。 不同层次的学习实施方法 了解层次： 官网guide wiki 知乎/quora/stackoverflow/github 知名blog google 熟练层次： 官网doc,完成demo 找一个经典项目并完成 精通层次 相关高评分书籍（amazon,豆瓣） 项目重构 阅读源码 END]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql 系列--Invalid default value for UPDATE_TIME]]></title>
      <url>%2F2013%2F06%2F05%2Ffrance-removes-internet-cut-off%2F</url>
      <content type="text"><![CDATA[导入SQL文件时出现“Invalid default value for UPDATE_TIME”,UPDATE_TIME 是timeStamp类型。设置的默认值为“00-00-00 0000 0000” 。 经过不断试验发现是因为sql_mode 的设置原因，按照网上说的更改my_default.ini 并重启mysql后发现还是不行。SELECT @@global.sql_mode;SELECT @@session.sql_mode; sql_mode 依然没有改变 后直接在数据库层面改： SET GLOBAL sql_mode=’’; SET SESSION sql_mode=’’; 将sql_mode 设置为空，问题解决。 关于sql_mode ，见此 END]]></content>
    </entry>

    
  
  
</search>
